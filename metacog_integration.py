# #!/usr/bin/env python3
# """
# å®Œæ•´çš„å…ƒè®¤çŸ¥æ¨¡å—ä¸gr00t+RoboCasaé›†æˆ
# åŒ…å«æ‰€æœ‰ç¥ç»ç½‘ç»œç»„ä»¶çš„å®Œæ•´å®ç°
# """

# import numpy as np
# import torch
# import torch.nn as nn
# import time
# import json
# import threading
# import queue
# from typing import Dict, List, Any, Optional, Tuple
# from dataclasses import dataclass, asdict
# from pathlib import Path
# import cv2
# from enum import Enum

# # ==================== æ ¸å¿ƒæ•°æ®ç»“æ„å®šä¹‰ ====================

# class DirectiveType(Enum):
#     """æŒ‡ä»¤ç±»å‹æšä¸¾"""
#     CONTINUE = "continue"
#     ADJUST = "adjust"
#     RETRY = "retry"
#     ABORT = "abort"
#     SWITCH_MODE = "switch_mode"

# class TaskPhase(Enum):
#     """ä»»åŠ¡æ‰§è¡Œé˜¶æ®µ"""
#     APPROACH = "approach"
#     CONTACT = "contact"
#     MANIPULATION = "manipulation"
#     COMPLETE = "complete"

# @dataclass
# class SensorData:
#     """ä¼ æ„Ÿå™¨æ•°æ®ç»“æ„"""
#     rgb_image: np.ndarray
#     depth_image: np.ndarray
#     force_torque: np.ndarray
#     contact_detected: bool
#     joint_positions: np.ndarray
#     joint_velocities: np.ndarray
#     end_effector_pose: np.ndarray
#     system1_commands: np.ndarray
#     execution_status: str
#     timestamp: float

# @dataclass
# class CognitiveState:
#     """è®¤çŸ¥çŠ¶æ€è¡¨ç¤º"""
#     lighting_condition: str
#     scene_complexity: float
#     occlusion_level: float
#     current_phase: TaskPhase
#     success_probability: float
#     risk_level: str
#     visual_confidence: float
#     force_noise_level: float
#     position_accuracy: float

# @dataclass
# class MetaCognitiveOutput:
#     """å…ƒè®¤çŸ¥è¾“å‡ºç»“æ„"""
#     directive: DirectiveType
#     reasoning: str
#     parameters: Dict[str, Any]
#     confidence: float
#     urgency: str

# # ==================== ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆæ‚¨çš„åŸå§‹æ¶æ„ï¼‰====================

# class VisualEncoder(nn.Module):
#     """è§†è§‰ç¼–ç å™¨"""
#     def __init__(self, input_channels=4, hidden_dim=512):
#         super().__init__()
#         self.rgb_encoder = nn.Sequential(
#             nn.Conv2d(3, 64, 3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#             nn.Conv2d(64, 128, 3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#             nn.AdaptiveAvgPool2d((8, 8)),
#             nn.Flatten(),
#             nn.Linear(128 * 8 * 8, hidden_dim)
#         )
        
#         self.depth_encoder = nn.Sequential(
#             nn.Conv2d(1, 32, 3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#             nn.Conv2d(32, 64, 3, padding=1),
#             nn.ReLU(),
#             nn.AdaptiveAvgPool2d((8, 8)),
#             nn.Flatten(),
#             nn.Linear(64 * 8 * 8, hidden_dim // 2)
#         )
        
#         self.fusion = nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim)
    
#     def forward(self, rgb_image, depth_image):
#         rgb_features = self.rgb_encoder(rgb_image)
#         depth_features = self.depth_encoder(depth_image)
#         combined = torch.cat([rgb_features, depth_features], dim=-1)
#         return self.fusion(combined)

# class ForceEncoder(nn.Module):
#     """åŠ›è§‰ç¼–ç å™¨"""
#     def __init__(self, force_dim=6, hidden_dim=128):
#         super().__init__()
#         self.encoder = nn.Sequential(
#             nn.Linear(force_dim, 64),
#             nn.ReLU(),
#             nn.Linear(64, hidden_dim),
#             nn.ReLU(),
#             nn.Linear(hidden_dim, hidden_dim)
#         )
    
#     def forward(self, force_torque):
#         return self.encoder(force_torque)

# class ProprioceptiveEncoder(nn.Module):
#     """æœ¬ä½“æ„Ÿè§‰ç¼–ç å™¨"""
#     def __init__(self, joint_dim=5, pose_dim=7, hidden_dim=128):
#         super().__init__()
#         self.joint_encoder = nn.Linear(joint_dim * 2, hidden_dim // 2)
#         self.pose_encoder = nn.Linear(pose_dim, hidden_dim // 2)
#         self.fusion = nn.Linear(hidden_dim, hidden_dim)
    
#     def forward(self, joint_positions, joint_velocities, end_effector_pose):
#         joint_input = torch.cat([joint_positions, joint_velocities], dim=-1)
#         joint_features = self.joint_encoder(joint_input)
#         pose_features = self.pose_encoder(end_effector_pose)
#         combined = torch.cat([joint_features, pose_features], dim=-1)
#         return self.fusion(combined)

# class MultimodalFusion(nn.Module):
#     """å¤šæ¨¡æ€èåˆç½‘ç»œ"""
#     def __init__(self, visual_dim=512, force_dim=128, proprio_dim=128, output_dim=256):
#         super().__init__()
#         self.cross_attention = nn.MultiheadAttention(
#             embed_dim=output_dim, num_heads=8, batch_first=True
#         )
        
#         self.visual_proj = nn.Linear(visual_dim, output_dim)
#         self.force_proj = nn.Linear(force_dim, output_dim)
#         self.proprio_proj = nn.Linear(proprio_dim, output_dim)
        
#         self.norm = nn.LayerNorm(output_dim)
#         self.output_proj = nn.Linear(output_dim * 3, output_dim)
    
#     def forward(self, visual_features, force_features, proprio_features):
#         # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
#         visual_proj = self.visual_proj(visual_features).unsqueeze(1)
#         force_proj = self.force_proj(force_features).unsqueeze(1)
#         proprio_proj = self.proprio_proj(proprio_features).unsqueeze(1)
        
#         # æ‹¼æ¥ä¸ºåºåˆ—
#         multimodal_seq = torch.cat([visual_proj, force_proj, proprio_proj], dim=1)
        
#         # äº¤å‰æ³¨æ„åŠ›èåˆ
#         attended, _ = self.cross_attention(multimodal_seq, multimodal_seq, multimodal_seq)
#         attended = self.norm(attended)
        
#         # è¾“å‡ºèåˆ
#         fused = attended.flatten(start_dim=1)
#         return self.output_proj(fused)

# class CognitiveStateEstimator(nn.Module):
#     """è®¤çŸ¥çŠ¶æ€ä¼°è®¡å™¨"""
#     def __init__(self, input_dim=256, hidden_dim=128):
#         super().__init__()
#         self.state_network = nn.Sequential(
#             nn.Linear(input_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Linear(hidden_dim, hidden_dim),
#             nn.ReLU()
#         )
        
#         # ç¯å¢ƒè¯„ä¼°åˆ†æ”¯
#         self.env_lighting = nn.Linear(hidden_dim, 3)  # normal, bright, dim
#         self.env_complexity = nn.Linear(hidden_dim, 1)  # 0-1
#         self.env_occlusion = nn.Linear(hidden_dim, 1)   # 0-1
        
#         # æ‰§è¡ŒçŠ¶æ€åˆ†æ”¯
#         self.exec_phase = nn.Linear(hidden_dim, 4)      # 4ä¸ªé˜¶æ®µ
#         self.exec_success_prob = nn.Linear(hidden_dim, 1)  # 0-1
#         self.exec_risk = nn.Linear(hidden_dim, 3)       # low, medium, high
        
#         # ä¼ æ„Ÿå™¨å¯é æ€§åˆ†æ”¯
#         self.sensor_visual = nn.Linear(hidden_dim, 1)   # 0-1
#         self.sensor_force = nn.Linear(hidden_dim, 1)    # 0-1
#         self.sensor_position = nn.Linear(hidden_dim, 1) # 0-1
    
#     def forward(self, fused_features):
#         hidden = self.state_network(fused_features)
        
#         # ç¯å¢ƒè¯„ä¼°
#         lighting = torch.softmax(self.env_lighting(hidden), dim=-1)
#         complexity = torch.sigmoid(self.env_complexity(hidden))
#         occlusion = torch.sigmoid(self.env_occlusion(hidden))
        
#         # æ‰§è¡ŒçŠ¶æ€
#         phase = torch.softmax(self.exec_phase(hidden), dim=-1)
#         success_prob = torch.sigmoid(self.exec_success_prob(hidden))
#         risk = torch.softmax(self.exec_risk(hidden), dim=-1)
        
#         # ä¼ æ„Ÿå™¨å¯é æ€§
#         visual_conf = torch.sigmoid(self.sensor_visual(hidden))
#         force_noise = torch.sigmoid(self.sensor_force(hidden))
#         position_acc = torch.sigmoid(self.sensor_position(hidden))
        
#         return {
#             'lighting': lighting,
#             'complexity': complexity,
#             'occlusion': occlusion,
#             'phase': phase,
#             'success_prob': success_prob,
#             'risk': risk,
#             'visual_confidence': visual_conf,
#             'force_noise': force_noise,
#             'position_accuracy': position_acc
#         }

# # ==================== CoTæ¨ç†å¼•æ“ ====================

# class CoTReasoner:
#     """é“¾å¼æ€ç»´æ¨ç†å¼•æ“"""
#     def __init__(self, reasoning_mode="neural", model_name=None, device="cuda"):
#         self.reasoning_mode = reasoning_mode
#         self.model_name = model_name
#         self.device = device
        
#         if reasoning_mode == "neural":
#             self._init_neural_reasoner()
#         elif reasoning_mode == "rule":
#             self._init_rule_engine()
#         else:  # hybrid
#             self._init_neural_reasoner()
#             self._init_rule_engine()
    
#     def _init_neural_reasoner(self):
#         """åˆå§‹åŒ–ç¥ç»ç½‘ç»œæ¨ç†å™¨"""
#         self.reasoning_net = nn.Sequential(
#             nn.Linear(256 + 8, 128),  # ä¿®æ­£è¾“å…¥ç»´åº¦
#             nn.ReLU(),
#             nn.Linear(128, 64),
#             nn.ReLU(),
#             nn.Linear(64, 32)
#         ).to(self.device)
        
#         self.analysis_net = nn.Linear(32, 16).to(self.device)
#         self.reasoning_net_causal = nn.Linear(32, 16).to(self.device)
#         self.decision_net = nn.Linear(32, 8).to(self.device)
        
#         # æ¨ç†æ¨¡æ¿
#         self.analysis_templates = [
#             "è§†è§‰ä¿¡æ¯ä¸è¶³ï¼Œå¯èƒ½å½±å“æ“ä½œç²¾åº¦",
#             "æ¥è§¦åŠ›å¼‚å¸¸ï¼Œå¯èƒ½æŠ“å–å¤±è´¥", 
#             "å…³èŠ‚ä½ç½®åå·®è¾ƒå¤§ï¼Œéœ€è¦é‡æ–°å®šä½",
#             "æ‰§è¡Œé€Ÿåº¦è¿‡å¿«ï¼Œé£é™©è¾ƒé«˜",
#             "ç¯å¢ƒå…‰ç…§æ¡ä»¶å·®ï¼Œå¢åŠ æ“ä½œéš¾åº¦"
#         ]
        
#         self.reasoning_templates = [
#             "ç”±äºä¼ æ„Ÿå™¨å™ªå£°å¢åŠ ï¼Œå»ºè®®é™ä½æ“ä½œé€Ÿåº¦",
#             "åŸºäºå½“å‰åŠ›åé¦ˆï¼Œéœ€è¦è°ƒæ•´å¤¹æŒåŠ›åº¦",
#             "è€ƒè™‘åˆ°è§†è§‰é®æŒ¡ï¼Œåº”è¯¥åˆ‡æ¢åˆ°åŠ›å¼•å¯¼æ¨¡å¼",
#             "æ ¹æ®å…³èŠ‚çŠ¶æ€ï¼Œå»ºè®®é‡æ–°è§„åˆ’è½¨è¿¹"
#         ]
        
#         self.decision_templates = [
#             "ç»§ç»­å½“å‰æ“ä½œï¼Œæ— éœ€è°ƒæ•´",
#             "å¾®è°ƒåŠ›æ§å‚æ•°ï¼Œé™ä½æ¥è§¦åŠ›",
#             "æš‚åœæ“ä½œï¼Œé‡æ–°è·å–è§†è§‰ä¿¡æ¯",
#             "åˆ‡æ¢åˆ°åŒæ‰‹åè°ƒæ¨¡å¼",
#             "ä¸­æ­¢å½“å‰ä»»åŠ¡ï¼Œè¿”å›å®‰å…¨å§¿æ€"
#         ]
    
#     def _init_rule_engine(self):
#         """åˆå§‹åŒ–è§„åˆ™å¼•æ“"""
#         self.risk_rules = {
#             "high_risk": lambda v, f, s: v < 0.3 or f > 0.8 or s < 0.2,
#             "medium_risk": lambda v, f, s: v < 0.6 or f > 0.5 or s < 0.5,
#             "low_risk": lambda v, f, s: v >= 0.6 and f <= 0.5 and s >= 0.5
#         }
        
#         self.action_rules = {
#             "force_too_high": lambda force: force > 10.0,
#             "position_error": lambda error: np.linalg.norm(error) > 0.05,
#             "visual_occlusion": lambda confidence: confidence < 0.4
#         }
    
#     def reason(self, sensor_data: SensorData, cognitive_state: Dict) -> str:
#         """æ‰§è¡ŒCoTæ¨ç†"""
#         if self.reasoning_mode == "neural":
#             return self._neural_reasoning(sensor_data, cognitive_state)
#         elif self.reasoning_mode == "rule":
#             return self._rule_reasoning(sensor_data, cognitive_state)
#         else:
#             neural_result = self._neural_reasoning(sensor_data, cognitive_state)
#             rule_result = self._rule_reasoning(sensor_data, cognitive_state)
#             return self._combine_reasoning(neural_result, rule_result)
    
#     def _neural_reasoning(self, sensor_data: SensorData, cognitive_state: Dict) -> str:
#         """ç¥ç»ç½‘ç»œæ¨ç†"""
#         # æ„å»ºè¾“å…¥ç‰¹å¾
#         visual_conf = cognitive_state['visual_confidence'].item()
#         force_noise = cognitive_state['force_noise'].item()
#         success_prob = cognitive_state['success_prob'].item()
#         risk_level = torch.argmax(cognitive_state['risk']).item()
        
#         # ä¼ æ„Ÿå™¨çŠ¶æ€ç‰¹å¾ - ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
#         sensor_features = torch.tensor([
#             visual_conf, force_noise, success_prob, risk_level,
#             np.linalg.norm(sensor_data.force_torque),
#             np.linalg.norm(sensor_data.joint_velocities),
#             float(sensor_data.contact_detected),
#             0.0  # å¡«å……åˆ°8ç»´
#         ], device=self.device, dtype=torch.float32).unsqueeze(0)
        
#         # è®¤çŸ¥çŠ¶æ€ç‰¹å¾ - ä»GPUå¼ é‡ä¸­æå–å¹¶é‡æ–°ç»„åˆ
#         cognitive_tensor_parts = [
#             cognitive_state['visual_confidence'].flatten(),
#             cognitive_state['force_noise'].flatten(), 
#             cognitive_state['success_prob'].flatten(),
#             cognitive_state['phase'].flatten(),
#             cognitive_state['risk'].flatten()
#         ]
        
#         # è®¡ç®—å½“å‰ç‰¹å¾æ€»ç»´åº¦
#         current_dim = sum(part.numel() for part in cognitive_tensor_parts)
#         target_dim = 256
        
#         # åˆ›å»ºå¡«å……å¼ é‡ï¼Œç¡®ä¿åœ¨ç›¸åŒè®¾å¤‡ä¸Š
#         if current_dim < target_dim:
#             padding = torch.zeros(target_dim - current_dim, device=self.device, dtype=torch.float32)
#             cognitive_tensor_parts.append(padding)
        
#         # åˆå¹¶è®¤çŸ¥çŠ¶æ€ç‰¹å¾
#         cognitive_features = torch.cat(cognitive_tensor_parts, dim=0).unsqueeze(0)
        
#         # åˆå¹¶æ‰€æœ‰ç‰¹å¾
#         combined_features = torch.cat([cognitive_features, sensor_features], dim=-1)
        
#         # æ¨ç†ç½‘ç»œå‰å‘ä¼ æ’­
#         with torch.no_grad():
#             reasoning_features = self.reasoning_net(combined_features)
            
#             analysis_logits = self.analysis_net(reasoning_features)
#             reasoning_logits = self.reasoning_net_causal(reasoning_features)
#             decision_logits = self.decision_net(reasoning_features)
        
#         # é€‰æ‹©æœ€ç›¸å…³çš„æ¨¡æ¿
#         analysis_idx = torch.argmax(analysis_logits[:, :len(self.analysis_templates)]).item()
#         reasoning_idx = torch.argmax(reasoning_logits[:, :len(self.reasoning_templates)]).item()
#         decision_idx = torch.argmax(decision_logits[:, :len(self.decision_templates)]).item()
        
#         # æ„å»ºæ¨ç†é“¾
#         observation = f"å½“å‰æˆåŠŸæ¦‚ç‡{success_prob:.2f}ï¼Œè§†è§‰ç½®ä¿¡åº¦{visual_conf:.2f}"
#         analysis = self.analysis_templates[analysis_idx]
#         reasoning = self.reasoning_templates[reasoning_idx]
#         decision = self.decision_templates[decision_idx]
#         prediction = self._generate_prediction(decision, success_prob)
        
#         reasoning_chain = [
#             f"è§‚å¯Ÿ: {observation}",
#             f"åˆ†æ: {analysis}",
#             f"æ¨ç†: {reasoning}",
#             f"å†³ç­–: {decision}",
#             f"é¢„æµ‹: {prediction}"
#         ]
        
#         return " -> ".join(reasoning_chain)
    
#     def _rule_reasoning(self, sensor_data: SensorData, cognitive_state: Dict) -> str:
#         """è§„åˆ™æ¨ç†"""
#         visual_conf = cognitive_state['visual_confidence'].item()
#         force_noise = cognitive_state['force_noise'].item()
#         success_prob = cognitive_state['success_prob'].item()
        
#         # åº”ç”¨è§„åˆ™
#         if self.risk_rules["high_risk"](visual_conf, force_noise, success_prob):
#             risk_assessment = "é«˜é£é™©"
#             analysis = "å¤šä¸ªä¼ æ„Ÿå™¨æŒ‡æ ‡å¼‚å¸¸ï¼Œæ‰§è¡Œé£é™©å¾ˆé«˜"
#             reasoning = "åŸºäºå®‰å…¨è€ƒè™‘å’Œä»»åŠ¡æˆåŠŸç‡"
#             decision = "ç«‹å³åœæ­¢æ“ä½œï¼Œé‡æ–°è¯„ä¼°"
#         elif self.risk_rules["medium_risk"](visual_conf, force_noise, success_prob):
#             risk_assessment = "ä¸­ç­‰é£é™©"
#             analysis = "éƒ¨åˆ†ä¼ æ„Ÿå™¨æŒ‡æ ‡è¶…å‡ºæ­£å¸¸èŒƒå›´"
#             reasoning = "éœ€è¦è°ƒæ•´ç­–ç•¥ä»¥æé«˜æˆåŠŸç‡"
#             decision = "é™ä½æ“ä½œé€Ÿåº¦ï¼Œå¢å¼ºæ„ŸçŸ¥"
#         else:
#             risk_assessment = "ä½é£é™©"
#             analysis = "å„é¡¹æŒ‡æ ‡æ­£å¸¸ï¼Œæ‰§è¡ŒçŠ¶æ€è‰¯å¥½"
#             reasoning = "å½“å‰ç­–ç•¥æœ‰æ•ˆ"
#             decision = "ç»§ç»­å½“å‰æ“ä½œ"
        
#         observation = f"é£é™©è¯„ä¼°: {risk_assessment}, æˆåŠŸæ¦‚ç‡: {success_prob:.2f}"
#         prediction = f"é¢„æœŸè°ƒæ•´åæˆåŠŸç‡æå‡è‡³{min(success_prob + 0.2, 1.0):.2f}"
        
#         return f"è§‚å¯Ÿ: {observation} -> åˆ†æ: {analysis} -> æ¨ç†: {reasoning} -> å†³ç­–: {decision} -> é¢„æµ‹: {prediction}"
    
#     def _generate_prediction(self, decision: str, current_success_prob: float) -> str:
#         """ç”Ÿæˆé¢„æµ‹ç»“æœ"""
#         if "ä¸­æ­¢" in decision or "åœæ­¢" in decision:
#             return "é¢„æœŸé¿å…ä»»åŠ¡å¤±è´¥ï¼Œå®‰å…¨æ€§æå‡"
#         elif "è°ƒæ•´" in decision or "é™ä½" in decision:
#             new_prob = min(current_success_prob + 0.15, 1.0)
#             return f"é¢„æœŸæˆåŠŸç‡æå‡è‡³{new_prob:.2f}"
#         else:
#             return f"é¢„æœŸç»´æŒå½“å‰æˆåŠŸç‡{current_success_prob:.2f}"
    
#     def _combine_reasoning(self, neural_result: str, rule_result: str) -> str:
#         """æ··åˆæ¨ç†ç»“æœ"""
#         return f"ç¥ç»æ¨ç†: {neural_result} | è§„åˆ™æ¨ç†: {rule_result}"

# # ==================== è¯­ä¹‰è¾“å‡ºç”Ÿæˆå™¨ ====================

# class SemanticOutputGenerator:
#     """è¯­ä¹‰è¾“å‡ºç”Ÿæˆå™¨"""
#     def __init__(self):
#         self.directive_mapping = {
#             "ç»§ç»­": DirectiveType.CONTINUE,
#             "è°ƒæ•´": DirectiveType.ADJUST,
#             "é‡è¯•": DirectiveType.RETRY,
#             "åœæ­¢": DirectiveType.ABORT,
#             "åˆ‡æ¢": DirectiveType.SWITCH_MODE
#         }
    
#     def generate_output(self, reasoning_chain: str, cognitive_state: Dict) -> MetaCognitiveOutput:
#         """ç”Ÿæˆè¯­ä¹‰è¾“å‡º"""
#         directive = self._parse_directive(reasoning_chain)
#         parameters = self._generate_parameters(directive, cognitive_state)
#         confidence = self._calculate_confidence(cognitive_state)
#         urgency = self._determine_urgency(cognitive_state)
        
#         return MetaCognitiveOutput(
#             directive=directive,
#             reasoning=reasoning_chain.split(" -> ")[-2] if " -> " in reasoning_chain else reasoning_chain,
#             parameters=parameters,
#             confidence=confidence,
#             urgency=urgency
#         )
    
#     def _parse_directive(self, reasoning_chain: str) -> DirectiveType:
#         """è§£ææŒ‡ä»¤ç±»å‹"""
#         if "åœæ­¢" in reasoning_chain or "ä¸­æ­¢" in reasoning_chain:
#             return DirectiveType.ABORT
#         elif "è°ƒæ•´" in reasoning_chain:
#             return DirectiveType.ADJUST
#         elif "é‡è¯•" in reasoning_chain:
#             return DirectiveType.RETRY
#         elif "åˆ‡æ¢" in reasoning_chain:
#             return DirectiveType.SWITCH_MODE
#         else:
#             return DirectiveType.CONTINUE
    
#     def _generate_parameters(self, directive: DirectiveType, cognitive_state: Dict) -> Dict[str, Any]:
#         """ç”Ÿæˆå‚æ•°è°ƒæ•´å»ºè®®"""
#         params = {}
        
#         if directive == DirectiveType.ADJUST:
#             force_noise = cognitive_state['force_noise'].item()
#             if force_noise > 0.7:
#                 params['force_limit'] = 0.5
#                 params['approach_speed'] = 0.3
#             else:
#                 params['force_limit'] = 0.8
#                 params['approach_speed'] = 0.5
        
#         return params
    
#     def _calculate_confidence(self, cognitive_state: Dict) -> float:
#         """è®¡ç®—ç½®ä¿¡åº¦"""
#         visual_conf = cognitive_state['visual_confidence'].item()
#         success_prob = cognitive_state['success_prob'].item()
#         return (visual_conf + success_prob) / 2
    
#     def _determine_urgency(self, cognitive_state: Dict) -> str:
#         """ç¡®å®šç´§æ€¥ç¨‹åº¦"""
#         risk_level = torch.argmax(cognitive_state['risk']).item()
#         if risk_level >= 2:
#             return "high"
#         elif risk_level == 1:
#             return "medium"
#         else:
#             return "low"

# # ==================== å®Œæ•´çš„å…ƒè®¤çŸ¥æ¨¡å— ====================

# class CompleteMetaCognitiveModule:
#     """å®Œæ•´çš„å…ƒè®¤çŸ¥åé¦ˆæ¨¡å—ï¼ˆåŒ…å«æ‰€æœ‰nn.Moduleç»„ä»¶ï¼‰"""
#     def __init__(self, device="cuda" if torch.cuda.is_available() else "cpu"):
#         self.device = device
        
#         print(f"åˆå§‹åŒ–å…ƒè®¤çŸ¥æ¨¡å— (device: {device})")
        
#         # åˆå§‹åŒ–æ‰€æœ‰ç¥ç»ç½‘ç»œç»„ä»¶
#         self.visual_encoder = VisualEncoder().to(device)
#         self.force_encoder = ForceEncoder().to(device)
#         self.proprio_encoder = ProprioceptiveEncoder().to(device)
#         self.multimodal_fusion = MultimodalFusion().to(device)
#         self.cognitive_estimator = CognitiveStateEstimator().to(device)
        
#         # åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
#         self.cot_reasoner = CoTReasoner(device=device)
#         self.output_generator = SemanticOutputGenerator()
        
#         # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
#         self.set_eval_mode()
        
#         print("âœ… æ‰€æœ‰ç¥ç»ç½‘ç»œç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
#     def set_eval_mode(self):
#         """è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼"""
#         self.visual_encoder.eval()
#         self.force_encoder.eval()
#         self.proprio_encoder.eval()
#         self.multimodal_fusion.eval()
#         self.cognitive_estimator.eval()
        
#         # è®¾ç½®CoTæ¨ç†å™¨çš„ç¥ç»ç½‘ç»œä¸ºè¯„ä¼°æ¨¡å¼
#         if hasattr(self.cot_reasoner, 'reasoning_net'):
#             self.cot_reasoner.reasoning_net.eval()
#             self.cot_reasoner.analysis_net.eval()
#             self.cot_reasoner.reasoning_net_causal.eval()
#             self.cot_reasoner.decision_net.eval()
    
#     def set_train_mode(self):
#         """è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼"""
#         self.visual_encoder.train()
#         self.force_encoder.train()
#         self.proprio_encoder.train()
#         self.multimodal_fusion.train()
#         self.cognitive_estimator.train()
        
#         # è®¾ç½®CoTæ¨ç†å™¨çš„ç¥ç»ç½‘ç»œä¸ºè®­ç»ƒæ¨¡å¼
#         if hasattr(self.cot_reasoner, 'reasoning_net'):
#             self.cot_reasoner.reasoning_net.train()
#             self.cot_reasoner.analysis_net.train()
#             self.cot_reasoner.reasoning_net_causal.train()
#             self.cot_reasoner.decision_net.train()
    
#     def process_sensor_data(self, sensor_data: SensorData) -> MetaCognitiveOutput:
#         """å¤„ç†ä¼ æ„Ÿå™¨æ•°æ®ï¼Œç”Ÿæˆå…ƒè®¤çŸ¥è¾“å‡º"""
#         with torch.no_grad():
#             # 1. å¤šæ¨¡æ€ç¼–ç 
#             rgb_tensor = torch.FloatTensor(sensor_data.rgb_image).permute(2, 0, 1).unsqueeze(0).to(self.device)
#             depth_tensor = torch.FloatTensor(sensor_data.depth_image).unsqueeze(0).unsqueeze(0).to(self.device)
#             force_tensor = torch.FloatTensor(sensor_data.force_torque).unsqueeze(0).to(self.device)
#             joint_pos_tensor = torch.FloatTensor(sensor_data.joint_positions).unsqueeze(0).to(self.device)
#             joint_vel_tensor = torch.FloatTensor(sensor_data.joint_velocities).unsqueeze(0).to(self.device)
#             ee_pose_tensor = torch.FloatTensor(sensor_data.end_effector_pose).unsqueeze(0).to(self.device)
            
#             # ç‰¹å¾æå–
#             visual_features = self.visual_encoder(rgb_tensor, depth_tensor)
#             force_features = self.force_encoder(force_tensor)
#             proprio_features = self.proprio_encoder(joint_pos_tensor, joint_vel_tensor, ee_pose_tensor)
            
#             # 2. å¤šæ¨¡æ€èåˆ
#             fused_features = self.multimodal_fusion(visual_features, force_features, proprio_features)
            
#             # 3. è®¤çŸ¥çŠ¶æ€ä¼°è®¡
#             cognitive_state = self.cognitive_estimator(fused_features)
            
#             # 4. CoTæ¨ç†
#             reasoning_chain = self.cot_reasoner.reason(sensor_data, cognitive_state)
            
#             # 5. è¯­ä¹‰è¾“å‡ºç”Ÿæˆ
#             metacog_output = self.output_generator.generate_output(reasoning_chain, cognitive_state)
            
#             return metacog_output
    
#     def save_model(self, filepath: str):
#         """ä¿å­˜æ¨¡å‹"""
#         save_dict = {
#             'visual_encoder': self.visual_encoder.state_dict(),
#             'force_encoder': self.force_encoder.state_dict(),
#             'proprio_encoder': self.proprio_encoder.state_dict(),
#             'multimodal_fusion': self.multimodal_fusion.state_dict(),
#             'cognitive_estimator': self.cognitive_estimator.state_dict(),
#             'device': self.device
#         }
        
#         # ä¿å­˜CoTæ¨ç†å™¨çš„ç¥ç»ç½‘ç»œç»„ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
#         if hasattr(self.cot_reasoner, 'reasoning_net'):
#             save_dict.update({
#                 'cot_reasoning_net': self.cot_reasoner.reasoning_net.state_dict(),
#                 'cot_analysis_net': self.cot_reasoner.analysis_net.state_dict(),
#                 'cot_reasoning_net_causal': self.cot_reasoner.reasoning_net_causal.state_dict(),
#                 'cot_decision_net': self.cot_reasoner.decision_net.state_dict(),
#             })
        
#         torch.save(save_dict, filepath)
#         print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
#     def load_model(self, filepath: str):
#         """åŠ è½½æ¨¡å‹"""
#         checkpoint = torch.load(filepath, map_location=self.device)
        
#         self.visual_encoder.load_state_dict(checkpoint['visual_encoder'])
#         self.force_encoder.load_state_dict(checkpoint['force_encoder'])
#         self.proprio_encoder.load_state_dict(checkpoint['proprio_encoder'])
#         self.multimodal_fusion.load_state_dict(checkpoint['multimodal_fusion'])
#         self.cognitive_estimator.load_state_dict(checkpoint['cognitive_estimator'])
        
#         # åŠ è½½CoTæ¨ç†å™¨çš„ç¥ç»ç½‘ç»œç»„ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
#         if 'cot_reasoning_net' in checkpoint and hasattr(self.cot_reasoner, 'reasoning_net'):
#             self.cot_reasoner.reasoning_net.load_state_dict(checkpoint['cot_reasoning_net'])
#             self.cot_reasoner.analysis_net.load_state_dict(checkpoint['cot_analysis_net'])
#             self.cot_reasoner.reasoning_net_causal.load_state_dict(checkpoint['cot_reasoning_net_causal'])
#             self.cot_reasoner.decision_net.load_state_dict(checkpoint['cot_decision_net'])
        
#         print(f"æ¨¡å‹å·²ä» {filepath} åŠ è½½")

# # ==================== RoboCasaæ•°æ®é€‚é…å™¨ ====================

# class RoboCasaToMetacogAdapter:
#     """RoboCasaåˆ°å…ƒè®¤çŸ¥æ¨¡å—çš„æ•°æ®é€‚é…å™¨"""
    
#     def __init__(self, image_size: Tuple[int, int] = (224, 224)):
#         self.image_size = image_size
#         self.force_history = queue.deque(maxlen=5)
#         self.position_history = queue.deque(maxlen=5)
        
#     def convert_observation(self, robocasa_obs: Dict[str, np.ndarray], 
#                           action: np.ndarray, 
#                           execution_status: str = "normal") -> SensorData:
#         """å°†RoboCasaè§‚å¯Ÿè½¬æ¢ä¸ºSensorDataæ ¼å¼"""
        
#         # 1. å¤„ç†è§†è§‰æ•°æ®
#         rgb_image = self._extract_rgb_image(robocasa_obs)
#         depth_image = self._extract_depth_image(robocasa_obs)
        
#         # 2. å¤„ç†åŠ›è§‰æ•°æ®
#         force_torque = self._extract_force_data(robocasa_obs, action)
#         contact_detected = self._detect_contact(force_torque)
        
#         # 3. å¤„ç†æœ¬ä½“æ„Ÿè§‰æ•°æ®
#         joint_positions = robocasa_obs.get("robot0_joint_pos", np.zeros(7))
#         joint_velocities = robocasa_obs.get("robot0_joint_vel", np.zeros(7))
#         end_effector_pose = self._get_ee_pose(robocasa_obs)
        
#         # 4. ç³»ç»ŸçŠ¶æ€
#         system1_commands = action if action is not None else np.zeros(8)
        
#         return SensorData(
#             rgb_image=rgb_image,
#             depth_image=depth_image,
#             force_torque=force_torque,
#             contact_detected=contact_detected,
#             joint_positions=joint_positions,
#             joint_velocities=joint_velocities,
#             end_effector_pose=end_effector_pose,
#             system1_commands=system1_commands,
#             execution_status=execution_status,
#             timestamp=time.time()
#         )
    
#     def _extract_rgb_image(self, obs: Dict[str, np.ndarray]) -> np.ndarray:
#         """æå–RGBå›¾åƒ"""
#         for key in ["frontview_image", "robot0_eye_in_hand_image"]:
#             if key in obs:
#                 img = obs[key]
#                 if img.shape[:2] != self.image_size:
#                     img = cv2.resize(img, self.image_size)
#                 return img.astype(np.float32) / 255.0
        
#         return np.zeros((*self.image_size, 3), dtype=np.float32)
    
#     def _extract_depth_image(self, obs: Dict[str, np.ndarray]) -> np.ndarray:
#         """æå–æ·±åº¦å›¾åƒ"""
#         for key in ["frontview_depth", "robot0_eye_in_hand_depth"]:
#             if key in obs:
#                 depth = obs[key]
#                 if depth.shape != self.image_size:
#                     depth = cv2.resize(depth, self.image_size)
#                 return depth.astype(np.float32)
        
#         return np.ones(self.image_size, dtype=np.float32)
    
#     def _extract_force_data(self, obs: Dict[str, np.ndarray], action: np.ndarray) -> np.ndarray:
#         """æå–æˆ–ä¼°ç®—åŠ›è§‰æ•°æ®"""
#         if "force_torque" in obs:
#             force_data = obs["force_torque"]
#         else:
#             force_data = self._estimate_force_from_action(action)
        
#         if len(force_data) < 6:
#             force_data = np.pad(force_data, (0, 6 - len(force_data)))
        
#         self.force_history.append(force_data[:6])
#         return force_data[:6]
    
#     def _estimate_force_from_action(self, action: np.ndarray) -> np.ndarray:
#         """ä»åŠ¨ä½œä¼°ç®—åŠ›çŸ©"""
#         if action is None:
#             return np.zeros(6)
        
#         action_magnitude = np.linalg.norm(action)
#         estimated_force = np.random.normal(0, action_magnitude * 0.1, 6)
#         estimated_force = np.clip(estimated_force, -10, 10)
        
#         return estimated_force
    
#     def _detect_contact(self, force_torque: np.ndarray) -> bool:
#         """æ£€æµ‹æ¥è§¦"""
#         force_magnitude = np.linalg.norm(force_torque[:3])
#         return force_magnitude > 1.0
    
#     def _get_ee_pose(self, obs: Dict[str, np.ndarray]) -> np.ndarray:
#         """è·å–æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€"""
#         pose = []
        
#         if "robot0_eef_pos" in obs:
#             pose.extend(obs["robot0_eef_pos"])
#         else:
#             pose.extend([0.5, 0.0, 0.8])
        
#         if "robot0_eef_quat" in obs:
#             pose.extend(obs["robot0_eef_quat"])
#         else:
#             pose.extend([0, 0, 0, 1])
        
#         return np.array(pose)

# # ==================== gr00té€‚é…å™¨ ====================

# class MetacogToGR00TAdapter:
#     """å…ƒè®¤çŸ¥è¾“å‡ºåˆ°gr00tè°ƒæ•´çš„é€‚é…å™¨"""
    
#     def __init__(self):
#         self.adjustment_history = []
#         self.param_limits = {
#             "speed_multiplier": (0.1, 2.0),
#             "force_multiplier": (0.1, 2.0)
#         }
    
#     def convert_metacog_output(self, metacog_output: MetaCognitiveOutput) -> Dict[str, Any]:
#         """å°†å…ƒè®¤çŸ¥è¾“å‡ºè½¬æ¢ä¸ºgr00tè°ƒæ•´å‚æ•°"""
        
#         adjustment = {
#             "directive": metacog_output.directive.value,
#             "reasoning": metacog_output.reasoning,
#             "confidence": metacog_output.confidence,
#             "urgency": metacog_output.urgency,
#             "timestamp": time.time()
#         }
        
#         # æ ¹æ®æŒ‡ä»¤ç±»å‹ç”Ÿæˆå…·ä½“è°ƒæ•´
#         if metacog_output.directive == DirectiveType.ADJUST:
#             adjustment.update(self._generate_adjustment_params(metacog_output))
#         elif metacog_output.directive == DirectiveType.ABORT:
#             adjustment.update({"emergency_stop": True, "speed_multiplier": 0.0})
#         elif metacog_output.directive == DirectiveType.RETRY:
#             adjustment.update({"retry_action": True, "speed_multiplier": 0.3})
#         elif metacog_output.directive == DirectiveType.SWITCH_MODE:
#             adjustment.update({"switch_to_force_control": True, "force_multiplier": 0.5})
#         else:  # CONTINUE
#             adjustment.update({"maintain_current": True})
        
#         self.adjustment_history.append(adjustment)
#         return adjustment
    
#     def _generate_adjustment_params(self, metacog_output: MetaCognitiveOutput) -> Dict[str, Any]:
#         """ç”Ÿæˆè°ƒæ•´å‚æ•°"""
#         params = {}
#         metacog_params = metacog_output.parameters
        
#         if "force_limit" in metacog_params:
#             force_mult = min(metacog_params["force_limit"], 1.0)
#             params["force_multiplier"] = np.clip(force_mult, *self.param_limits["force_multiplier"])
        
#         if "approach_speed" in metacog_params:
#             speed_mult = metacog_params["approach_speed"]
#             params["speed_multiplier"] = np.clip(speed_mult, *self.param_limits["speed_multiplier"])
        
#         if metacog_output.confidence < 0.5:
#             params["precision_mode"] = True
#             params["speed_multiplier"] = 0.5
        
#         if metacog_output.urgency == "high":
#             params["safety_mode"] = True
#             params["force_multiplier"] = 0.3
        
#         return params

# # ==================== åŠ¨ä½œè°ƒæ•´å™¨ ====================

# class ActionAdjuster:
#     """æ ¹æ®å…ƒè®¤çŸ¥åé¦ˆè°ƒæ•´åŠ¨ä½œ"""
    
#     def __init__(self):
#         self.current_adjustments = {}
    
#     def apply_adjustments(self, original_action: Dict[str, np.ndarray], 
#                          adjustments: Dict[str, Any]) -> Dict[str, np.ndarray]:
#         """åº”ç”¨è°ƒæ•´åˆ°åŸå§‹åŠ¨ä½œ"""
#         adjusted_action = {}
        
#         for key, action_seq in original_action.items():
#             adjusted_seq = action_seq.copy()
            
#             # åº”ç”¨é€Ÿåº¦å€æ•°
#             if "speed_multiplier" in adjustments:
#                 adjusted_seq = adjusted_seq * adjustments["speed_multiplier"]
            
#             # åº”ç”¨åŠ›åº¦å€æ•°
#             if "force_multiplier" in adjustments and "arm" in key:
#                 adjusted_seq = adjusted_seq * adjustments["force_multiplier"]
            
#             # ç´§æ€¥åœæ­¢
#             if adjustments.get("emergency_stop", False):
#                 adjusted_seq = adjusted_seq * 0.0
            
#             # ç²¾ç¡®æ¨¡å¼
#             if adjustments.get("precision_mode", False):
#                 adjusted_seq = adjusted_seq * 0.5
            
#             # æ·»åŠ æ¢ç´¢å™ªå£°
#             if adjustments.get("add_exploration_noise", False):
#                 noise = np.random.normal(0, 0.05, adjusted_seq.shape)
#                 adjusted_seq = adjusted_seq + noise
            
#             adjusted_action[key] = adjusted_seq
        
#         return adjusted_action

# # ==================== æµ‹è¯•å’Œä½¿ç”¨ç¤ºä¾‹ ====================

# def test_complete_metacognitive_module():
#     """æµ‹è¯•å®Œæ•´çš„å…ƒè®¤çŸ¥æ¨¡å—"""
#     print("ğŸ§ª æµ‹è¯•å®Œæ•´çš„å…ƒè®¤çŸ¥æ¨¡å—...")
    
#     # åˆ›å»ºæ¨¡å—
#     device = "cuda" if torch.cuda.is_available() else "cpu"
#     metacog_module = CompleteMetaCognitiveModule(device=device)
    
#     # åˆ›å»ºæµ‹è¯•æ•°æ®
#     sensor_data = SensorData(
#         rgb_image=np.random.rand(224, 224, 3),
#         depth_image=np.random.rand(224, 224),
#         force_torque=np.random.rand(6),
#         contact_detected=True,
#         joint_positions=np.random.rand(7),
#         joint_velocities=np.random.rand(7),
#         end_effector_pose=np.random.rand(7),
#         system1_commands=np.random.rand(8),
#         execution_status="normal",
#         timestamp=time.time()
#     )
    
#     # å¤„ç†æ•°æ®
#     start_time = time.time()
#     output = metacog_module.process_sensor_data(sensor_data)
#     processing_time = time.time() - start_time
    
#     # è¾“å‡ºç»“æœ
#     print(f"âœ… å…ƒè®¤çŸ¥å¤„ç†å®Œæˆ")
#     print(f"  æŒ‡ä»¤: {output.directive.value}")
#     print(f"  æ¨ç†: {output.reasoning}")
#     print(f"  å‚æ•°: {output.parameters}")
#     print(f"  ç½®ä¿¡åº¦: {output.confidence:.3f}")
#     print(f"  ç´§æ€¥ç¨‹åº¦: {output.urgency}")
#     print(f"  å¤„ç†æ—¶é—´: {processing_time*1000:.1f}ms")
    
#     # æµ‹è¯•æ¨¡å‹ä¿å­˜/åŠ è½½
#     metacog_module.save_model("test_metacog_model.pth")
#     print("âœ… æ¨¡å‹ä¿å­˜æµ‹è¯•é€šè¿‡")
    
#     return True


# # ==================== å…ƒè®¤çŸ¥è¾“å‡ºåˆ°VLA System2çš„é€‚é…å™¨ ====================

# class MetacogToVLASystem2Adapter:
#     """å°†å…ƒè®¤çŸ¥æ¨¡å—çš„è¾“å‡ºè½¬æ¢ä¸ºç»™VLA System2çš„ç®€ç»ƒæŒ‡ä»¤"""

#     def __init__(self):
#         # é¢„å®šä¹‰ä¸€äº›å…³é”®è¯å’ŒçŸ­è¯­ï¼Œç”¨äºä»reasoningä¸­æå–ä¿¡æ¯
#         self.force_keywords = ["åŠ›", "force", "æ¥è§¦", "contact", "å¤¹æŒ", "grasp"]
#         self.position_keywords = ["ä½ç½®", "pose", "æ‰‹è‡‚", "arm", "å§¿æ€", "orientation"]
#         self.speed_keywords = ["é€Ÿåº¦", "speed", "ç§»åŠ¨", "move"]
#         self.visual_keywords = ["è§†è§‰", "visual", "å›¾åƒ", "image", "é®æŒ¡", "occlusion", "å…‰ç…§", "lighting"]

#         # æ›´å¤šå…³é”®è¯å¯ä»¥æ ¹æ®reasoningçš„å†…å®¹æ·»åŠ 
#         self.increase_keywords = ["åŠ å¤§", "å¢åŠ ", "æé«˜", "å¢å¼º", "increase", "enhance", "raise"]
#         self.decrease_keywords = ["å‡å°", "é™ä½", "å‡å¼±", "decrease", "reduce", "lower"]


#     def _extract_adjustment_detail(self, reasoning_text: str, param_value: Optional[float],
#                                    keywords: List[str], positive_verb: str, negative_verb: str,
#                                    threshold: Optional[float] = None) -> Optional[str]:
#         """è¾…åŠ©å‡½æ•°ï¼Œç”¨äºä»reasoningå’Œparametersä¸­æå–è°ƒæ•´ç»†èŠ‚"""
#         reasoning_lower = reasoning_text.lower()
#         found_keyword = any(kw in reasoning_lower for kw in keywords)

#         if not found_keyword and param_value is None:
#             return None

#         # ä¼˜å…ˆåŸºäºå‚æ•°å€¼ï¼ˆå¦‚æœæä¾›é˜ˆå€¼ï¼‰
#         if param_value is not None and threshold is not None:
#             if param_value < threshold:
#                 return negative_verb
#             else: # >= threshold
#                 return positive_verb # æˆ–è€…æ˜¯ä¸€ä¸ªä¸­æ€§/å¢å¼ºçš„è¯

#         # å¦‚æœæ²¡æœ‰å‚æ•°æˆ–é˜ˆå€¼ï¼Œæˆ–å‚æ•°ä¸é€‚ç”¨ï¼Œåˆ™å°è¯•ä»reasoningæ–‡æœ¬ä¸­åˆ¤æ–­
#         if found_keyword:
#             if any(kw in reasoning_lower for kw in self.increase_keywords):
#                 return positive_verb
#             if any(kw in reasoning_lower for kw in self.decrease_keywords):
#                 return negative_verb
#             # å¦‚æœåªæ‰¾åˆ°æ ¸å¿ƒå…³é”®è¯ä½†æ²¡æœ‰æ˜ç¡®å¢å‡ï¼Œå¯ä»¥è¿”å›ä¸€ä¸ªé€šç”¨æç¤ºæˆ–None
#             # return f"æ£€æŸ¥{keywords[0]}" # ä¾‹å¦‚
#         return None


#     def convert_to_system2_instruction(self, metacog_output: MetaCognitiveOutput) -> Optional[str]:
#         directive = metacog_output.directive
#         # ä½¿ç”¨CoTçš„"æ¨ç†"éƒ¨åˆ†ä½œä¸ºè¯­ä¹‰åé¦ˆçš„æ¥æºï¼Œå®ƒé€šå¸¸æ›´å…·åˆ†ææ€§
#         # æ‚¨ä»£ç ä¸­ MetaCognitiveOutput.reasoning å–çš„æ˜¯CoTçš„å€’æ•°ç¬¬äºŒéƒ¨åˆ†ï¼Œå³"å†³ç­–"
#         # è¿™é‡Œæˆ‘ä»¬å‡è®¾CoTé“¾æ¡æ˜¯ï¼š"è§‚å¯Ÿ -> åˆ†æ -> æ¨ç† -> å†³ç­– -> é¢„æµ‹"
#         # å¦‚æœ metacog_output.reasoning å·²ç»æ˜¯CoTçš„"æ¨ç†"éƒ¨åˆ†ï¼Œåˆ™æ— éœ€ä¿®æ”¹
#         # å¦åˆ™ï¼Œæ‚¨å¯èƒ½éœ€è¦ä¸€ç§æ–¹å¼ä»å®Œæ•´çš„CoTé“¾æ¡ä¸­æå–"æ¨ç†"éƒ¨åˆ†
#         # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å‡è®¾ metacog_output.reasoning å·²ç»æ˜¯æˆ‘ä»¬æƒ³è¦çš„æ–‡æœ¬
#         reasoning_text = metacog_output.reasoning
#         params = metacog_output.parameters

#         core_instruction = ""
#         adjustment_details = []

#         # 1. ç¡®å®šæ ¸å¿ƒæŒ‡ä»¤
#         if directive == DirectiveType.RETRY:
#             core_instruction = "ä»»åŠ¡å¼‚å¸¸ï¼Œå»ºè®®é‡æ–°æ‰§è¡Œ" # å¯ä»¥æ ¹æ®reasoningç»†åŒ–"ä»»åŠ¡å¼‚å¸¸"çš„åŸå› 
#             # å°è¯•ä»reasoningä¸­æå–å¤±è´¥åŸå› çš„å…³é”®è¯
#             if any(kw in reasoning_text.lower() for kw in self.visual_keywords):
#                 core_instruction = "è§†è§‰æ„ŸçŸ¥é—®é¢˜ï¼Œå»ºè®®é‡æ–°æ‰§è¡Œ"
#             elif any(kw in reasoning_text.lower() for kw in self.force_keywords):
#                 core_instruction = "åŠ›åé¦ˆå¼‚å¸¸ï¼Œå»ºè®®é‡æ–°æ‰§è¡Œ"

#         elif directive == DirectiveType.ADJUST:
#             core_instruction = "å»ºè®®è°ƒæ•´å½“å‰ç­–ç•¥"

#         elif directive == DirectiveType.ABORT:
#             return "æ£€æµ‹åˆ°é«˜é£é™©ï¼Œå»ºè®®ä¸­æ­¢å½“å‰ä»»åŠ¡" # ä¸­æ­¢æŒ‡ä»¤é€šå¸¸æ¯”è¾ƒç›´æ¥

#         elif directive == DirectiveType.SWITCH_MODE:
#             # ä»reasoningä¸­æå–è¦åˆ‡æ¢åˆ°çš„æ¨¡å¼
#             if "åŠ›" in reasoning_text or "force" in reasoning_text:
#                 core_instruction = "å»ºè®®åˆ‡æ¢åˆ°åŠ›æ§æ¨¡å¼"
#             elif "è§†è§‰" in reasoning_text or "visual" in reasoning_text:
#                  core_instruction = "å»ºè®®åˆ‡æ¢åˆ°è§†è§‰ä¼ºæœæ¨¡å¼"
#             else:
#                 core_instruction = "å»ºè®®åˆ‡æ¢æ“ä½œæ¨¡å¼"


#         elif directive == DirectiveType.CONTINUE:
#             return None # æ— éœ€åé¦ˆæŒ‡ä»¤
#         else:
#             return None # å…¶ä»–æœªå¤„ç†çš„æŒ‡ä»¤

#         # 2. æå–è°ƒæ•´ç»†èŠ‚ (ä¸»è¦é’ˆå¯¹ADJUSTå’ŒRETRY)
#         if directive == DirectiveType.ADJUST or directive == DirectiveType.RETRY:
#             # åŠ›çŸ©è°ƒæ•´
#             force_adj = self._extract_adjustment_detail(
#                 reasoning_text, params.get("force_limit"), self.force_keywords,
#                 "é€‚å½“å¢å¤§åŠ›çŸ©", "é€‚å½“å‡å°åŠ›çŸ©", threshold=0.7 # å‡è®¾å‚æ•°å€¼æ˜¯å½’ä¸€åŒ–çš„
#             )
#             if force_adj: adjustment_details.append(force_adj)

#             # ä½ç½®/å§¿æ€è°ƒæ•´
#             # è¿™ä¸ªæ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦æ›´ç²¾ç»†çš„reasoningè§£ææˆ–æ›´ç»“æ„åŒ–çš„parameters
#             # ç®€å•ç¤ºä¾‹ï¼š
#             if "æ‰‹è‡‚" in reasoning_text and "æé«˜" in reasoning_text:
#                 adjustment_details.append("å°†æ‰‹è‡‚çš„ä½ç½®é€‚å½“æé«˜")
#             elif "æ‰‹è‡‚" in reasoning_text and "é™ä½" in reasoning_text:
#                 adjustment_details.append("å°†æ‰‹è‡‚çš„ä½ç½®é€‚å½“é™ä½")

#             # é€Ÿåº¦è°ƒæ•´
#             speed_adj = self._extract_adjustment_detail(
#                 reasoning_text, params.get("approach_speed"), self.speed_keywords,
#                 "é€‚å½“åŠ å¿«é€Ÿåº¦", "é€‚å½“å‡æ…¢é€Ÿåº¦", threshold=0.6 # å‡è®¾å‚æ•°å€¼æ˜¯å½’ä¸€åŒ–çš„
#             )
#             if speed_adj: adjustment_details.append(speed_adj)

#         # 3. ç»„åˆæŒ‡ä»¤
#         if not core_instruction: # å¦‚æœæ²¡æœ‰æ ¸å¿ƒæŒ‡ä»¤ï¼ˆä¾‹å¦‚ï¼ŒCONTINUEå·²ç»è¢«è¿‡æ»¤ï¼‰
#             return None

#         if adjustment_details:
#             return f"{core_instruction}ï¼š{'ï¼Œ'.join(adjustment_details)}"
#         else:
#             return core_instruction


# if __name__ == "__main__":
#     # è¿è¡Œæµ‹è¯•
#     test_complete_metacognitive_module()
    
#     print("\nğŸ‰ å®Œæ•´çš„å…ƒè®¤çŸ¥æ¨¡å—æµ‹è¯•å®Œæˆï¼")
#     print("æ‰€æœ‰nn.Moduleç»„ä»¶éƒ½å·²åŒ…å«å¹¶æ­£å¸¸å·¥ä½œã€‚")
#     print("\nç°åœ¨æ‚¨å¯ä»¥å°†æ­¤æ¨¡å—ä¸gr00tå’ŒRoboCasaé›†æˆä½¿ç”¨ï¼š")
#     print("1. å¯¼å…¥ CompleteMetaCognitiveModule")
#     print("2. ä½¿ç”¨ RoboCasaToMetacogAdapter è½¬æ¢æ•°æ®")
#     print("3. ä½¿ç”¨ MetacogToGR00TAdapter è½¬æ¢è¾“å‡º")
#     print("4. ä½¿ç”¨ ActionAdjuster è°ƒæ•´åŠ¨ä½œ")


#!/usr/bin/env python3
"""
ä¿®å¤çš„å…ƒè®¤çŸ¥æ¨¡å— - é€‚é…GR00Tæ•°æ®ç»“æ„
ä¸»è¦ä¿®å¤ï¼šå…³èŠ‚ç»´åº¦ä»5ç»´æ”¹ä¸º7ç»´ï¼Œé€‚é…GR00Tçš„å®é™…æ•°æ®ç»“æ„
"""

import numpy as np
import torch
import torch.nn as nn
import time
import json
import threading
import queue
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import cv2
from enum import Enum

# ==================== æ ¸å¿ƒæ•°æ®ç»“æ„å®šä¹‰ ====================

class DirectiveType(Enum):
    """æŒ‡ä»¤ç±»å‹æšä¸¾"""
    CONTINUE = "continue"
    ADJUST = "adjust"
    RETRY = "retry"
    ABORT = "abort"
    SWITCH_MODE = "switch_mode"

class TaskPhase(Enum):
    """ä»»åŠ¡æ‰§è¡Œé˜¶æ®µ"""
    APPROACH = "approach"
    CONTACT = "contact"
    MANIPULATION = "manipulation"
    COMPLETE = "complete"

@dataclass
class SensorData:
    """ä¼ æ„Ÿå™¨æ•°æ®ç»“æ„"""
    rgb_image: np.ndarray
    depth_image: np.ndarray
    force_torque: np.ndarray
    contact_detected: bool
    joint_positions: np.ndarray
    joint_velocities: np.ndarray
    end_effector_pose: np.ndarray
    system1_commands: np.ndarray
    execution_status: str
    timestamp: float

@dataclass
class CognitiveState:
    """è®¤çŸ¥çŠ¶æ€è¡¨ç¤º"""
    lighting_condition: str
    scene_complexity: float
    occlusion_level: float
    current_phase: TaskPhase
    success_probability: float
    risk_level: str
    visual_confidence: float
    force_noise_level: float
    position_accuracy: float

@dataclass
class MetaCognitiveOutput:
    """å…ƒè®¤çŸ¥è¾“å‡ºç»“æ„"""
    directive: DirectiveType
    reasoning: str
    parameters: Dict[str, Any]
    confidence: float
    urgency: str

# ==================== ä¿®å¤çš„ç¥ç»ç½‘ç»œæ¨¡å— ====================

class VisualEncoder(nn.Module):
    """è§†è§‰ç¼–ç å™¨ - æ— éœ€ä¿®æ”¹"""
    def __init__(self, input_channels=4, hidden_dim=512):
        super().__init__()
        self.rgb_encoder = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.AdaptiveAvgPool2d((8, 8)),
            nn.Flatten(),
            nn.Linear(128 * 8 * 8, hidden_dim)
        )
        
        self.depth_encoder = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((8, 8)),
            nn.Flatten(),
            nn.Linear(64 * 8 * 8, hidden_dim // 2)
        )
        
        self.fusion = nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim)
    
    def forward(self, rgb_image, depth_image):
        rgb_features = self.rgb_encoder(rgb_image)
        depth_features = self.depth_encoder(depth_image)
        combined = torch.cat([rgb_features, depth_features], dim=-1)
        return self.fusion(combined)

class ForceEncoder(nn.Module):
    """åŠ›è§‰ç¼–ç å™¨ - æ— éœ€ä¿®æ”¹"""
    def __init__(self, force_dim=6, hidden_dim=128):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(force_dim, 64),
            nn.ReLU(),
            nn.Linear(64, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
    
    def forward(self, force_torque):
        return self.encoder(force_torque)

class ProprioceptiveEncoder(nn.Module):
    """æœ¬ä½“æ„Ÿè§‰ç¼–ç å™¨ - ä¿®å¤ï¼šé€‚é…GR00Tçš„7ç»´å…³èŠ‚æ•°æ®"""
    def __init__(self, joint_dim=7, pose_dim=7, hidden_dim=128):  # ä¿®æ”¹ï¼šjoint_dim=7
        super().__init__()
        self.joint_encoder = nn.Linear(joint_dim * 2, hidden_dim // 2)  # 7*2=14ç»´
        self.pose_encoder = nn.Linear(pose_dim, hidden_dim // 2)
        self.fusion = nn.Linear(hidden_dim, hidden_dim)
    
    def forward(self, joint_positions, joint_velocities, end_effector_pose):
        joint_input = torch.cat([joint_positions, joint_velocities], dim=-1)
        joint_features = self.joint_encoder(joint_input)
        pose_features = self.pose_encoder(end_effector_pose)
        combined = torch.cat([joint_features, pose_features], dim=-1)
        return self.fusion(combined)

class MultimodalFusion(nn.Module):
    """å¤šæ¨¡æ€èåˆç½‘ç»œ - æ— éœ€ä¿®æ”¹"""
    def __init__(self, visual_dim=512, force_dim=128, proprio_dim=128, output_dim=256):
        super().__init__()
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=output_dim, num_heads=8, batch_first=True
        )
        
        self.visual_proj = nn.Linear(visual_dim, output_dim)
        self.force_proj = nn.Linear(force_dim, output_dim)
        self.proprio_proj = nn.Linear(proprio_dim, output_dim)
        
        self.norm = nn.LayerNorm(output_dim)
        self.output_proj = nn.Linear(output_dim * 3, output_dim)
    
    def forward(self, visual_features, force_features, proprio_features):
        # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        visual_proj = self.visual_proj(visual_features).unsqueeze(1)
        force_proj = self.force_proj(force_features).unsqueeze(1)
        proprio_proj = self.proprio_proj(proprio_features).unsqueeze(1)
        
        # æ‹¼æ¥ä¸ºåºåˆ—
        multimodal_seq = torch.cat([visual_proj, force_proj, proprio_proj], dim=1)
        
        # äº¤å‰æ³¨æ„åŠ›èåˆ
        attended, _ = self.cross_attention(multimodal_seq, multimodal_seq, multimodal_seq)
        attended = self.norm(attended)
        
        # è¾“å‡ºèåˆ
        fused = attended.flatten(start_dim=1)
        return self.output_proj(fused)

class CognitiveStateEstimator(nn.Module):
    """è®¤çŸ¥çŠ¶æ€ä¼°è®¡å™¨ - æ— éœ€ä¿®æ”¹"""
    def __init__(self, input_dim=256, hidden_dim=128):
        super().__init__()
        self.state_network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # ç¯å¢ƒè¯„ä¼°åˆ†æ”¯
        self.env_lighting = nn.Linear(hidden_dim, 3)  # normal, bright, dim
        self.env_complexity = nn.Linear(hidden_dim, 1)  # 0-1
        self.env_occlusion = nn.Linear(hidden_dim, 1)   # 0-1
        
        # æ‰§è¡ŒçŠ¶æ€åˆ†æ”¯
        self.exec_phase = nn.Linear(hidden_dim, 4)      # 4ä¸ªé˜¶æ®µ
        self.exec_success_prob = nn.Linear(hidden_dim, 1)  # 0-1
        self.exec_risk = nn.Linear(hidden_dim, 3)       # low, medium, high
        
        # ä¼ æ„Ÿå™¨å¯é æ€§åˆ†æ”¯
        self.sensor_visual = nn.Linear(hidden_dim, 1)   # 0-1
        self.sensor_force = nn.Linear(hidden_dim, 1)    # 0-1
        self.sensor_position = nn.Linear(hidden_dim, 1) # 0-1
    
    def forward(self, fused_features):
        hidden = self.state_network(fused_features)
        
        # ç¯å¢ƒè¯„ä¼°
        lighting = torch.softmax(self.env_lighting(hidden), dim=-1)
        complexity = torch.sigmoid(self.env_complexity(hidden))
        occlusion = torch.sigmoid(self.env_occlusion(hidden))
        
        # æ‰§è¡ŒçŠ¶æ€
        phase = torch.softmax(self.exec_phase(hidden), dim=-1)
        success_prob = torch.sigmoid(self.exec_success_prob(hidden))
        risk = torch.softmax(self.exec_risk(hidden), dim=-1)
        
        # ä¼ æ„Ÿå™¨å¯é æ€§
        visual_conf = torch.sigmoid(self.sensor_visual(hidden))
        force_noise = torch.sigmoid(self.sensor_force(hidden))
        position_acc = torch.sigmoid(self.sensor_position(hidden))
        
        return {
            'lighting': lighting,
            'complexity': complexity,
            'occlusion': occlusion,
            'phase': phase,
            'success_prob': success_prob,
            'risk': risk,
            'visual_confidence': visual_conf,
            'force_noise': force_noise,
            'position_accuracy': position_acc
        }

# ==================== ä¿®å¤çš„CoTæ¨ç†å¼•æ“ ====================

class CoTReasoner:
    """é“¾å¼æ€ç»´æ¨ç†å¼•æ“ - ä¿®å¤ä¼ æ„Ÿå™¨ç‰¹å¾ç»´åº¦"""
    def __init__(self, reasoning_mode="neural", model_name=None, device="cuda"):
        self.reasoning_mode = reasoning_mode
        self.model_name = model_name
        self.device = device
        
        if reasoning_mode == "neural":
            self._init_neural_reasoner()
        elif reasoning_mode == "rule":
            self._init_rule_engine()
        else:  # hybrid
            self._init_neural_reasoner()
            self._init_rule_engine()
    
    def _init_neural_reasoner(self):
        """åˆå§‹åŒ–ç¥ç»ç½‘ç»œæ¨ç†å™¨ - ä¿®å¤ä¼ æ„Ÿå™¨ç‰¹å¾ç»´åº¦"""
        # ä¿®å¤ï¼šè°ƒæ•´ä¼ æ„Ÿå™¨ç‰¹å¾ç»´åº¦ä»¥åŒ¹é…GR00Tæ•°æ®
        sensor_feature_dim = 10  # å¢åŠ ä¼ æ„Ÿå™¨ç‰¹å¾ç»´åº¦
        self.reasoning_net = nn.Sequential(
            nn.Linear(256 + sensor_feature_dim, 128),  # ä¿®å¤è¾“å…¥ç»´åº¦
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32)
        ).to(self.device)
        
        self.analysis_net = nn.Linear(32, 16).to(self.device)
        self.reasoning_net_causal = nn.Linear(32, 16).to(self.device)
        self.decision_net = nn.Linear(32, 8).to(self.device)
        
        # æ¨ç†æ¨¡æ¿
        self.analysis_templates = [
            "è§†è§‰ä¿¡æ¯ä¸è¶³ï¼Œå¯èƒ½å½±å“æ“ä½œç²¾åº¦",
            "æ¥è§¦åŠ›å¼‚å¸¸ï¼Œå¯èƒ½æŠ“å–å¤±è´¥", 
            "å…³èŠ‚ä½ç½®åå·®è¾ƒå¤§ï¼Œéœ€è¦é‡æ–°å®šä½",
            "æ‰§è¡Œé€Ÿåº¦è¿‡å¿«ï¼Œé£é™©è¾ƒé«˜",
            "ç¯å¢ƒå…‰ç…§æ¡ä»¶å·®ï¼Œå¢åŠ æ“ä½œéš¾åº¦"
        ]
        
        self.reasoning_templates = [
            "ç”±äºä¼ æ„Ÿå™¨å™ªå£°å¢åŠ ï¼Œå»ºè®®é™ä½æ“ä½œé€Ÿåº¦",
            "åŸºäºå½“å‰åŠ›åé¦ˆï¼Œéœ€è¦è°ƒæ•´å¤¹æŒåŠ›åº¦",
            "è€ƒè™‘åˆ°è§†è§‰é®æŒ¡ï¼Œåº”è¯¥åˆ‡æ¢åˆ°åŠ›å¼•å¯¼æ¨¡å¼",
            "æ ¹æ®å…³èŠ‚çŠ¶æ€ï¼Œå»ºè®®é‡æ–°è§„åˆ’è½¨è¿¹"
        ]
        
        self.decision_templates = [
            "ç»§ç»­å½“å‰æ“ä½œï¼Œæ— éœ€è°ƒæ•´",
            "å¾®è°ƒåŠ›æ§å‚æ•°ï¼Œé™ä½æ¥è§¦åŠ›",
            "æš‚åœæ“ä½œï¼Œé‡æ–°è·å–è§†è§‰ä¿¡æ¯",
            "åˆ‡æ¢åˆ°åŒæ‰‹åè°ƒæ¨¡å¼",
            "ä¸­æ­¢å½“å‰ä»»åŠ¡ï¼Œè¿”å›å®‰å…¨å§¿æ€"
        ]
    
    def _init_rule_engine(self):
        """åˆå§‹åŒ–è§„åˆ™å¼•æ“"""
        self.risk_rules = {
            "high_risk": lambda v, f, s: v < 0.3 or f > 0.8 or s < 0.2,
            "medium_risk": lambda v, f, s: v < 0.6 or f > 0.5 or s < 0.5,
            "low_risk": lambda v, f, s: v >= 0.6 and f <= 0.5 and s >= 0.5
        }
        
        self.action_rules = {
            "force_too_high": lambda force: force > 10.0,
            "position_error": lambda error: np.linalg.norm(error) > 0.05,
            "visual_occlusion": lambda confidence: confidence < 0.4
        }
    
    def reason(self, sensor_data: SensorData, cognitive_state: Dict) -> str:
        """æ‰§è¡ŒCoTæ¨ç†"""
        if self.reasoning_mode == "neural":
            return self._neural_reasoning(sensor_data, cognitive_state)
        elif self.reasoning_mode == "rule":
            return self._rule_reasoning(sensor_data, cognitive_state)
        else:
            neural_result = self._neural_reasoning(sensor_data, cognitive_state)
            rule_result = self._rule_reasoning(sensor_data, cognitive_state)
            return self._combine_reasoning(neural_result, rule_result)
    
    def _neural_reasoning(self, sensor_data: SensorData, cognitive_state: Dict) -> str:
        """ç¥ç»ç½‘ç»œæ¨ç† - ä¿®å¤ä¼ æ„Ÿå™¨ç‰¹å¾æ„å»º"""
        # æ„å»ºè¾“å…¥ç‰¹å¾
        visual_conf = cognitive_state['visual_confidence'].item()
        force_noise = cognitive_state['force_noise'].item()
        success_prob = cognitive_state['success_prob'].item()
        risk_level = torch.argmax(cognitive_state['risk']).item()
        
        # ä¿®å¤ï¼šæ„å»º10ç»´ä¼ æ„Ÿå™¨çŠ¶æ€ç‰¹å¾ä»¥åŒ¹é…ç½‘ç»œæœŸæœ›
        sensor_features = torch.tensor([
            visual_conf, 
            force_noise, 
            success_prob, 
            risk_level,
            np.linalg.norm(sensor_data.force_torque),
            np.linalg.norm(sensor_data.joint_velocities),
            float(sensor_data.contact_detected),
            np.linalg.norm(sensor_data.joint_positions),  # æ–°å¢
            np.linalg.norm(sensor_data.end_effector_pose[:3]),  # æ–°å¢ï¼šä½ç½®å¹…åº¦
            np.linalg.norm(sensor_data.end_effector_pose[3:])   # æ–°å¢ï¼šå§¿æ€å¹…åº¦
        ], device=self.device, dtype=torch.float32).unsqueeze(0)
        
        # è®¤çŸ¥çŠ¶æ€ç‰¹å¾ - ä»GPUå¼ é‡ä¸­æå–å¹¶é‡æ–°ç»„åˆ
        cognitive_tensor_parts = [
            cognitive_state['visual_confidence'].flatten(),
            cognitive_state['force_noise'].flatten(), 
            cognitive_state['success_prob'].flatten(),
            cognitive_state['phase'].flatten(),
            cognitive_state['risk'].flatten()
        ]
        
        # è®¡ç®—å½“å‰ç‰¹å¾æ€»ç»´åº¦
        current_dim = sum(part.numel() for part in cognitive_tensor_parts)
        target_dim = 256
        
        # åˆ›å»ºå¡«å……å¼ é‡ï¼Œç¡®ä¿åœ¨ç›¸åŒè®¾å¤‡ä¸Š
        if current_dim < target_dim:
            padding = torch.zeros(target_dim - current_dim, device=self.device, dtype=torch.float32)
            cognitive_tensor_parts.append(padding)
        elif current_dim > target_dim:
            # å¦‚æœè¶…è¿‡ç›®æ ‡ç»´åº¦ï¼Œæˆªæ–­
            total_so_far = 0
            truncated_parts = []
            for part in cognitive_tensor_parts:
                if total_so_far + part.numel() <= target_dim:
                    truncated_parts.append(part)
                    total_so_far += part.numel()
                else:
                    remaining = target_dim - total_so_far
                    if remaining > 0:
                        truncated_parts.append(part.flatten()[:remaining])
                    break
            cognitive_tensor_parts = truncated_parts
        
        # åˆå¹¶è®¤çŸ¥çŠ¶æ€ç‰¹å¾
        cognitive_features = torch.cat(cognitive_tensor_parts, dim=0).unsqueeze(0)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        combined_features = torch.cat([cognitive_features, sensor_features], dim=-1)
        
        # æ¨ç†ç½‘ç»œå‰å‘ä¼ æ’­
        with torch.no_grad():
            reasoning_features = self.reasoning_net(combined_features)
            
            analysis_logits = self.analysis_net(reasoning_features)
            reasoning_logits = self.reasoning_net_causal(reasoning_features)
            decision_logits = self.decision_net(reasoning_features)
        
        # é€‰æ‹©æœ€ç›¸å…³çš„æ¨¡æ¿
        analysis_idx = torch.argmax(analysis_logits[:, :len(self.analysis_templates)]).item()
        reasoning_idx = torch.argmax(reasoning_logits[:, :len(self.reasoning_templates)]).item()
        decision_idx = torch.argmax(decision_logits[:, :len(self.decision_templates)]).item()
        
        # æ„å»ºæ¨ç†é“¾
        observation = f"å½“å‰æˆåŠŸæ¦‚ç‡{success_prob:.2f}ï¼Œè§†è§‰ç½®ä¿¡åº¦{visual_conf:.2f}"
        analysis = self.analysis_templates[analysis_idx]
        reasoning = self.reasoning_templates[reasoning_idx]
        decision = self.decision_templates[decision_idx]
        prediction = self._generate_prediction(decision, success_prob)
        
        reasoning_chain = [
            f"è§‚å¯Ÿ: {observation}",
            f"åˆ†æ: {analysis}",
            f"æ¨ç†: {reasoning}",
            f"å†³ç­–: {decision}",
            f"é¢„æµ‹: {prediction}"
        ]
        
        return " -> ".join(reasoning_chain)
    
    def _rule_reasoning(self, sensor_data: SensorData, cognitive_state: Dict) -> str:
        """è§„åˆ™æ¨ç†"""
        visual_conf = cognitive_state['visual_confidence'].item()
        force_noise = cognitive_state['force_noise'].item()
        success_prob = cognitive_state['success_prob'].item()
        
        # åº”ç”¨è§„åˆ™
        if self.risk_rules["high_risk"](visual_conf, force_noise, success_prob):
            risk_assessment = "é«˜é£é™©"
            analysis = "å¤šä¸ªä¼ æ„Ÿå™¨æŒ‡æ ‡å¼‚å¸¸ï¼Œæ‰§è¡Œé£é™©å¾ˆé«˜"
            reasoning = "åŸºäºå®‰å…¨è€ƒè™‘å’Œä»»åŠ¡æˆåŠŸç‡"
            decision = "ç«‹å³åœæ­¢æ“ä½œï¼Œé‡æ–°è¯„ä¼°"
        elif self.risk_rules["medium_risk"](visual_conf, force_noise, success_prob):
            risk_assessment = "ä¸­ç­‰é£é™©"
            analysis = "éƒ¨åˆ†ä¼ æ„Ÿå™¨æŒ‡æ ‡è¶…å‡ºæ­£å¸¸èŒƒå›´"
            reasoning = "éœ€è¦è°ƒæ•´ç­–ç•¥ä»¥æé«˜æˆåŠŸç‡"
            decision = "é™ä½æ“ä½œé€Ÿåº¦ï¼Œå¢å¼ºæ„ŸçŸ¥"
        else:
            risk_assessment = "ä½é£é™©"
            analysis = "å„é¡¹æŒ‡æ ‡æ­£å¸¸ï¼Œæ‰§è¡ŒçŠ¶æ€è‰¯å¥½"
            reasoning = "å½“å‰ç­–ç•¥æœ‰æ•ˆ"
            decision = "ç»§ç»­å½“å‰æ“ä½œ"
        
        observation = f"é£é™©è¯„ä¼°: {risk_assessment}, æˆåŠŸæ¦‚ç‡: {success_prob:.2f}"
        prediction = f"é¢„æœŸè°ƒæ•´åæˆåŠŸç‡æå‡è‡³{min(success_prob + 0.2, 1.0):.2f}"
        
        return f"è§‚å¯Ÿ: {observation} -> åˆ†æ: {analysis} -> æ¨ç†: {reasoning} -> å†³ç­–: {decision} -> é¢„æµ‹: {prediction}"
    
    def _generate_prediction(self, decision: str, current_success_prob: float) -> str:
        """ç”Ÿæˆé¢„æµ‹ç»“æœ"""
        if "ä¸­æ­¢" in decision or "åœæ­¢" in decision:
            return "é¢„æœŸé¿å…ä»»åŠ¡å¤±è´¥ï¼Œå®‰å…¨æ€§æå‡"
        elif "è°ƒæ•´" in decision or "é™ä½" in decision:
            new_prob = min(current_success_prob + 0.15, 1.0)
            return f"é¢„æœŸæˆåŠŸç‡æå‡è‡³{new_prob:.2f}"
        else:
            return f"é¢„æœŸç»´æŒå½“å‰æˆåŠŸç‡{current_success_prob:.2f}"
    
    def _combine_reasoning(self, neural_result: str, rule_result: str) -> str:
        """æ··åˆæ¨ç†ç»“æœ"""
        return f"ç¥ç»æ¨ç†: {neural_result} | è§„åˆ™æ¨ç†: {rule_result}"

# ==================== è¯­ä¹‰è¾“å‡ºç”Ÿæˆå™¨ - æ— éœ€ä¿®æ”¹ ====================

class SemanticOutputGenerator:
    """è¯­ä¹‰è¾“å‡ºç”Ÿæˆå™¨"""
    def __init__(self):
        self.directive_mapping = {
            "ç»§ç»­": DirectiveType.CONTINUE,
            "è°ƒæ•´": DirectiveType.ADJUST,
            "é‡è¯•": DirectiveType.RETRY,
            "åœæ­¢": DirectiveType.ABORT,
            "åˆ‡æ¢": DirectiveType.SWITCH_MODE
        }
    
    def generate_output(self, reasoning_chain: str, cognitive_state: Dict) -> MetaCognitiveOutput:
        """ç”Ÿæˆè¯­ä¹‰è¾“å‡º"""
        directive = self._parse_directive(reasoning_chain)
        parameters = self._generate_parameters(directive, cognitive_state)
        confidence = self._calculate_confidence(cognitive_state)
        urgency = self._determine_urgency(cognitive_state)
        
        return MetaCognitiveOutput(
            directive=directive,
            reasoning=reasoning_chain.split(" -> ")[-2] if " -> " in reasoning_chain else reasoning_chain,
            parameters=parameters,
            confidence=confidence,
            urgency=urgency
        )
    
    def _parse_directive(self, reasoning_chain: str) -> DirectiveType:
        """è§£ææŒ‡ä»¤ç±»å‹"""
        if "åœæ­¢" in reasoning_chain or "ä¸­æ­¢" in reasoning_chain:
            return DirectiveType.ABORT
        elif "è°ƒæ•´" in reasoning_chain:
            return DirectiveType.ADJUST
        elif "é‡è¯•" in reasoning_chain:
            return DirectiveType.RETRY
        elif "åˆ‡æ¢" in reasoning_chain:
            return DirectiveType.SWITCH_MODE
        else:
            return DirectiveType.CONTINUE
    
    def _generate_parameters(self, directive: DirectiveType, cognitive_state: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆå‚æ•°è°ƒæ•´å»ºè®®"""
        params = {}
        
        if directive == DirectiveType.ADJUST:
            force_noise = cognitive_state['force_noise'].item()
            if force_noise > 0.7:
                params['force_limit'] = 0.5
                params['approach_speed'] = 0.3
            else:
                params['force_limit'] = 0.8
                params['approach_speed'] = 0.5
        
        return params
    
    def _calculate_confidence(self, cognitive_state: Dict) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        visual_conf = cognitive_state['visual_confidence'].item()
        success_prob = cognitive_state['success_prob'].item()
        return (visual_conf + success_prob) / 2
    
    def _determine_urgency(self, cognitive_state: Dict) -> str:
        """ç¡®å®šç´§æ€¥ç¨‹åº¦"""
        risk_level = torch.argmax(cognitive_state['risk']).item()
        if risk_level >= 2:
            return "high"
        elif risk_level == 1:
            return "medium"
        else:
            return "low"

# ==================== ä¿®å¤çš„å®Œæ•´å…ƒè®¤çŸ¥æ¨¡å— ====================

class CompleteMetaCognitiveModule:
    """å®Œæ•´çš„å…ƒè®¤çŸ¥åé¦ˆæ¨¡å—ï¼ˆä¿®å¤GR00Tæ•°æ®é€‚é…ï¼‰"""
    def __init__(self, device="cuda" if torch.cuda.is_available() else "cpu"):
        self.device = device
        
        print(f"åˆå§‹åŒ–å…ƒè®¤çŸ¥æ¨¡å— (device: {device}) - é€‚é…GR00Tæ•°æ®ç»“æ„")
        
        # åˆå§‹åŒ–æ‰€æœ‰ç¥ç»ç½‘ç»œç»„ä»¶ - ä½¿ç”¨ä¿®å¤çš„ç»´åº¦
        self.visual_encoder = VisualEncoder().to(device)
        self.force_encoder = ForceEncoder().to(device)
        self.proprio_encoder = ProprioceptiveEncoder(joint_dim=7, pose_dim=7).to(device)  # ä¿®å¤ï¼š7ç»´å…³èŠ‚
        self.multimodal_fusion = MultimodalFusion().to(device)
        self.cognitive_estimator = CognitiveStateEstimator().to(device)
        
        # åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
        self.cot_reasoner = CoTReasoner(device=device)
        self.output_generator = SemanticOutputGenerator()
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.set_eval_mode()
        
        print("âœ… æ‰€æœ‰ç¥ç»ç½‘ç»œç»„ä»¶åˆå§‹åŒ–å®Œæˆ (é€‚é…GR00T 7ç»´å…³èŠ‚æ•°æ®)")
    
    def set_eval_mode(self):
        """è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼"""
        self.visual_encoder.eval()
        self.force_encoder.eval()
        self.proprio_encoder.eval()
        self.multimodal_fusion.eval()
        self.cognitive_estimator.eval()
        
        # è®¾ç½®CoTæ¨ç†å™¨çš„ç¥ç»ç½‘ç»œä¸ºè¯„ä¼°æ¨¡å¼
        if hasattr(self.cot_reasoner, 'reasoning_net'):
            self.cot_reasoner.reasoning_net.eval()
            self.cot_reasoner.analysis_net.eval()
            self.cot_reasoner.reasoning_net_causal.eval()
            self.cot_reasoner.decision_net.eval()
    
    def set_train_mode(self):
        """è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼"""
        self.visual_encoder.train()
        self.force_encoder.train()
        self.proprio_encoder.train()
        self.multimodal_fusion.train()
        self.cognitive_estimator.train()
        
        # è®¾ç½®CoTæ¨ç†å™¨çš„ç¥ç»ç½‘ç»œä¸ºè®­ç»ƒæ¨¡å¼
        if hasattr(self.cot_reasoner, 'reasoning_net'):
            self.cot_reasoner.reasoning_net.train()
            self.cot_reasoner.analysis_net.train()
            self.cot_reasoner.reasoning_net_causal.train()
            self.cot_reasoner.decision_net.train()
    
    def process_sensor_data(self, sensor_data: SensorData) -> MetaCognitiveOutput:
        """å¤„ç†ä¼ æ„Ÿå™¨æ•°æ®ï¼Œç”Ÿæˆå…ƒè®¤çŸ¥è¾“å‡º - è‡ªåŠ¨é€‚é…æ•°æ®ç»´åº¦"""
        with torch.no_grad():
            # è‡ªåŠ¨è°ƒæ•´æ•°æ®ç»´åº¦ä»¥åŒ¹é…GR00Tç»“æ„
            sensor_data = self._adapt_sensor_data_for_groot(sensor_data)
            
            # 1. å¤šæ¨¡æ€ç¼–ç 
            rgb_tensor = torch.FloatTensor(sensor_data.rgb_image).permute(2, 0, 1).unsqueeze(0).to(self.device)
            depth_tensor = torch.FloatTensor(sensor_data.depth_image).unsqueeze(0).unsqueeze(0).to(self.device)
            force_tensor = torch.FloatTensor(sensor_data.force_torque).unsqueeze(0).to(self.device)
            joint_pos_tensor = torch.FloatTensor(sensor_data.joint_positions).unsqueeze(0).to(self.device)
            joint_vel_tensor = torch.FloatTensor(sensor_data.joint_velocities).unsqueeze(0).to(self.device)
            ee_pose_tensor = torch.FloatTensor(sensor_data.end_effector_pose).unsqueeze(0).to(self.device)
            
            # ç‰¹å¾æå–
            visual_features = self.visual_encoder(rgb_tensor, depth_tensor)
            force_features = self.force_encoder(force_tensor)
            proprio_features = self.proprio_encoder(joint_pos_tensor, joint_vel_tensor, ee_pose_tensor)
            
            # 2. å¤šæ¨¡æ€èåˆ
            fused_features = self.multimodal_fusion(visual_features, force_features, proprio_features)
            
            # 3. è®¤çŸ¥çŠ¶æ€ä¼°è®¡
            cognitive_state = self.cognitive_estimator(fused_features)
            
            # 4. CoTæ¨ç†
            reasoning_chain = self.cot_reasoner.reason(sensor_data, cognitive_state)
            
            # 5. è¯­ä¹‰è¾“å‡ºç”Ÿæˆ
            metacog_output = self.output_generator.generate_output(reasoning_chain, cognitive_state)
            
            return metacog_output
    
    def _adapt_sensor_data_for_groot(self, sensor_data: SensorData) -> SensorData:
        """è‡ªåŠ¨é€‚é…ä¼ æ„Ÿå™¨æ•°æ®ä»¥åŒ¹é…GR00Tç»“æ„"""
        # ç¡®ä¿å…³èŠ‚æ•°æ®æ˜¯7ç»´
        if len(sensor_data.joint_positions) != 7:
            if len(sensor_data.joint_positions) > 7:
                sensor_data.joint_positions = sensor_data.joint_positions[:7]
            else:
                padded = np.zeros(7, dtype=np.float32)
                padded[:len(sensor_data.joint_positions)] = sensor_data.joint_positions
                sensor_data.joint_positions = padded
        
        if len(sensor_data.joint_velocities) != 7:
            if len(sensor_data.joint_velocities) > 7:
                sensor_data.joint_velocities = sensor_data.joint_velocities[:7]
            else:
                padded = np.zeros(7, dtype=np.float32)
                padded[:len(sensor_data.joint_velocities)] = sensor_data.joint_velocities
                sensor_data.joint_velocities = padded
        
        # ç¡®ä¿æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€æ˜¯7ç»´
        if len(sensor_data.end_effector_pose) != 7:
            padded = np.zeros(7, dtype=np.float32)
            if len(sensor_data.end_effector_pose) >= 3:
                padded[:3] = sensor_data.end_effector_pose[:3]  # ä½ç½®
            if len(sensor_data.end_effector_pose) >= 7:
                padded[3:7] = sensor_data.end_effector_pose[3:7]  # å››å…ƒæ•°
            else:
                padded[3:7] = [0, 0, 0, 1]  # é»˜è®¤å››å…ƒæ•°
            sensor_data.end_effector_pose = padded
        
        # ç¡®ä¿åŠ›çŸ©æ•°æ®æ˜¯6ç»´
        if len(sensor_data.force_torque) != 6:
            padded = np.zeros(6, dtype=np.float32)
            copy_len = min(len(sensor_data.force_torque), 6)
            padded[:copy_len] = sensor_data.force_torque[:copy_len]
            sensor_data.force_torque = padded
        
        return sensor_data
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        save_dict = {
            'visual_encoder': self.visual_encoder.state_dict(),
            'force_encoder': self.force_encoder.state_dict(),
            'proprio_encoder': self.proprio_encoder.state_dict(),
            'multimodal_fusion': self.multimodal_fusion.state_dict(),
            'cognitive_estimator': self.cognitive_estimator.state_dict(),
            'device': self.device,
            'version': 'groot_adapted'  # æ ‡è®°ä¸ºé€‚é…GR00Tç‰ˆæœ¬
        }
        
        # ä¿å­˜CoTæ¨ç†å™¨çš„ç¥ç»ç½‘ç»œç»„ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(self.cot_reasoner, 'reasoning_net'):
            save_dict.update({
                'cot_reasoning_net': self.cot_reasoner.reasoning_net.state_dict(),
                'cot_analysis_net': self.cot_reasoner.analysis_net.state_dict(),
                'cot_reasoning_net_causal': self.cot_reasoner.reasoning_net_causal.state_dict(),
                'cot_decision_net': self.cot_reasoner.decision_net.state_dict(),
            })
        
        torch.save(save_dict, filepath)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath} (GR00Té€‚é…ç‰ˆæœ¬)")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.visual_encoder.load_state_dict(checkpoint['visual_encoder'])
        self.force_encoder.load_state_dict(checkpoint['force_encoder'])
        self.proprio_encoder.load_state_dict(checkpoint['proprio_encoder'])
        self.multimodal_fusion.load_state_dict(checkpoint['multimodal_fusion'])
        self.cognitive_estimator.load_state_dict(checkpoint['cognitive_estimator'])
        
        # åŠ è½½CoTæ¨ç†å™¨çš„ç¥ç»ç½‘ç»œç»„ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'cot_reasoning_net' in checkpoint and hasattr(self.cot_reasoner, 'reasoning_net'):
            self.cot_reasoner.reasoning_net.load_state_dict(checkpoint['cot_reasoning_net'])
            self.cot_reasoner.analysis_net.load_state_dict(checkpoint['cot_analysis_net'])
            self.cot_reasoner.reasoning_net_causal.load_state_dict(checkpoint['cot_reasoning_net_causal'])
            self.cot_reasoner.decision_net.load_state_dict(checkpoint['cot_decision_net'])
        
        version = checkpoint.get('version', 'unknown')
        print(f"æ¨¡å‹å·²ä» {filepath} åŠ è½½ (ç‰ˆæœ¬: {version})")

# ==================== å…¶ä»–é€‚é…å™¨ä¿æŒä¸å˜ ====================

class RoboCasaToMetacogAdapter:
    """RoboCasaåˆ°å…ƒè®¤çŸ¥æ¨¡å—çš„æ•°æ®é€‚é…å™¨"""
    
    def __init__(self, image_size: Tuple[int, int] = (224, 224)):
        self.image_size = image_size
        self.force_history = queue.deque(maxlen=5)
        self.position_history = queue.deque(maxlen=5)
        
    def convert_observation(self, robocasa_obs: Dict[str, np.ndarray], 
                          action: np.ndarray, 
                          execution_status: str = "normal") -> SensorData:
        """å°†RoboCasaè§‚å¯Ÿè½¬æ¢ä¸ºSensorDataæ ¼å¼"""
        
        # 1. å¤„ç†è§†è§‰æ•°æ®
        rgb_image = self._extract_rgb_image(robocasa_obs)
        depth_image = self._extract_depth_image(robocasa_obs)
        
        # 2. å¤„ç†åŠ›è§‰æ•°æ®
        force_torque = self._extract_force_data(robocasa_obs, action)
        contact_detected = self._detect_contact(force_torque)
        
        # 3. å¤„ç†æœ¬ä½“æ„Ÿè§‰æ•°æ® - æ— éœ€ä¿®æ”¹ï¼Œå› ä¸ºå…ƒè®¤çŸ¥æ¨¡å—ä¼šè‡ªåŠ¨é€‚é…
        joint_positions = robocasa_obs.get("robot0_joint_pos", np.zeros(7))
        joint_velocities = robocasa_obs.get("robot0_joint_vel", np.zeros(7))
        end_effector_pose = self._get_ee_pose(robocasa_obs)
        
        # 4. ç³»ç»ŸçŠ¶æ€
        system1_commands = action if action is not None else np.zeros(8)
        
        return SensorData(
            rgb_image=rgb_image,
            depth_image=depth_image,
            force_torque=force_torque,
            contact_detected=contact_detected,
            joint_positions=joint_positions,
            joint_velocities=joint_velocities,
            end_effector_pose=end_effector_pose,
            system1_commands=system1_commands,
            execution_status=execution_status,
            timestamp=time.time()
        )
    
    def _extract_rgb_image(self, obs: Dict[str, np.ndarray]) -> np.ndarray:
        """æå–RGBå›¾åƒ"""
        for key in ["frontview_image", "robot0_eye_in_hand_image"]:
            if key in obs:
                img = obs[key]
                if img.shape[:2] != self.image_size:
                    img = cv2.resize(img, self.image_size)
                return img.astype(np.float32) / 255.0
        
        return np.zeros((*self.image_size, 3), dtype=np.float32)
    
    def _extract_depth_image(self, obs: Dict[str, np.ndarray]) -> np.ndarray:
        """æå–æ·±åº¦å›¾åƒ"""
        for key in ["frontview_depth", "robot0_eye_in_hand_depth"]:
            if key in obs:
                depth = obs[key]
                if depth.shape != self.image_size:
                    depth = cv2.resize(depth, self.image_size)
                return depth.astype(np.float32)
        
        return np.ones(self.image_size, dtype=np.float32)
    
    def _extract_force_data(self, obs: Dict[str, np.ndarray], action: np.ndarray) -> np.ndarray:
        """æå–æˆ–ä¼°ç®—åŠ›è§‰æ•°æ®"""
        if "force_torque" in obs:
            force_data = obs["force_torque"]
        else:
            force_data = self._estimate_force_from_action(action)
        
        if len(force_data) < 6:
            force_data = np.pad(force_data, (0, 6 - len(force_data)))
        
        self.force_history.append(force_data[:6])
        return force_data[:6]
    
    def _estimate_force_from_action(self, action: np.ndarray) -> np.ndarray:
        """ä»åŠ¨ä½œä¼°ç®—åŠ›çŸ©"""
        if action is None:
            return np.zeros(6)
        
        action_magnitude = np.linalg.norm(action)
        estimated_force = np.random.normal(0, action_magnitude * 0.1, 6)
        estimated_force = np.clip(estimated_force, -10, 10)
        
        return estimated_force
    
    def _detect_contact(self, force_torque: np.ndarray) -> bool:
        """æ£€æµ‹æ¥è§¦"""
        force_magnitude = np.linalg.norm(force_torque[:3])
        return force_magnitude > 1.0
    
    def _get_ee_pose(self, obs: Dict[str, np.ndarray]) -> np.ndarray:
        """è·å–æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€"""
        pose = []
        
        if "robot0_eef_pos" in obs:
            pose.extend(obs["robot0_eef_pos"])
        else:
            pose.extend([0.5, 0.0, 0.8])
        
        if "robot0_eef_quat" in obs:
            pose.extend(obs["robot0_eef_quat"])
        else:
            pose.extend([0, 0, 0, 1])
        
        return np.array(pose)

# ==================== VLA System2é€‚é…å™¨ - æ— éœ€ä¿®æ”¹ ====================

class MetacogToVLASystem2Adapter:
    """å°†å…ƒè®¤çŸ¥æ¨¡å—çš„è¾“å‡ºè½¬æ¢ä¸ºç»™VLA System2çš„ç®€ç»ƒæŒ‡ä»¤"""

    def __init__(self):
        # é¢„å®šä¹‰ä¸€äº›å…³é”®è¯å’ŒçŸ­è¯­ï¼Œç”¨äºä»reasoningä¸­æå–ä¿¡æ¯
        self.force_keywords = ["åŠ›", "force", "æ¥è§¦", "contact", "å¤¹æŒ", "grasp"]
        self.position_keywords = ["ä½ç½®", "pose", "æ‰‹è‡‚", "arm", "å§¿æ€", "orientation"]
        self.speed_keywords = ["é€Ÿåº¦", "speed", "ç§»åŠ¨", "move"]
        self.visual_keywords = ["è§†è§‰", "visual", "å›¾åƒ", "image", "é®æŒ¡", "occlusion", "å…‰ç…§", "lighting"]

        # æ›´å¤šå…³é”®è¯å¯ä»¥æ ¹æ®reasoningçš„å†…å®¹æ·»åŠ 
        self.increase_keywords = ["åŠ å¤§", "å¢åŠ ", "æé«˜", "å¢å¼º", "increase", "enhance", "raise"]
        self.decrease_keywords = ["å‡å°", "é™ä½", "å‡å¼±", "decrease", "reduce", "lower"]


    def _extract_adjustment_detail(self, reasoning_text: str, param_value: Optional[float],
                                   keywords: List[str], positive_verb: str, negative_verb: str,
                                   threshold: Optional[float] = None) -> Optional[str]:
        """è¾…åŠ©å‡½æ•°ï¼Œç”¨äºä»reasoningå’Œparametersä¸­æå–è°ƒæ•´ç»†èŠ‚"""
        reasoning_lower = reasoning_text.lower()
        found_keyword = any(kw in reasoning_lower for kw in keywords)

        if not found_keyword and param_value is None:
            return None

        # ä¼˜å…ˆåŸºäºå‚æ•°å€¼ï¼ˆå¦‚æœæä¾›é˜ˆå€¼ï¼‰
        if param_value is not None and threshold is not None:
            if param_value < threshold:
                return negative_verb
            else: # >= threshold
                return positive_verb # æˆ–è€…æ˜¯ä¸€ä¸ªä¸­æ€§/å¢å¼ºçš„è¯

        # å¦‚æœæ²¡æœ‰å‚æ•°æˆ–é˜ˆå€¼ï¼Œæˆ–å‚æ•°ä¸é€‚ç”¨ï¼Œåˆ™å°è¯•ä»reasoningæ–‡æœ¬ä¸­åˆ¤æ–­
        if found_keyword:
            if any(kw in reasoning_lower for kw in self.increase_keywords):
                return positive_verb
            if any(kw in reasoning_lower for kw in self.decrease_keywords):
                return negative_verb
            # å¦‚æœåªæ‰¾åˆ°æ ¸å¿ƒå…³é”®è¯ä½†æ²¡æœ‰æ˜ç¡®å¢å‡ï¼Œå¯ä»¥è¿”å›ä¸€ä¸ªé€šç”¨æç¤ºæˆ–None
            # return f"æ£€æŸ¥{keywords[0]}" # ä¾‹å¦‚
        return None


    def convert_to_system2_instruction(self, metacog_output: MetaCognitiveOutput) -> Optional[str]:
        directive = metacog_output.directive
        reasoning_text = metacog_output.reasoning
        params = metacog_output.parameters

        core_instruction = ""
        adjustment_details = []

        # 1. ç¡®å®šæ ¸å¿ƒæŒ‡ä»¤
        if directive == DirectiveType.RETRY:
            core_instruction = "ä»»åŠ¡å¼‚å¸¸ï¼Œå»ºè®®é‡æ–°æ‰§è¡Œ"
            if any(kw in reasoning_text.lower() for kw in self.visual_keywords):
                core_instruction = "è§†è§‰æ„ŸçŸ¥é—®é¢˜ï¼Œå»ºè®®é‡æ–°æ‰§è¡Œ"
            elif any(kw in reasoning_text.lower() for kw in self.force_keywords):
                core_instruction = "åŠ›åé¦ˆå¼‚å¸¸ï¼Œå»ºè®®é‡æ–°æ‰§è¡Œ"

        elif directive == DirectiveType.ADJUST:
            core_instruction = "å»ºè®®è°ƒæ•´å½“å‰ç­–ç•¥"

        elif directive == DirectiveType.ABORT:
            return "æ£€æµ‹åˆ°é«˜é£é™©ï¼Œå»ºè®®ä¸­æ­¢å½“å‰ä»»åŠ¡"

        elif directive == DirectiveType.SWITCH_MODE:
            if "åŠ›" in reasoning_text or "force" in reasoning_text:
                core_instruction = "å»ºè®®åˆ‡æ¢åˆ°åŠ›æ§æ¨¡å¼"
            elif "è§†è§‰" in reasoning_text or "visual" in reasoning_text:
                 core_instruction = "å»ºè®®åˆ‡æ¢åˆ°è§†è§‰ä¼ºæœæ¨¡å¼"
            else:
                core_instruction = "å»ºè®®åˆ‡æ¢æ“ä½œæ¨¡å¼"

        elif directive == DirectiveType.CONTINUE:
            return None
        else:
            return None

        # 2. æå–è°ƒæ•´ç»†èŠ‚ (ä¸»è¦é’ˆå¯¹ADJUSTå’ŒRETRY)
        if directive == DirectiveType.ADJUST or directive == DirectiveType.RETRY:
            # åŠ›çŸ©è°ƒæ•´
            force_adj = self._extract_adjustment_detail(
                reasoning_text, params.get("force_limit"), self.force_keywords,
                "é€‚å½“å¢å¤§åŠ›çŸ©", "é€‚å½“å‡å°åŠ›çŸ©", threshold=0.7
            )
            if force_adj: adjustment_details.append(force_adj)

            # ä½ç½®/å§¿æ€è°ƒæ•´
            if "æ‰‹è‡‚" in reasoning_text and "æé«˜" in reasoning_text:
                adjustment_details.append("å°†æ‰‹è‡‚çš„ä½ç½®é€‚å½“æé«˜")
            elif "æ‰‹è‡‚" in reasoning_text and "é™ä½" in reasoning_text:
                adjustment_details.append("å°†æ‰‹è‡‚çš„ä½ç½®é€‚å½“é™ä½")

            # é€Ÿåº¦è°ƒæ•´
            speed_adj = self._extract_adjustment_detail(
                reasoning_text, params.get("approach_speed"), self.speed_keywords,
                "é€‚å½“åŠ å¿«é€Ÿåº¦", "é€‚å½“å‡æ…¢é€Ÿåº¦", threshold=0.6
            )
            if speed_adj: adjustment_details.append(speed_adj)

        # 3. ç»„åˆæŒ‡ä»¤
        if not core_instruction:
            return None

        if adjustment_details:
            return f"{core_instruction}ï¼š{'ï¼Œ'.join(adjustment_details)}"
        else:
            return core_instruction

# ==================== å…¶ä»–é€‚é…å™¨ç±»ä¿æŒä¸å˜ ====================

class MetacogToGR00TAdapter:
    """å…ƒè®¤çŸ¥è¾“å‡ºåˆ°gr00tè°ƒæ•´çš„é€‚é…å™¨"""
    
    def __init__(self):
        self.adjustment_history = []
        self.param_limits = {
            "speed_multiplier": (0.1, 2.0),
            "force_multiplier": (0.1, 2.0)
        }
    
    def convert_metacog_output(self, metacog_output: MetaCognitiveOutput) -> Dict[str, Any]:
        """å°†å…ƒè®¤çŸ¥è¾“å‡ºè½¬æ¢ä¸ºgr00tè°ƒæ•´å‚æ•°"""
        
        adjustment = {
            "directive": metacog_output.directive.value,
            "reasoning": metacog_output.reasoning,
            "confidence": metacog_output.confidence,
            "urgency": metacog_output.urgency,
            "timestamp": time.time()
        }
        
        # æ ¹æ®æŒ‡ä»¤ç±»å‹ç”Ÿæˆå…·ä½“è°ƒæ•´
        if metacog_output.directive == DirectiveType.ADJUST:
            adjustment.update(self._generate_adjustment_params(metacog_output))
        elif metacog_output.directive == DirectiveType.ABORT:
            adjustment.update({"emergency_stop": True, "speed_multiplier": 0.0})
        elif metacog_output.directive == DirectiveType.RETRY:
            adjustment.update({"retry_action": True, "speed_multiplier": 0.3})
        elif metacog_output.directive == DirectiveType.SWITCH_MODE:
            adjustment.update({"switch_to_force_control": True, "force_multiplier": 0.5})
        else:  # CONTINUE
            adjustment.update({"maintain_current": True})
        
        self.adjustment_history.append(adjustment)
        return adjustment
    
    def _generate_adjustment_params(self, metacog_output: MetaCognitiveOutput) -> Dict[str, Any]:
        """ç”Ÿæˆè°ƒæ•´å‚æ•°"""
        params = {}
        metacog_params = metacog_output.parameters
        
        if "force_limit" in metacog_params:
            force_mult = min(metacog_params["force_limit"], 1.0)
            params["force_multiplier"] = np.clip(force_mult, *self.param_limits["force_multiplier"])
        
        if "approach_speed" in metacog_params:
            speed_mult = metacog_params["approach_speed"]
            params["speed_multiplier"] = np.clip(speed_mult, *self.param_limits["speed_multiplier"])
        
        if metacog_output.confidence < 0.5:
            params["precision_mode"] = True
            params["speed_multiplier"] = 0.5
        
        if metacog_output.urgency == "high":
            params["safety_mode"] = True
            params["force_multiplier"] = 0.3
        
        return params

class ActionAdjuster:
    """æ ¹æ®å…ƒè®¤çŸ¥åé¦ˆè°ƒæ•´åŠ¨ä½œ"""
    
    def __init__(self):
        self.current_adjustments = {}
    
    def apply_adjustments(self, original_action: Dict[str, np.ndarray], 
                         adjustments: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """åº”ç”¨è°ƒæ•´åˆ°åŸå§‹åŠ¨ä½œ"""
        adjusted_action = {}
        
        for key, action_seq in original_action.items():
            adjusted_seq = action_seq.copy()
            
            # åº”ç”¨é€Ÿåº¦å€æ•°
            if "speed_multiplier" in adjustments:
                adjusted_seq = adjusted_seq * adjustments["speed_multiplier"]
            
            # åº”ç”¨åŠ›åº¦å€æ•°
            if "force_multiplier" in adjustments and "arm" in key:
                adjusted_seq = adjusted_seq * adjustments["force_multiplier"]
            
            # ç´§æ€¥åœæ­¢
            if adjustments.get("emergency_stop", False):
                adjusted_seq = adjusted_seq * 0.0
            
            # ç²¾ç¡®æ¨¡å¼
            if adjustments.get("precision_mode", False):
                adjusted_seq = adjusted_seq * 0.5
            
            # æ·»åŠ æ¢ç´¢å™ªå£°
            if adjustments.get("add_exploration_noise", False):
                noise = np.random.normal(0, 0.05, adjusted_seq.shape)
                adjusted_seq = adjusted_seq + noise
            
            adjusted_action[key] = adjusted_seq
        
        return adjusted_action

# ==================== æµ‹è¯•å‡½æ•° ====================

def test_fixed_metacognitive_module():
    """æµ‹è¯•ä¿®å¤çš„å…ƒè®¤çŸ¥æ¨¡å—"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤çš„å…ƒè®¤çŸ¥æ¨¡å— (é€‚é…GR00T)...")
    
    # åˆ›å»ºæ¨¡å—
    device = "cuda" if torch.cuda.is_available() else "cpu"
    metacog_module = CompleteMetaCognitiveModule(device=device)
    
    # åˆ›å»ºGR00Tæ ¼å¼çš„æµ‹è¯•æ•°æ®
    sensor_data = SensorData(
        rgb_image=np.random.rand(224, 224, 3),
        depth_image=np.random.rand(224, 224),
        force_torque=np.random.rand(6),
        contact_detected=True,
        joint_positions=np.random.rand(7),  # GR00T: 7ç»´å…³èŠ‚
        joint_velocities=np.random.rand(7),  # GR00T: 7ç»´å…³èŠ‚
        end_effector_pose=np.random.rand(7),
        system1_commands=np.random.rand(8),
        execution_status="normal",
        timestamp=time.time()
    )
    
    # å¤„ç†æ•°æ®
    start_time = time.time()
    output = metacog_module.process_sensor_data(sensor_data)
    processing_time = time.time() - start_time
    
    # è¾“å‡ºç»“æœ
    print(f"âœ… å…ƒè®¤çŸ¥å¤„ç†å®Œæˆ (é€‚é…GR00T)")
    print(f"  æŒ‡ä»¤: {output.directive.value}")
    print(f"  æ¨ç†: {output.reasoning}")
    print(f"  å‚æ•°: {output.parameters}")
    print(f"  ç½®ä¿¡åº¦: {output.confidence:.3f}")
    print(f"  ç´§æ€¥ç¨‹åº¦: {output.urgency}")
    print(f"  å¤„ç†æ—¶é—´: {processing_time*1000:.1f}ms")
    
    # æµ‹è¯•VLA System2é€‚é…å™¨
    vla_adapter = MetacogToVLASystem2Adapter()
    instruction = vla_adapter.convert_to_system2_instruction(output)
    if instruction:
        print(f"  VLAæŒ‡ä»¤: {instruction}")
    
    # æµ‹è¯•æ¨¡å‹ä¿å­˜/åŠ è½½
    metacog_module.save_model("test_groot_adapted_model.pth")
    print("âœ… æ¨¡å‹ä¿å­˜æµ‹è¯•é€šè¿‡ (GR00Té€‚é…ç‰ˆæœ¬)")
    
    return True


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_fixed_metacognitive_module()
    
    print("\nğŸ‰ ä¿®å¤çš„å…ƒè®¤çŸ¥æ¨¡å—æµ‹è¯•å®Œæˆï¼")
    print("âœ… å…³é”®ä¿®å¤å†…å®¹:")
    print("   - ProprioceptiveEncoder: joint_dim 5â†’7 (é€‚é…GR00T)")
    print("   - CoTReasoner: ä¼ æ„Ÿå™¨ç‰¹å¾ç»´åº¦ 8â†’10")
    print("   - è‡ªåŠ¨æ•°æ®ç»´åº¦é€‚é…")
    print("   - ä¿æŒä¸GR00Tæ•°æ®ç»“æ„ä¸€è‡´")
    print("\nç°åœ¨åº”è¯¥ä¸å†å‡ºç°ç»´åº¦ä¸åŒ¹é…é”™è¯¯ï¼")