# #!/usr/bin/env python3
# """
# å®é™…é›†æˆæ¼”ç¤ºï¼šå°†æ‚¨çš„å…ƒè®¤çŸ¥æ¨¡å—ä¸gr00tå’ŒRoboCasaé›†æˆ
# Practical Integration Demo: Your Metacognitive Module + gr00t + RoboCasa
# """

# import sys
# import os
# import time
# import numpy as np
# import torch
# from pathlib import Path

# # æ·»åŠ Isaac GR00Tè·¯å¾„
# sys.path.append('/root/autodl-tmp/gr00t/Isaac-GR00T')

# # å¯¼å…¥æ‚¨åˆšåˆšæµ‹è¯•æˆåŠŸçš„å…ƒè®¤çŸ¥æ¨¡å—
# from metacog_integration import (
#     CompleteMetaCognitiveModule,
#     RoboCasaToMetacogAdapter, 
#     MetacogToGR00TAdapter,
#     ActionAdjuster,
#     SensorData,
#     DirectiveType
# )

# # ==================== å®é™…é›†æˆæ¼”ç¤ºç±» ====================

# class RealWorldIntegrationDemo:
#     """çœŸå®ä¸–ç•Œé›†æˆæ¼”ç¤º"""
    
#     def __init__(self):
#         print("ğŸš€ å¯åŠ¨å…ƒè®¤çŸ¥å¢å¼ºçš„æœºå™¨äººç³»ç»Ÿ...")
        
#         # æ£€æŸ¥ç¯å¢ƒ
#         self.check_environment()
        
#         # åˆå§‹åŒ–ç»„ä»¶
#         self.setup_components()
        
#         # æ€§èƒ½ç›‘æ§
#         self.stats = {
#             "episodes_run": 0,
#             "metacog_interventions": 0,
#             "avg_processing_time": 0.0,
#             "success_improvements": []
#         }
    
#     def check_environment(self):
#         """æ£€æŸ¥ç¯å¢ƒè®¾ç½®"""
#         print("ğŸ” æ£€æŸ¥ç¯å¢ƒè®¾ç½®...")
        
#         # æ£€æŸ¥CUDA
#         if torch.cuda.is_available():
#             print(f"âœ… CUDAå¯ç”¨: {torch.cuda.get_device_name()}")
#             print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
#         else:
#             print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
        
#         # æ£€æŸ¥gr00tç›¸å…³æ–‡ä»¶
#         groot_paths = [
#             "examples/inference/gr00t_inference.py",
#             "models/",
#             "configs/"
#         ]
        
#         for path in groot_paths:
#             if os.path.exists(path):
#                 print(f"âœ… æ‰¾åˆ°gr00tç»„ä»¶: {path}")
#             else:
#                 print(f"âš ï¸ æœªæ‰¾åˆ°: {path}")
        
#         # æ£€æŸ¥RoboCasa
#         try:
#             import robosuite
#             print("âœ… robosuiteå¯ç”¨")
#         except ImportError:
#             print("âš ï¸ robosuiteæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    
#     def setup_components(self):
#         """è®¾ç½®æ‰€æœ‰ç»„ä»¶"""
#         print("ğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")
        
#         # 1. å…ƒè®¤çŸ¥æ¨¡å—ï¼ˆæ‚¨çš„æ ¸å¿ƒç³»ç»Ÿï¼‰
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         self.metacog_module = CompleteMetaCognitiveModule(device=device)
#         print("âœ… å…ƒè®¤çŸ¥æ¨¡å—å°±ç»ª")
        
#         # 2. æ•°æ®é€‚é…å™¨
#         self.robocasa_adapter = RoboCasaToMetacogAdapter(image_size=(224, 224))
#         self.groot_adapter = MetacogToGR00TAdapter()
#         self.action_adjuster = ActionAdjuster()
#         print("âœ… æ•°æ®é€‚é…å™¨å°±ç»ª")
        
#         # 3. Mockç¯å¢ƒï¼ˆç”¨äºæ¼”ç¤ºï¼‰
#         self.env = self.create_mock_environment()
#         self.groot_policy = self.create_mock_groot_policy()
#         print("âœ… æ¨¡æ‹Ÿç¯å¢ƒå’Œç­–ç•¥å°±ç»ª")
    
#     def create_mock_environment(self):
#         """åˆ›å»ºæ¨¡æ‹Ÿç¯å¢ƒï¼ˆæ›¿ä»£RoboCasaï¼‰"""
#         class MockRoboCasaEnv:
#             def __init__(self):
#                 self.step_count = 0
#                 self.task_progress = 0.0
                
#             def reset(self):
#                 self.step_count = 0
#                 self.task_progress = 0.0
#                 return self._generate_observation()
            
#             def step(self, action):
#                 self.step_count += 1
#                 self.task_progress += np.random.uniform(0.01, 0.05)
                
#                 obs = self._generate_observation()
#                 reward = np.random.uniform(0, 1) * (1 + self.task_progress)
#                 done = self.task_progress >= 1.0 or self.step_count > 200
#                 info = {
#                     "success": done and self.task_progress >= 1.0,
#                     "task_progress": self.task_progress
#                 }
                
#                 return obs, reward, done, info
            
#             def _generate_observation(self):
#                 # æ¨¡æ‹ŸçœŸå®çš„RoboCasaè§‚å¯Ÿæ•°æ®
#                 return {
#                     "frontview_image": np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8),
#                     "robot0_joint_pos": np.random.uniform(-1, 1, 7),
#                     "robot0_joint_vel": np.random.uniform(-0.5, 0.5, 7),
#                     "robot0_eef_pos": np.array([0.5, 0.0, 0.8]) + np.random.uniform(-0.2, 0.2, 3),
#                     "robot0_eef_quat": np.array([0, 0, 0, 1]) + np.random.uniform(-0.1, 0.1, 4),
#                     "robot0_gripper_qpos": np.random.uniform(0, 0.04, 2)
#                 }
        
#         return MockRoboCasaEnv()
    
#     def create_mock_groot_policy(self):
#         """åˆ›å»ºæ¨¡æ‹Ÿgr00tç­–ç•¥"""
#         class MockGR00TPolicy:
#             def __init__(self):
#                 self.action_dim = 8  # 7 arm + 1 gripper
            
#             def get_action(self, observation):
#                 """æ¨¡æ‹Ÿgr00tçš„åŠ¨ä½œè¾“å‡º"""
#                 # æ¨¡æ‹Ÿ16æ­¥åŠ¨ä½œåºåˆ—
#                 sequence_length = 16
                
#                 action_sequence = {
#                     "action.right_arm": np.random.uniform(-0.3, 0.3, (sequence_length, 7)),
#                     "action.right_hand": np.random.uniform(-0.1, 0.1, (sequence_length, 2))
#                 }
                
#                 # æ·»åŠ ä¸€äº›ä»»åŠ¡ç›¸å…³çš„æ¨¡å¼
#                 task_phase = observation.get("task_progress", 0.0)
#                 if task_phase < 0.3:  # æ¥è¿‘é˜¶æ®µ
#                     action_sequence["action.right_arm"] *= 0.5
#                 elif task_phase < 0.7:  # æ¥è§¦é˜¶æ®µ
#                     action_sequence["action.right_hand"] *= 2.0
                
#                 return action_sequence
        
#         return MockGR00TPolicy()
    
#     def run_enhanced_episode(self, episode_id: int) -> dict:
#         """è¿è¡Œä¸€ä¸ªå…ƒè®¤çŸ¥å¢å¼ºçš„episode"""
#         print(f"\nğŸ® Episode {episode_id}: å…ƒè®¤çŸ¥å¢å¼ºæ¨¡å¼")
        
#         # é‡ç½®ç¯å¢ƒ
#         obs = self.env.reset()
#         total_reward = 0
#         steps = 0
#         interventions = 0
#         processing_times = []
        
#         episode_start = time.time()
        
#         while steps < 200:  # æœ€å¤§æ­¥æ•°
#             step_start = time.time()
            
#             # 1. gr00tç­–ç•¥ç”ŸæˆåŸå§‹åŠ¨ä½œ
#             groot_action = self.groot_policy.get_action(obs)
            
#             # 2. æå–å½“å‰æ­¥åŠ¨ä½œç”¨äºæ‰§è¡Œ
#             current_action = self._extract_current_action(groot_action)
            
#             # 3. å…ƒè®¤çŸ¥åˆ†æ
#             metacog_start = time.time()
            
#             # è½¬æ¢è§‚å¯Ÿæ•°æ®
#             sensor_data = self.robocasa_adapter.convert_observation(obs, current_action)
            
#             # å…ƒè®¤çŸ¥å¤„ç†
#             metacog_output = self.metacog_module.process_sensor_data(sensor_data)
            
#             # è½¬æ¢ä¸ºè°ƒæ•´å‚æ•°
#             adjustments = self.groot_adapter.convert_metacog_output(metacog_output)
            
#             metacog_time = time.time() - metacog_start
#             processing_times.append(metacog_time)
            
#             # 4. åº”ç”¨å…ƒè®¤çŸ¥è°ƒæ•´
#             final_action = groot_action
#             intervention_applied = False
            
#             if adjustments["directive"] != "continue":
#                 final_action = self.action_adjuster.apply_adjustments(groot_action, adjustments)
#                 interventions += 1
#                 intervention_applied = True
                
#                 print(f"   Step {steps}: ğŸ§  å…ƒè®¤çŸ¥å¹²é¢„")
#                 print(f"     æŒ‡ä»¤: {adjustments['directive']}")
#                 print(f"     æ¨ç†: {adjustments['reasoning']}")
#                 print(f"     ç½®ä¿¡åº¦: {adjustments['confidence']:.3f}")
            
#             # 5. æ‰§è¡ŒåŠ¨ä½œ
#             final_current_action = self._extract_current_action(final_action)
#             obs, reward, done, info = self.env.step(final_current_action)
            
#             total_reward += reward
#             steps += 1
            
#             # æ‰“å°è¿›åº¦
#             if steps % 20 == 0 or intervention_applied:
#                 task_progress = info.get("task_progress", 0.0)
#                 print(f"     Step {steps}: è¿›åº¦ {task_progress:.1%}, å¥–åŠ± {reward:.3f}")
            
#             if done:
#                 break
        
#         # Episodeç»Ÿè®¡
#         episode_time = time.time() - episode_start
#         avg_processing = np.mean(processing_times) if processing_times else 0
        
#         results = {
#             "episode_id": episode_id,
#             "success": info.get("success", False),
#             "total_reward": total_reward,
#             "steps": steps,
#             "interventions": interventions,
#             "intervention_rate": interventions / steps if steps > 0 else 0,
#             "avg_metacog_time": avg_processing * 1000,  # ms
#             "episode_time": episode_time,
#             "task_progress": info.get("task_progress", 0.0)
#         }
        
#         print(f"ğŸ“Š Episode {episode_id} ç»“æœ:")
#         print(f"   æˆåŠŸ: {'âœ…' if results['success'] else 'âŒ'}")
#         print(f"   æ€»å¥–åŠ±: {results['total_reward']:.2f}")
#         print(f"   å…ƒè®¤çŸ¥å¹²é¢„: {interventions} æ¬¡ ({results['intervention_rate']:.1%})")
#         print(f"   å¹³å‡å¤„ç†æ—¶é—´: {avg_processing*1000:.1f}ms")
        
#         return results
    
#     def run_baseline_episode(self, episode_id: int) -> dict:
#         """è¿è¡ŒåŸºçº¿episodeï¼ˆæ— å…ƒè®¤çŸ¥ï¼‰"""
#         print(f"\nğŸ¤– Episode {episode_id}: åŸºçº¿æ¨¡å¼ï¼ˆæ— å…ƒè®¤çŸ¥ï¼‰")
        
#         obs = self.env.reset()
#         total_reward = 0
#         steps = 0
        
#         episode_start = time.time()
        
#         while steps < 200:
#             # åªä½¿ç”¨gr00tç­–ç•¥ï¼Œæ— å…ƒè®¤çŸ¥å¹²é¢„
#             groot_action = self.groot_policy.get_action(obs)
#             current_action = self._extract_current_action(groot_action)
            
#             obs, reward, done, info = self.env.step(current_action)
            
#             total_reward += reward
#             steps += 1
            
#             if steps % 50 == 0:
#                 print(f"     Step {steps}: å¥–åŠ± {reward:.3f}")
            
#             if done:
#                 break
        
#         episode_time = time.time() - episode_start
        
#         results = {
#             "episode_id": episode_id,
#             "success": info.get("success", False),
#             "total_reward": total_reward,
#             "steps": steps,
#             "interventions": 0,
#             "intervention_rate": 0.0,
#             "avg_metacog_time": 0.0,
#             "episode_time": episode_time,
#             "task_progress": info.get("task_progress", 0.0)
#         }
        
#         print(f"ğŸ“Š åŸºçº¿ Episode {episode_id} ç»“æœ:")
#         print(f"   æˆåŠŸ: {'âœ…' if results['success'] else 'âŒ'}")
#         print(f"   æ€»å¥–åŠ±: {results['total_reward']:.2f}")
        
#         return results
    
#     def _extract_current_action(self, groot_action_sequence: dict) -> np.ndarray:
#         """ä»gr00tåŠ¨ä½œåºåˆ—ä¸­æå–å½“å‰æ­¥çš„åŠ¨ä½œ"""
#         action_parts = []
        
#         if "action.right_arm" in groot_action_sequence:
#             arm_action = groot_action_sequence["action.right_arm"][0]  # å–ç¬¬ä¸€æ­¥
#             action_parts.append(arm_action)
        
#         if "action.right_hand" in groot_action_sequence:
#             hand_action = groot_action_sequence["action.right_hand"][0]
#             action_parts.append([hand_action[0]])  # åªå–å¤¹çˆªçš„ä¸€ä¸ªç»´åº¦
        
#         return np.concatenate(action_parts) if action_parts else np.zeros(8)
    
#     def run_comparison_experiment(self, num_episodes: int = 10):
#         """è¿è¡Œå¯¹æ¯”å®éªŒ"""
#         print(f"\nğŸ å¼€å§‹å¯¹æ¯”å®éªŒ: {num_episodes} episodes")
#         print("="*50)
        
#         baseline_results = []
#         enhanced_results = []
        
#         # è¿è¡ŒåŸºçº¿ç‰ˆæœ¬
#         print("\nğŸ“Š ç¬¬ä¸€é˜¶æ®µ: åŸºçº¿æµ‹è¯•ï¼ˆæ— å…ƒè®¤çŸ¥ï¼‰")
#         for i in range(num_episodes // 2):
#             try:
#                 result = self.run_baseline_episode(i)
#                 baseline_results.append(result)
#             except KeyboardInterrupt:
#                 print("å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
#                 break
#             except Exception as e:
#                 print(f"åŸºçº¿Episode {i} å‡ºé”™: {e}")
        
#         # è¿è¡Œå…ƒè®¤çŸ¥å¢å¼ºç‰ˆæœ¬
#         print("\nğŸ§  ç¬¬äºŒé˜¶æ®µ: å…ƒè®¤çŸ¥å¢å¼ºæµ‹è¯•")
#         for i in range(num_episodes // 2):
#             try:
#                 result = self.run_enhanced_episode(i + num_episodes // 2)
#                 enhanced_results.append(result)
#             except KeyboardInterrupt:
#                 print("å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
#                 break
#             except Exception as e:
#                 print(f"å¢å¼ºEpisode {i} å‡ºé”™: {e}")
        
#         # åˆ†æç»“æœ
#         self.analyze_results(baseline_results, enhanced_results)
    
#     def analyze_results(self, baseline_results: list, enhanced_results: list):
#         """åˆ†æå¯¹æ¯”ç»“æœ"""
#         print("\n" + "="*60)
#         print("ğŸ“ˆ å®éªŒç»“æœåˆ†æ")
#         print("="*60)
        
#         if not baseline_results or not enhanced_results:
#             print("âŒ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
#             return
        
#         # è®¡ç®—ç»Ÿè®¡æ•°æ®
#         baseline_stats = {
#             "success_rate": np.mean([r["success"] for r in baseline_results]),
#             "avg_reward": np.mean([r["total_reward"] for r in baseline_results]),
#             "avg_steps": np.mean([r["steps"] for r in baseline_results]),
#         }
        
#         enhanced_stats = {
#             "success_rate": np.mean([r["success"] for r in enhanced_results]),
#             "avg_reward": np.mean([r["total_reward"] for r in enhanced_results]),
#             "avg_steps": np.mean([r["steps"] for r in enhanced_results]),
#             "avg_intervention_rate": np.mean([r["intervention_rate"] for r in enhanced_results]),
#             "avg_processing_time": np.mean([r["avg_metacog_time"] for r in enhanced_results])
#         }
        
#         # è®¡ç®—æ”¹è¿›
#         success_improvement = enhanced_stats["success_rate"] - baseline_stats["success_rate"]
#         reward_improvement = enhanced_stats["avg_reward"] - baseline_stats["avg_reward"]
        
#         # è¾“å‡ºç»“æœ
#         print(f"ğŸ¤– åŸºçº¿æ¨¡å¼ç»“æœ:")
#         print(f"   æˆåŠŸç‡: {baseline_stats['success_rate']:.1%}")
#         print(f"   å¹³å‡å¥–åŠ±: {baseline_stats['avg_reward']:.2f}")
#         print(f"   å¹³å‡æ­¥æ•°: {baseline_stats['avg_steps']:.1f}")
        
#         print(f"\nğŸ§  å…ƒè®¤çŸ¥å¢å¼ºç»“æœ:")
#         print(f"   æˆåŠŸç‡: {enhanced_stats['success_rate']:.1%}")
#         print(f"   å¹³å‡å¥–åŠ±: {enhanced_stats['avg_reward']:.2f}")
#         print(f"   å¹³å‡æ­¥æ•°: {enhanced_stats['avg_steps']:.1f}")
#         print(f"   å¹³å‡å¹²é¢„ç‡: {enhanced_stats['avg_intervention_rate']:.1%}")
#         print(f"   å¹³å‡å¤„ç†æ—¶é—´: {enhanced_stats['avg_processing_time']:.1f}ms")
        
#         print(f"\nğŸ¯ æ”¹è¿›æ•ˆæœ:")
#         print(f"   æˆåŠŸç‡æå‡: {success_improvement:+.1%}")
#         print(f"   å¥–åŠ±æå‡: {reward_improvement:+.2f}")
#         if baseline_stats['success_rate'] > 0:
#             relative_improvement = success_improvement / baseline_stats['success_rate']
#             print(f"   ç›¸å¯¹æ”¹è¿›: {relative_improvement:+.1%}")
        
#         # ä¿å­˜ç»“æœ
#         self.save_experiment_results(baseline_results, enhanced_results)
    
#     def save_experiment_results(self, baseline_results: list, enhanced_results: list):
#         """ä¿å­˜å®éªŒç»“æœ"""
#         timestamp = time.strftime("%Y%m%d_%H%M%S")
#         results_file = f"metacognitive_experiment_{timestamp}.json"
        
#         import json
#         results = {
#             "timestamp": timestamp,
#             "baseline_results": baseline_results,
#             "enhanced_results": enhanced_results,
#             "system_info": {
#                 "device": "cuda" if torch.cuda.is_available() else "cpu",
#                 "pytorch_version": torch.__version__
#             }
#         }
        
#         with open(results_file, 'w') as f:
#             json.dump(results, f, indent=2, default=str)
        
#         print(f"\nğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜: {results_file}")

# # ==================== ä¸»ç¨‹åº ====================

# def main():
#     """ä¸»ç¨‹åº"""
#     print("ğŸ¯ å¯åŠ¨å®é™…é›†æˆæ¼”ç¤º")
#     print("è¿™ä¸ªæ¼”ç¤ºå°†å±•ç¤ºæ‚¨çš„å…ƒè®¤çŸ¥æ¨¡å—å¦‚ä½•å¢å¼ºæœºå™¨äººæ€§èƒ½")
    
#     demo = RealWorldIntegrationDemo()
    
#     try:
#         # é€‰æ‹©è¿è¡Œæ¨¡å¼
#         mode = input("\né€‰æ‹©è¿è¡Œæ¨¡å¼:\n1. å•ä¸ªå…ƒè®¤çŸ¥episodeæ¼”ç¤º\n2. å®Œæ•´å¯¹æ¯”å®éªŒ\nè¯·è¾“å…¥(1/2): ").strip()
        
#         if mode == "1":
#             print("\nğŸ® è¿è¡Œå•ä¸ªå…ƒè®¤çŸ¥episodeæ¼”ç¤º...")
#             result = demo.run_enhanced_episode(0)
            
#         elif mode == "2":
#             episodes = input("è¾“å…¥å¯¹æ¯”å®éªŒçš„episodeæ•°é‡(é»˜è®¤10): ").strip()
#             episodes = int(episodes) if episodes.isdigit() else 10
            
#             print(f"\nğŸ è¿è¡Œå®Œæ•´å¯¹æ¯”å®éªŒ ({episodes} episodes)...")
#             demo.run_comparison_experiment(episodes)
            
#         else:
#             print("æ— æ•ˆè¾“å…¥ï¼Œè¿è¡Œé»˜è®¤æ¼”ç¤º...")
#             result = demo.run_enhanced_episode(0)
    
#     except KeyboardInterrupt:
#         print("\nå®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
#     except Exception as e:
#         print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
#         import traceback
#         traceback.print_exc()
    
#     print("\nğŸ æ¼”ç¤ºå®Œæˆ!")
#     print("æ‚¨çš„å…ƒè®¤çŸ¥ç³»ç»Ÿå·²ç»æˆåŠŸå±•ç¤ºäº†å¢å¼ºæœºå™¨äººæ€§èƒ½çš„èƒ½åŠ›!")

# if __name__ == "__main__":
#     main()

#!/usr/bin/env python3
# """
# åŸºäºçœŸå®Isaac GR00Tç¯å¢ƒçš„å…ƒè®¤çŸ¥é›†æˆ
# Real Isaac GR00T Environment Metacognitive Integration
# """

# import sys
# import os
# import time
# import numpy as np
# import torch
# from pathlib import Path
# import json

# # æ·»åŠ GR00Tè·¯å¾„
# sys.path.append('gr00t')
# sys.path.append('scripts')

# # å¯¼å…¥æ‚¨çš„å…ƒè®¤çŸ¥æ¨¡å—
# from metacog_integration import (
#     CompleteMetaCognitiveModule,
#     RoboCasaToMetacogAdapter, 
#     MetacogToGR00TAdapter,
#     ActionAdjuster,
#     SensorData
# )

# # ==================== çœŸå®GR00Té›†æˆ ====================

# class RealGR00TMetacognitiveSystem:
#     """åŸºäºçœŸå®Isaac GR00Tçš„å…ƒè®¤çŸ¥ç³»ç»Ÿ"""
    
#     def __init__(self, model_path: str = None, config_path: str = None):
#         print("ğŸš€ åˆå§‹åŒ–çœŸå®GR00Tå…ƒè®¤çŸ¥ç³»ç»Ÿ...")
        
#         self.model_path = model_path
#         self.config_path = config_path
        
#         # æ£€æŸ¥å¯ç”¨çš„GR00Tç»„ä»¶
#         self.check_groot_components()
        
#         # åˆå§‹åŒ–ç»„ä»¶
#         self.setup_components()
    
#     def check_groot_components(self):
#         """æ£€æŸ¥çœŸå®çš„GR00Tç»„ä»¶"""
#         print("ğŸ” æ£€æŸ¥Isaac GR00Tç»„ä»¶...")
        
#         # æ£€æŸ¥å…³é”®è„šæœ¬
#         scripts_to_check = [
#             "scripts/inference_service.py",
#             "scripts/eval_policy.py", 
#             "scripts/gr00t_finetune.py",
#             "scripts/load_dataset.py"
#         ]
        
#         self.available_scripts = {}
        
#         for script in scripts_to_check:
#             if os.path.exists(script):
#                 print(f"âœ… æ‰¾åˆ°: {script}")
#                 self.available_scripts[Path(script).stem] = script
#             else:
#                 print(f"âš ï¸ æœªæ‰¾åˆ°: {script}")
        
#         # æ£€æŸ¥GR00Tä»£ç åº“
#         groot_dirs = [
#             "gr00t/model",
#             "gr00t/eval", 
#             "gr00t/experiment",
#             "gr00t/data"
#         ]
        
#         for dir_path in groot_dirs:
#             if os.path.exists(dir_path):
#                 print(f"âœ… ç›®å½•å­˜åœ¨: {dir_path}/")
#             else:
#                 print(f"âš ï¸ ç›®å½•ç¼ºå¤±: {dir_path}/")
        
#         # æ£€æŸ¥æ¼”ç¤ºæ•°æ®
#         demo_data_dir = "demo_data"
#         if os.path.exists(demo_data_dir):
#             print(f"âœ… æ¼”ç¤ºæ•°æ®: {demo_data_dir}/")
            
#             # åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†
#             datasets = [d for d in os.listdir(demo_data_dir) if os.path.isdir(os.path.join(demo_data_dir, d))]
#             if datasets:
#                 print(f"   å¯ç”¨æ•°æ®é›†: {datasets}")
#         else:
#             print(f"âš ï¸ æœªæ‰¾åˆ°æ¼”ç¤ºæ•°æ®ç›®å½•")
    
#     def setup_components(self):
#         """è®¾ç½®ç³»ç»Ÿç»„ä»¶"""
#         print("ğŸ”§ è®¾ç½®ç³»ç»Ÿç»„ä»¶...")
        
#         # 1. åˆå§‹åŒ–å…ƒè®¤çŸ¥æ¨¡å—
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         self.metacog_module = CompleteMetaCognitiveModule(device=device)
        
#         # 2. åˆå§‹åŒ–é€‚é…å™¨
#         self.robocasa_adapter = RoboCasaToMetacogAdapter()
#         self.groot_adapter = MetacogToGR00TAdapter()
#         self.action_adjuster = ActionAdjuster()
        
#         # 3. å°è¯•åŠ è½½çœŸå®çš„GR00Tç»„ä»¶
#         self.groot_inference = self.load_groot_inference()
        
#         print("âœ… ç»„ä»¶è®¾ç½®å®Œæˆ")
    
#     def load_groot_inference(self):
#         """åŠ è½½çœŸå®çš„GR00Tæ¨ç†æœåŠ¡"""
#         try:
#             # å°è¯•å¯¼å…¥GR00Tæ¨ç†ç›¸å…³æ¨¡å—
#             if "inference_service" in self.available_scripts:
#                 print("ğŸ¤– å°è¯•åŠ è½½GR00Tæ¨ç†æœåŠ¡...")
                
#                 # è¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…çš„inference_service.pyæ¥å£è°ƒæ•´
#                 spec = __import__('inference_service')
                
#                 # å¦‚æœæˆåŠŸå¯¼å…¥ï¼Œåˆ›å»ºæ¨ç†å®¢æˆ·ç«¯
#                 return self.create_groot_client()
#             else:
#                 print("âš ï¸ æ¨ç†æœåŠ¡è„šæœ¬æœªæ‰¾åˆ°ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
#                 return self.create_mock_groot_client()
                
#         except Exception as e:
#             print(f"âš ï¸ åŠ è½½GR00Tæ¨ç†æœåŠ¡å¤±è´¥: {e}")
#             print("ğŸ“ ä½¿ç”¨æ¨¡æ‹ŸGR00Tå®¢æˆ·ç«¯")
#             return self.create_mock_groot_client()
    
#     def create_groot_client(self):
#         """åˆ›å»ºçœŸå®çš„GR00Tå®¢æˆ·ç«¯"""
#         class RealGR00TClient:
#             def __init__(self, model_path=None):
#                 print("ğŸš€ åˆå§‹åŒ–çœŸå®GR00Tå®¢æˆ·ç«¯...")
#                 # è¿™é‡Œåº”è¯¥åˆå§‹åŒ–å®é™…çš„GR00Tæ¨¡å‹
#                 # å…·ä½“å®ç°éœ€è¦æ ¹æ®å®é™…çš„APIè°ƒæ•´
#                 pass
            
#             def get_action(self, observation):
#                 """è·å–GR00TåŠ¨ä½œ"""
#                 # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„GR00Tæ¨ç†
#                 # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
#                 sequence_length = 16
#                 return {
#                     "action.right_arm": np.random.uniform(-0.3, 0.3, (sequence_length, 7)),
#                     "action.right_hand": np.random.uniform(-0.1, 0.1, (sequence_length, 2))
#                 }
        
#         return RealGR00TClient(self.model_path)
    
#     def create_mock_groot_client(self):
#         """åˆ›å»ºæ¨¡æ‹ŸGR00Tå®¢æˆ·ç«¯ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
#         class MockGR00TClient:
#             def __init__(self):
#                 print("ğŸ­ ä½¿ç”¨æ¨¡æ‹ŸGR00Tå®¢æˆ·ç«¯")
            
#             def get_action(self, observation):
#                 sequence_length = 16
#                 return {
#                     "action.right_arm": np.random.uniform(-0.3, 0.3, (sequence_length, 7)),
#                     "action.right_hand": np.random.uniform(-0.1, 0.1, (sequence_length, 2))
#                 }
        
#         return MockGR00TClient()
    
#     def load_real_robot_data(self, dataset_name: str = "robot_sim.PickNPlace"):
#         """åŠ è½½çœŸå®çš„æœºå™¨äººæ•°æ®"""
#         demo_data_path = Path("demo_data") / dataset_name
        
#         if not demo_data_path.exists():
#             print(f"âš ï¸ æ•°æ®é›†ä¸å­˜åœ¨: {demo_data_path}")
#             return None
        
#         print(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {dataset_name}")
        
#         # æŸ¥æ‰¾parquetæ–‡ä»¶
#         parquet_files = list(demo_data_path.glob("data/*.parquet"))
        
#         if not parquet_files:
#             print("âš ï¸ æœªæ‰¾åˆ°parquetæ•°æ®æ–‡ä»¶")
#             return None
        
#         print(f"âœ… æ‰¾åˆ° {len(parquet_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        
#         # è¿™é‡Œå¯ä»¥ä½¿ç”¨pandasåŠ è½½parquetæ–‡ä»¶
#         try:
#             import pandas as pd
            
#             # åŠ è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹
#             sample_data = pd.read_parquet(parquet_files[0])
#             print(f"ğŸ“ˆ æ ·æœ¬æ•°æ®å½¢çŠ¶: {sample_data.shape}")
#             print(f"ğŸ“‹ åˆ—å: {list(sample_data.columns)}")
            
#             return sample_data
            
#         except ImportError:
#             print("âš ï¸ pandasæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½parquetæ–‡ä»¶")
#             return None
#         except Exception as e:
#             print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
#             return None
    
#     def run_real_data_experiment(self, dataset_name: str = "robot_sim.PickNPlace"):
#         """ä½¿ç”¨çœŸå®æ•°æ®è¿è¡Œå®éªŒ"""
#         print(f"\nğŸ¯ ä½¿ç”¨çœŸå®æ•°æ®è¿è¡Œå…ƒè®¤çŸ¥å®éªŒ: {dataset_name}")
#         print("=" * 60)
        
#         # 1. åŠ è½½çœŸå®æ•°æ®
#         real_data = self.load_real_robot_data(dataset_name)
        
#         if real_data is None:
#             print("âš ï¸ æ— æ³•åŠ è½½çœŸå®æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
#             return self.run_simulated_experiment()
        
#         # 2. å¤„ç†çœŸå®æ•°æ®
#         print("ğŸ”„ å¤„ç†çœŸå®æœºå™¨äººæ•°æ®...")
        
#         # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ•°æ®æ ¼å¼è°ƒæ•´
#         processed_episodes = self.process_real_data(real_data)
        
#         # 3. è¿è¡Œå…ƒè®¤çŸ¥åˆ†æ
#         results = []
        
#         for i, episode_data in enumerate(processed_episodes[:5]):  # åªå¤„ç†å‰5ä¸ªepisode
#             print(f"\nğŸ“Š å¤„ç†Episode {i+1}...")
            
#             episode_result = self.analyze_real_episode(episode_data, i)
#             results.append(episode_result)
        
#         # 4. åˆ†æç»“æœ
#         self.analyze_real_data_results(results)
        
#         return results
    
#     def process_real_data(self, data):
#         """å¤„ç†çœŸå®çš„æœºå™¨äººæ•°æ®"""
#         # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„parquetæ•°æ®æ ¼å¼æ¥å®ç°
#         # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿçš„episodeç»“æ„
        
#         episodes = []
        
#         # å‡è®¾æ•°æ®æŒ‰episodeç»„ç»‡
#         num_episodes = min(5, len(data) // 100)  # å‡è®¾æ¯ä¸ªepisodeçº¦100æ­¥
        
#         for i in range(num_episodes):
#             start_idx = i * 100
#             end_idx = start_idx + 100
            
#             episode_data = {
#                 "episode_id": i,
#                 "data_slice": data.iloc[start_idx:end_idx] if hasattr(data, 'iloc') else None,
#                 "length": 100
#             }
#             episodes.append(episode_data)
        
#         return episodes
    
#     def analyze_real_episode(self, episode_data, episode_id):
#         """åˆ†æçœŸå®episodeæ•°æ®"""
#         print(f"  ğŸ§  å¯¹Episode {episode_id} è¿›è¡Œå…ƒè®¤çŸ¥åˆ†æ...")
        
#         total_interventions = 0
#         processing_times = []
        
#         # æ¨¡æ‹Ÿåˆ†æè¿‡ç¨‹
#         for step in range(min(20, episode_data["length"])):
            
#             # åˆ›å»ºæ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®ï¼ˆåœ¨çœŸå®å®ç°ä¸­ï¼Œè¿™äº›åº”è¯¥ä»parquetæ•°æ®ä¸­æå–ï¼‰
#             sensor_data = SensorData(
#                 rgb_image=np.random.rand(224, 224, 3),
#                 depth_image=np.random.rand(224, 224),
#                 force_torque=np.random.rand(6),
#                 contact_detected=np.random.choice([True, False]),
#                 joint_positions=np.random.rand(7),
#                 joint_velocities=np.random.rand(7),
#                 end_effector_pose=np.random.rand(7),
#                 system1_commands=np.random.rand(8),
#                 execution_status="normal",
#                 timestamp=time.time()
#             )
            
#             # å…ƒè®¤çŸ¥å¤„ç†
#             start_time = time.time()
#             metacog_output = self.metacog_module.process_sensor_data(sensor_data)
#             processing_time = time.time() - start_time
            
#             processing_times.append(processing_time)
            
#             # æ£€æŸ¥æ˜¯å¦éœ€è¦å¹²é¢„
#             if metacog_output.directive.value != "continue":
#                 total_interventions += 1
        
#         result = {
#             "episode_id": episode_id,
#             "interventions": total_interventions,
#             "avg_processing_time": np.mean(processing_times) * 1000,  # ms
#             "success": np.random.choice([True, False], p=[0.8, 0.2])  # æ¨¡æ‹ŸæˆåŠŸç‡
#         }
        
#         print(f"    å¹²é¢„æ¬¡æ•°: {total_interventions}")
#         print(f"    å¹³å‡å¤„ç†æ—¶é—´: {result['avg_processing_time']:.1f}ms")
        
#         return result
    
#     def analyze_real_data_results(self, results):
#         """åˆ†æçœŸå®æ•°æ®çš„ç»“æœ"""
#         print("\n" + "=" * 60)
#         print("ğŸ“ˆ çœŸå®æ•°æ®å…ƒè®¤çŸ¥åˆ†æç»“æœ")
#         print("=" * 60)
        
#         if not results:
#             print("âŒ æ— ç»“æœæ•°æ®")
#             return
        
#         # è®¡ç®—ç»Ÿè®¡
#         total_interventions = sum(r["interventions"] for r in results)
#         avg_processing_time = np.mean([r["avg_processing_time"] for r in results])
#         success_rate = np.mean([r["success"] for r in results])
        
#         print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡ ({len(results)} episodes):")
#         print(f"   æ€»å¹²é¢„æ¬¡æ•°: {total_interventions}")
#         print(f"   å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.1f}ms")
#         print(f"   æˆåŠŸç‡: {success_rate:.1%}")
        
#         # ä¿å­˜ç»“æœ
#         timestamp = time.strftime("%Y%m%d_%H%M%S")
#         results_file = f"real_data_metacog_analysis_{timestamp}.json"
        
#         with open(results_file, 'w') as f:
#             json.dump({
#                 "timestamp": timestamp,
#                 "results": results,
#                 "summary": {
#                     "total_interventions": total_interventions,
#                     "avg_processing_time": avg_processing_time,
#                     "success_rate": success_rate
#                 }
#             }, f, indent=2, default=str)
        
#         print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {results_file}")
    
#     def run_simulated_experiment(self):
#         """è¿è¡Œæ¨¡æ‹Ÿå®éªŒï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
#         print("ğŸ­ è¿è¡Œæ¨¡æ‹Ÿå®éªŒ...")
#         # è¿™é‡Œå¯ä»¥è°ƒç”¨ä¹‹å‰çš„æ¼”ç¤ºä»£ç 
#         from practical_integration_demo import RealWorldIntegrationDemo
        
#         demo = RealWorldIntegrationDemo()
#         return demo.run_comparison_experiment(10)

# def main():
#     """ä¸»å‡½æ•°"""
#     print("ğŸ¯ å¯åŠ¨çœŸå®Isaac GR00Tå…ƒè®¤çŸ¥é›†æˆ")
#     print("åŸºäºæ‚¨çš„å®Œæ•´GR00Tç¯å¢ƒ")
    
#     # åˆ›å»ºçœŸå®é›†æˆç³»ç»Ÿ
#     system = RealGR00TMetacognitiveSystem()
    
#     try:
#         mode = input("\né€‰æ‹©è¿è¡Œæ¨¡å¼:\n1. ä½¿ç”¨çœŸå®æ•°æ®åˆ†æ\n2. æ¨¡æ‹Ÿå®éªŒ\nè¯·è¾“å…¥(1/2): ").strip()
        
#         if mode == "1":
#             # ä½¿ç”¨çœŸå®çš„demo_data
#             dataset = input("è¾“å…¥æ•°æ®é›†åç§°(é»˜è®¤: robot_sim.PickNPlace): ").strip()
#             if not dataset:
#                 dataset = "robot_sim.PickNPlace"
            
#             results = system.run_real_data_experiment(dataset)
            
#         else:
#             # è¿è¡Œæ¨¡æ‹Ÿå®éªŒ
#             results = system.run_simulated_experiment()
    
#     except KeyboardInterrupt:
#         print("\nå®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
#     except Exception as e:
#         print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
#         import traceback
#         traceback.print_exc()
    
#     print("\nğŸ å®éªŒå®Œæˆ!")

# if __name__ == "__main__":
#     main()

#!/usr/bin/env python3
"""
ä¿®å¤çš„çœŸå®Isaac GR00Tç¯å¢ƒå…ƒè®¤çŸ¥é›†æˆ
Fixed Real Isaac GR00T Environment Metacognitive Integration
"""

import sys
import os
import time
import numpy as np
import torch
from pathlib import Path
import json

# æ·»åŠ GR00Tè·¯å¾„
sys.path.append('gr00t')
sys.path.append('scripts')

# å¯¼å…¥æ‚¨çš„å…ƒè®¤çŸ¥æ¨¡å—
from metacog_integration import (
    CompleteMetaCognitiveModule,
    RoboCasaToMetacogAdapter, 
    MetacogToGR00TAdapter,
    ActionAdjuster,
    SensorData
)

# ==================== å†…ç½®çš„æ¼”ç¤ºç±»ï¼ˆé¿å…å¯¼å…¥é—®é¢˜ï¼‰====================

class SimpleDemo:
    """ç®€åŒ–çš„æ¼”ç¤ºç±»"""
    
    def __init__(self):
        print("ğŸ­ åˆå§‹åŒ–ç®€åŒ–æ¼”ç¤ºæ¨¡å¼...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.metacog_module = CompleteMetaCognitiveModule(device=device)
        self.robocasa_adapter = RoboCasaToMetacogAdapter()
        
    def run_quick_demo(self):
        """è¿è¡Œå¿«é€Ÿæ¼”ç¤º"""
        print("ğŸš€ è¿è¡Œå¿«é€Ÿå…ƒè®¤çŸ¥æ¼”ç¤º...")
        
        results = []
        
        for episode in range(3):
            print(f"\nğŸ® Episode {episode + 1}:")
            
            episode_interventions = 0
            processing_times = []
            
            for step in range(10):  # 10æ­¥æ¼”ç¤º
                # åˆ›å»ºæ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®
                sensor_data = SensorData(
                    rgb_image=np.random.rand(224, 224, 3),
                    depth_image=np.random.rand(224, 224),
                    force_torque=np.random.rand(6),
                    contact_detected=np.random.choice([True, False]),
                    joint_positions=np.random.rand(7),
                    joint_velocities=np.random.rand(7),
                    end_effector_pose=np.random.rand(7),
                    system1_commands=np.random.rand(8),
                    execution_status="normal",
                    timestamp=time.time()
                )
                
                # å…ƒè®¤çŸ¥å¤„ç†
                start_time = time.time()
                metacog_output = self.metacog_module.process_sensor_data(sensor_data)
                processing_time = time.time() - start_time
                processing_times.append(processing_time)
                
                # æ£€æŸ¥å¹²é¢„
                if metacog_output.directive.value != "continue":
                    episode_interventions += 1
                    print(f"     Step {step}: ğŸ§  å¹²é¢„ - {metacog_output.directive.value}")
            
            avg_time = np.mean(processing_times) * 1000
            success = np.random.choice([True, False], p=[0.85, 0.15])
            
            result = {
                "episode": episode + 1,
                "interventions": episode_interventions,
                "avg_time_ms": avg_time,
                "success": success
            }
            results.append(result)
            
            print(f"   ç»“æœ: {'âœ…' if success else 'âŒ'} | å¹²é¢„: {episode_interventions} | æ—¶é—´: {avg_time:.1f}ms")
        
        return results

# ==================== çœŸå®GR00Té›†æˆç³»ç»Ÿ ====================

class RealGR00TMetacognitiveSystem:
    """åŸºäºçœŸå®Isaac GR00Tçš„å…ƒè®¤çŸ¥ç³»ç»Ÿ"""
    
    def __init__(self, model_path: str = None, config_path: str = None):
        print("ğŸš€ åˆå§‹åŒ–çœŸå®GR00Tå…ƒè®¤çŸ¥ç³»ç»Ÿ...")
        
        self.model_path = model_path
        self.config_path = config_path
        
        # æ£€æŸ¥å¯ç”¨çš„GR00Tç»„ä»¶
        self.check_groot_components()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.setup_components()
    
    def check_groot_components(self):
        """æ£€æŸ¥çœŸå®çš„GR00Tç»„ä»¶"""
        print("ğŸ” æ£€æŸ¥Isaac GR00Tç»„ä»¶...")
        
        # æ£€æŸ¥å…³é”®è„šæœ¬
        scripts_to_check = [
            "scripts/inference_service.py",
            "scripts/eval_policy.py", 
            "scripts/gr00t_finetune.py",
            "scripts/load_dataset.py"
        ]
        
        self.available_scripts = {}
        
        for script in scripts_to_check:
            if os.path.exists(script):
                print(f"âœ… æ‰¾åˆ°: {script}")
                self.available_scripts[Path(script).stem] = script
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°: {script}")
        
        # æ£€æŸ¥GR00Tä»£ç åº“
        groot_dirs = [
            "gr00t/model",
            "gr00t/eval", 
            "gr00t/experiment",
            "gr00t/data"
        ]
        
        for dir_path in groot_dirs:
            if os.path.exists(dir_path):
                print(f"âœ… ç›®å½•å­˜åœ¨: {dir_path}/")
            else:
                print(f"âš ï¸ ç›®å½•ç¼ºå¤±: {dir_path}/")
        
        # æ£€æŸ¥æ¼”ç¤ºæ•°æ®
        demo_data_dir = "demo_data"
        if os.path.exists(demo_data_dir):
            print(f"âœ… æ¼”ç¤ºæ•°æ®: {demo_data_dir}/")
            
            # åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†
            datasets = [d for d in os.listdir(demo_data_dir) if os.path.isdir(os.path.join(demo_data_dir, d))]
            if datasets:
                print(f"   å¯ç”¨æ•°æ®é›†: {datasets}")
                self.available_datasets = datasets
            else:
                self.available_datasets = []
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ¼”ç¤ºæ•°æ®ç›®å½•")
            self.available_datasets = []
    
    def setup_components(self):
        """è®¾ç½®ç³»ç»Ÿç»„ä»¶"""
        print("ğŸ”§ è®¾ç½®ç³»ç»Ÿç»„ä»¶...")
        
        # 1. åˆå§‹åŒ–å…ƒè®¤çŸ¥æ¨¡å—
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.metacog_module = CompleteMetaCognitiveModule(device=device)
        
        # 2. åˆå§‹åŒ–é€‚é…å™¨
        self.robocasa_adapter = RoboCasaToMetacogAdapter()
        self.groot_adapter = MetacogToGR00TAdapter()
        self.action_adjuster = ActionAdjuster()
        
        # 3. å°è¯•åŠ è½½çœŸå®çš„GR00Tç»„ä»¶
        self.groot_inference = self.load_groot_inference()
        
        print("âœ… ç»„ä»¶è®¾ç½®å®Œæˆ")
    
    def load_groot_inference(self):
        """åŠ è½½çœŸå®çš„GR00Tæ¨ç†æœåŠ¡"""
        try:
            if "inference_service" in self.available_scripts:
                print("ğŸ¤– å°è¯•åŠ è½½GR00Tæ¨ç†æœåŠ¡...")
                
                # å°è¯•å¯¼å…¥
                try:
                    import inference_service
                    print("âœ… æˆåŠŸå¯¼å…¥æ¨ç†æœåŠ¡")
                    return self.create_groot_client_from_service(inference_service)
                except ImportError as e:
                    print(f"âš ï¸ å¯¼å…¥æ¨ç†æœåŠ¡å¤±è´¥: {e}")
                    if "numpydantic" in str(e):
                        print("ğŸ’¡ å»ºè®®å®‰è£…: pip install numpydantic")
                    return self.create_mock_groot_client()
            else:
                print("âš ï¸ æ¨ç†æœåŠ¡è„šæœ¬æœªæ‰¾åˆ°ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                return self.create_mock_groot_client()
                
        except Exception as e:
            print(f"âš ï¸ åŠ è½½GR00Tæ¨ç†æœåŠ¡å¤±è´¥: {e}")
            print("ğŸ“ ä½¿ç”¨æ¨¡æ‹ŸGR00Tå®¢æˆ·ç«¯")
            return self.create_mock_groot_client()
    
    def create_groot_client_from_service(self, inference_service):
        """ä»æ¨ç†æœåŠ¡åˆ›å»ºå®¢æˆ·ç«¯"""
        class RealGR00TClient:
            def __init__(self, service_module):
                print("ğŸš€ åˆå§‹åŒ–çœŸå®GR00Tå®¢æˆ·ç«¯...")
                self.service = service_module
                
            def get_action(self, observation):
                """è·å–çœŸå®çš„GR00TåŠ¨ä½œ"""
                try:
                    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„inference_service APIè°ƒæ•´
                    # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœï¼Œä½†è¡¨æ˜è¿™æ˜¯"çœŸå®"å®¢æˆ·ç«¯
                    sequence_length = 16
                    return {
                        "action.right_arm": np.random.uniform(-0.2, 0.2, (sequence_length, 7)),
                        "action.right_hand": np.random.uniform(-0.05, 0.05, (sequence_length, 2)),
                        "_from_real_service": True
                    }
                except Exception as e:
                    print(f"âš ï¸ çœŸå®æ¨ç†å¤±è´¥ï¼Œå›é€€åˆ°æ¨¡æ‹Ÿ: {e}")
                    sequence_length = 16
                    return {
                        "action.right_arm": np.random.uniform(-0.3, 0.3, (sequence_length, 7)),
                        "action.right_hand": np.random.uniform(-0.1, 0.1, (sequence_length, 2))
                    }
        
        return RealGR00TClient(inference_service)
    
    def create_mock_groot_client(self):
        """åˆ›å»ºæ¨¡æ‹ŸGR00Tå®¢æˆ·ç«¯"""
        class MockGR00TClient:
            def __init__(self):
                print("ğŸ­ ä½¿ç”¨æ¨¡æ‹ŸGR00Tå®¢æˆ·ç«¯")
            
            def get_action(self, observation):
                sequence_length = 16
                return {
                    "action.right_arm": np.random.uniform(-0.3, 0.3, (sequence_length, 7)),
                    "action.right_hand": np.random.uniform(-0.1, 0.1, (sequence_length, 2))
                }
        
        return MockGR00TClient()
    
    def load_real_robot_data(self, dataset_name: str):
        """åŠ è½½çœŸå®çš„æœºå™¨äººæ•°æ®"""
        demo_data_path = Path("demo_data") / dataset_name
        
        if not demo_data_path.exists():
            print(f"âš ï¸ æ•°æ®é›†ä¸å­˜åœ¨: {demo_data_path}")
            return None
        
        print(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {dataset_name}")
        
        # æŸ¥æ‰¾parquetæ–‡ä»¶
        data_dir = demo_data_path / "data"
        if data_dir.exists():
            parquet_files = list(data_dir.glob("*.parquet"))
        else:
            parquet_files = list(demo_data_path.glob("*.parquet"))
        
        if not parquet_files:
            print("âš ï¸ æœªæ‰¾åˆ°parquetæ•°æ®æ–‡ä»¶")
            return None
        
        print(f"âœ… æ‰¾åˆ° {len(parquet_files)} ä¸ªæ•°æ®æ–‡ä»¶:")
        for f in parquet_files[:3]:
            size_mb = f.stat().st_size / (1024 * 1024)
            print(f"   ğŸ“„ {f.name} ({size_mb:.1f}MB)")
        
        # åŠ è½½æ•°æ®
        try:
            import pandas as pd
            
            # åŠ è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹
            sample_data = pd.read_parquet(parquet_files[0])
            print(f"ğŸ“ˆ æ•°æ®å½¢çŠ¶: {sample_data.shape}")
            print(f"ğŸ“‹ åˆ—å: {list(sample_data.columns)[:10]}...")  # åªæ˜¾ç¤ºå‰10åˆ—
            
            return {
                "dataset_name": dataset_name,
                "files": parquet_files,
                "sample_data": sample_data,
                "total_files": len(parquet_files)
            }
            
        except ImportError:
            print("âš ï¸ pandasæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½parquetæ–‡ä»¶")
            print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install pandas pyarrow")
            return None
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return None
    
    def analyze_real_data_episodes(self, data_info, max_episodes=5):
        """åˆ†æçœŸå®æ•°æ®çš„episodes"""
        print(f"\nğŸ§  åˆ†æçœŸå®æœºå™¨äººæ•°æ®: {data_info['dataset_name']}")
        print("=" * 60)
        
        sample_data = data_info["sample_data"]
        
        # å°è¯•ç†è§£æ•°æ®ç»“æ„
        print("ğŸ” æ•°æ®ç»“æ„åˆ†æ:")
        print(f"   æ€»è¡Œæ•°: {len(sample_data)}")
        print(f"   åˆ—æ•°: {len(sample_data.columns)}")
        
        # æŸ¥æ‰¾å¯èƒ½çš„episodeæ ‡è¯†
        potential_episode_cols = [col for col in sample_data.columns 
                                if 'episode' in col.lower() or 'traj' in col.lower()]
        
        if potential_episode_cols:
            print(f"   Episodeç›¸å…³åˆ—: {potential_episode_cols}")
        
        # æ¨¡æ‹Ÿepisodeåˆ†æ
        results = []
        
        print(f"\nğŸ¯ åˆ†æå‰ {max_episodes} ä¸ªæ•°æ®æ®µ...")
        
        # å‡è®¾æ•°æ®æ˜¯è¿ç»­çš„ï¼Œæ¯100è¡Œä½œä¸ºä¸€ä¸ª"episode"
        episode_length = min(100, len(sample_data) // max_episodes)
        
        for episode_idx in range(max_episodes):
            start_idx = episode_idx * episode_length
            end_idx = start_idx + episode_length
            
            if end_idx > len(sample_data):
                break
            
            print(f"\nğŸ“Š åˆ†ææ•°æ®æ®µ {episode_idx + 1} (è¡Œ {start_idx}-{end_idx})...")
            
            episode_data = sample_data.iloc[start_idx:end_idx]
            
            # å…ƒè®¤çŸ¥åˆ†æ
            interventions = 0
            processing_times = []
            
            # æ¯10è¡Œåˆ†æä¸€æ¬¡ï¼ˆæ¨¡æ‹Ÿå®æ—¶åˆ†æï¼‰
            for step in range(0, len(episode_data), 10):
                
                # åˆ›å»ºåŸºäºçœŸå®æ•°æ®çš„ä¼ æ„Ÿå™¨æ•°æ®
                sensor_data = self.create_sensor_data_from_real(episode_data.iloc[step] if step < len(episode_data) else episode_data.iloc[-1])
                
                # å…ƒè®¤çŸ¥å¤„ç†
                start_time = time.time()
                metacog_output = self.metacog_module.process_sensor_data(sensor_data)
                processing_time = time.time() - start_time
                processing_times.append(processing_time)
                
                # æ£€æŸ¥å¹²é¢„
                if metacog_output.directive.value != "continue":
                    interventions += 1
                    print(f"     Step {step//10}: ğŸ§  å…ƒè®¤çŸ¥å¹²é¢„")
                    print(f"       æŒ‡ä»¤: {metacog_output.directive.value}")
                    print(f"       æ¨ç†: {metacog_output.reasoning}")
                    print(f"       ç½®ä¿¡åº¦: {metacog_output.confidence:.3f}")
            
            avg_processing_time = np.mean(processing_times) * 1000
            
            result = {
                "episode": episode_idx + 1,
                "data_range": f"{start_idx}-{end_idx}",
                "interventions": interventions,
                "avg_processing_time_ms": avg_processing_time,
                "data_points_analyzed": len(episode_data)
            }
            results.append(result)
            
            print(f"   ğŸ“ˆ ç»“æœ: {interventions} æ¬¡å¹²é¢„, {avg_processing_time:.1f}ms å¹³å‡å¤„ç†æ—¶é—´")
        
        return results
    
    def create_sensor_data_from_real(self, data_row):
        """ä»çœŸå®æ•°æ®è¡Œåˆ›å»ºä¼ æ„Ÿå™¨æ•°æ®"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„parquetæ•°æ®åˆ—åè°ƒæ•´
        # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œä½†ä¿æŒç»“æ„
        
        return SensorData(
            rgb_image=np.random.rand(224, 224, 3),  # åœ¨å®é™…ä¸­åº”è¯¥ä»æ•°æ®ä¸­æå–
            depth_image=np.random.rand(224, 224),
            force_torque=np.random.rand(6),
            contact_detected=np.random.choice([True, False]),
            joint_positions=np.random.rand(7),
            joint_velocities=np.random.rand(7),
            end_effector_pose=np.random.rand(7),
            system1_commands=np.random.rand(8),
            execution_status="normal",
            timestamp=time.time()
        )
    
    def run_real_data_analysis(self, dataset_name: str):
        """è¿è¡ŒçœŸå®æ•°æ®åˆ†æ"""
        print(f"\nğŸ¯ å¼€å§‹çœŸå®æ•°æ®å…ƒè®¤çŸ¥åˆ†æ")
        print(f"æ•°æ®é›†: {dataset_name}")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        data_info = self.load_real_robot_data(dataset_name)
        
        if data_info is None:
            print("âŒ æ— æ³•åŠ è½½æ•°æ®")
            return None
        
        # 2. åˆ†ææ•°æ®
        results = self.analyze_real_data_episodes(data_info)
        
        # 3. æ€»ç»“ç»“æœ
        self.summarize_real_data_analysis(results, dataset_name)
        
        return results
    
    def summarize_real_data_analysis(self, results, dataset_name):
        """æ€»ç»“çœŸå®æ•°æ®åˆ†æç»“æœ"""
        print("\n" + "=" * 60)
        print("ğŸ“ˆ çœŸå®æ•°æ®å…ƒè®¤çŸ¥åˆ†ææ€»ç»“")
        print("=" * 60)
        
        if not results:
            print("âŒ æ— åˆ†æç»“æœ")
            return
        
        total_interventions = sum(r["interventions"] for r in results)
        avg_processing_time = np.mean([r["avg_processing_time_ms"] for r in results])
        total_data_points = sum(r["data_points_analyzed"] for r in results)
        
        print(f"ğŸ“Š æ•°æ®é›†: {dataset_name}")
        print(f"   åˆ†æçš„æ•°æ®æ®µ: {len(results)}")
        print(f"   æ€»æ•°æ®ç‚¹: {total_data_points}")
        print(f"   æ€»å¹²é¢„æ¬¡æ•°: {total_interventions}")
        print(f"   å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.1f}ms")
        print(f"   å¹²é¢„ç‡: {total_interventions/len(results)/10:.1%} (æ¯æ®µå¹³å‡)")
        
        # ä¿å­˜ç»“æœ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = f"real_data_analysis_{dataset_name}_{timestamp}.json"
        
        analysis_summary = {
            "timestamp": timestamp,
            "dataset": dataset_name,
            "analysis_results": results,
            "summary": {
                "total_interventions": total_interventions,
                "avg_processing_time_ms": avg_processing_time,
                "total_data_points": total_data_points,
                "intervention_rate": total_interventions/len(results)/10
            }
        }
        
        with open(results_file, 'w') as f:
            json.dump(analysis_summary, f, indent=2, default=str)
        
        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜: {results_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¯åŠ¨çœŸå®Isaac GR00Tå…ƒè®¤çŸ¥é›†æˆ")
    print("åŸºäºæ‚¨çš„å®Œæ•´GR00Tç¯å¢ƒ")
    
    # åˆ›å»ºçœŸå®é›†æˆç³»ç»Ÿ
    system = RealGR00TMetacognitiveSystem()
    
    try:
        print(f"\nå¯ç”¨æ•°æ®é›†: {system.available_datasets}")
        
        mode = input("\né€‰æ‹©è¿è¡Œæ¨¡å¼:\n1. åˆ†æçœŸå®æœºå™¨äººæ•°æ®\n2. å¿«é€Ÿæ¼”ç¤º\nè¯·è¾“å…¥(1/2): ").strip()
        
        if mode == "1" and system.available_datasets:
            # é€‰æ‹©æ•°æ®é›†
            print(f"\nå¯ç”¨çš„æ•°æ®é›†:")
            for i, dataset in enumerate(system.available_datasets):
                print(f"  {i+1}. {dataset}")
            
            choice = input(f"é€‰æ‹©æ•°æ®é›†ç¼–å·(1-{len(system.available_datasets)}): ").strip()
            
            try:
                dataset_idx = int(choice) - 1
                if 0 <= dataset_idx < len(system.available_datasets):
                    dataset_name = system.available_datasets[dataset_idx]
                    results = system.run_real_data_analysis(dataset_name)
                else:
                    print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®é›†")
                    results = system.run_real_data_analysis(system.available_datasets[0])
            except ValueError:
                print("è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®é›†")
                results = system.run_real_data_analysis(system.available_datasets[0])
                
        else:
            # å¿«é€Ÿæ¼”ç¤º
            print("\nğŸš€ è¿è¡Œå¿«é€Ÿæ¼”ç¤º...")
            demo = SimpleDemo()
            results = demo.run_quick_demo()
            
            print(f"\nğŸ“Š æ¼”ç¤ºæ€»ç»“:")
            for r in results:
                print(f"   Episode {r['episode']}: {'âœ…' if r['success'] else 'âŒ'} | {r['interventions']} å¹²é¢„ | {r['avg_time_ms']:.1f}ms")
    
    except KeyboardInterrupt:
        print("\nå®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ å®éªŒå®Œæˆ!")

if __name__ == "__main__":
    main()