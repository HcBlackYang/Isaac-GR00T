# #!/usr/bin/env python3
# """
# é›†æˆçœŸå®RoboCasaç¯å¢ƒçš„ä¿®æ”¹ç‰ˆæœ¬
# Real RoboCasa Environment Integration - Modified Version
# """

# import os
# import sys
# import time
# import json
# import numpy as np
# import torch
# from pathlib import Path
# from typing import Dict, List, Any, Tuple, Optional
# from dataclasses import dataclass, asdict
# from datetime import datetime
# import cv2

# # å¯¼å…¥RoboCasa
# try:
#     import robocasa
#     from robocasa.environments.kitchen.kitchen import Kitchen
#     ROBOCASA_AVAILABLE = True
#     print("âœ… RoboCasaå¯ç”¨")
# except ImportError as e:
#     print(f"âŒ RoboCasaä¸å¯ç”¨: {e}")
#     print("è¯·å®‰è£…RoboCasa: pip install robocasa")
#     ROBOCASA_AVAILABLE = False

# # å¯¼å…¥GR00Tå®˜æ–¹å®¢æˆ·ç«¯
# try:
#     from gr00t.eval.robot import RobotInferenceClient
#     GROOT_CLIENT_AVAILABLE = True
#     print("âœ… GR00Tå®˜æ–¹å®¢æˆ·ç«¯å¯ç”¨")
# except ImportError as e:
#     print(f"âŒ GR00Tå®˜æ–¹å®¢æˆ·ç«¯ä¸å¯ç”¨: {e}")
#     GROOT_CLIENT_AVAILABLE = False

# # å¯¼å…¥å…ƒè®¤çŸ¥æ¨¡å—
# try:
#     from metacog_integration import (
#         CompleteMetaCognitiveModule,
#         RoboCasaToMetacogAdapter,
#         MetacogToGR00TAdapter,
#         ActionAdjuster,
#         SensorData,
#         MetaCognitiveOutput,
#         DirectiveType,
#         MetacogToVLASystem2Adapter
#     )
#     METACOG_AVAILABLE = True
#     print("âœ… å…ƒè®¤çŸ¥æ¨¡å—å¯ç”¨")
# except ImportError as e:
#     print(f"âŒ å…ƒè®¤çŸ¥æ¨¡å—ä¸å¯ç”¨: {e}")
#     METACOG_AVAILABLE = False

# # ==================== RoboCasaä»»åŠ¡é€‰æ‹©å™¨ ====================

# class RoboCasaTaskSelector:
#     """RoboCasaä»»åŠ¡é€‰æ‹©å’ŒéªŒè¯ï¼ˆä½¿ç”¨å®é™…å¯ç”¨çš„ä»»åŠ¡åç§°ï¼‰"""
    
#     # åŸºäºå®é™…RoboCasaæ³¨å†Œçš„ç¯å¢ƒåç§°
#     ATOMIC_TASKS = {
#         # ç®€å•ä»»åŠ¡ï¼ˆæ¨èç”¨äºæµ‹è¯•ï¼‰
#         "beginner": [
#             "Lift",                    # åŸºç¡€æŠ“å–ä»»åŠ¡
#             "Stack",                   # å †å ä»»åŠ¡
#             "PnP",                     # Pick and Place
#             "PnPCounterToSink",        # æŸœå°åˆ°æ°´æ§½
#             "OpenDoor",                # å¼€é—¨
#             "CloseDoor",               # å…³é—¨
#             "OpenSingleDoor",          # å¼€å•é—¨
#             "CloseSingleDoor"          # å…³å•é—¨
#         ],
        
#         # ä¸­ç­‰éš¾åº¦ä»»åŠ¡
#         "intermediate": [
#             "PnPCounterToCab",         # æŸœå°åˆ°æ©±æŸœ
#             "PnPCabToCounter",         # æ©±æŸœåˆ°æŸœå°
#             "PnPSinkToCounter",        # æ°´æ§½åˆ°æŸœå°
#             "OpenDrawer",              # å¼€æŠ½å±‰
#             "CloseDrawer",             # å…³æŠ½å±‰
#             "TurnOnSinkFaucet",        # å¼€æ°´é¾™å¤´
#             "TurnOffSinkFaucet",       # å…³æ°´é¾™å¤´
#             "OpenDoubleDoor",          # å¼€åŒé—¨
#             "CloseDoubleDoor"          # å…³åŒé—¨
#         ],
        
#         # é«˜éš¾åº¦ä»»åŠ¡
#         "advanced": [
#             "Kitchen",                 # ç»¼åˆå¨æˆ¿ä»»åŠ¡
#             "KitchenDemo",             # å¨æˆ¿æ¼”ç¤º
#             "TurnOnMicrowave",         # å¼€å¾®æ³¢ç‚‰
#             "TurnOffMicrowave",        # å…³å¾®æ³¢ç‚‰
#             "TurnOnStove",             # å¼€ç‚‰å­
#             "TurnOffStove",            # å…³ç‚‰å­
#             "CoffeeSetupMug",          # å’–å•¡æ¯è®¾ç½®
#             "CoffeeServeMug",          # å’–å•¡æœåŠ¡
#             "PrepareCoffee",           # å‡†å¤‡å’–å•¡
#             "CupcakeCleanup"           # çº¸æ¯è›‹ç³•æ¸…ç†
#         ]
#     }
    
#     @classmethod
#     def get_test_tasks(cls) -> List[str]:
#         """è·å–ç”¨äºæµ‹è¯•çš„ç®€å•ä»»åŠ¡"""
#         return cls.ATOMIC_TASKS["beginner"]
    
#     @classmethod
#     def get_all_tasks(cls) -> List[str]:
#         """è·å–æ‰€æœ‰åŸå­ä»»åŠ¡"""
#         all_tasks = []
#         for category in cls.ATOMIC_TASKS.values():
#             all_tasks.extend(category)
#         return all_tasks
    
#     @classmethod
#     def validate_task(cls, task_name: str) -> bool:
#         """éªŒè¯ä»»åŠ¡åç§°æ˜¯å¦æœ‰æ•ˆ"""
#         return task_name in cls.get_all_tasks()
    
#     @classmethod
#     def print_available_tasks(cls):
#         """æ‰“å°æ‰€æœ‰å¯ç”¨ä»»åŠ¡"""
#         print("ğŸ“‹ å¯ç”¨çš„RoboCasaä»»åŠ¡:")
#         for category, tasks in cls.ATOMIC_TASKS.items():
#             print(f"\nğŸ¯ {category.upper()} ä»»åŠ¡:")
#             for i, task in enumerate(tasks, 1):
#                 print(f"   {i}. {task}")
    
#     @classmethod
#     def get_recommended_task(cls) -> str:
#         """è·å–æ¨èçš„æµ‹è¯•ä»»åŠ¡"""
#         return cls.ATOMIC_TASKS["beginner"][0]  # è¿”å›æœ€ç®€å•çš„ä»»åŠ¡

# # ==================== çœŸå®RoboCasaç¯å¢ƒåŒ…è£…å™¨ ====================

# class RealRoboCasaEnvironment:
#     """çœŸå®çš„RoboCasaç¯å¢ƒåŒ…è£…å™¨"""
    
#     def __init__(self, 
#                  task_name: str = "Lift",
#                  robot_type: str = "PandaMobile",
#                  horizon: int = 500,
#                  camera_names: List[str] = None):
#         """
#         åˆå§‹åŒ–çœŸå®RoboCasaç¯å¢ƒ
        
#         Args:
#             task_name: RoboCasaä»»åŠ¡åç§°
#             robot_type: æœºå™¨äººç±»å‹
#             horizon: æœ€å¤§æ­¥æ•°
#             camera_names: ç›¸æœºåç§°åˆ—è¡¨
#         """
#         if not ROBOCASA_AVAILABLE:
#             raise ImportError("RoboCasaä¸å¯ç”¨ï¼Œè¯·å…ˆå®‰è£…")
        
#         self.task_name = task_name
#         self.robot_type = robot_type
#         self.horizon = horizon
        
#         # ä½¿ç”¨å®é™…å¯ç”¨çš„ç›¸æœºåç§°
#         if camera_names is None:
#             self.camera_names = ["robot0_frontview", "robot0_eye_in_hand"]
#         else:
#             self.camera_names = camera_names
        
#         print(f"ğŸ—ï¸ åˆ›å»ºRoboCasaç¯å¢ƒ: {task_name}")
#         print(f"   æœºå™¨äºº: {robot_type}")
#         print(f"   ç›¸æœº: {self.camera_names}")
#         print(f"   æœ€å¤§æ­¥æ•°: {horizon}")
        
#         # éªŒè¯ä»»åŠ¡åç§°
#         if not RoboCasaTaskSelector.validate_task(task_name):
#             print(f"âš ï¸ ä»»åŠ¡åç§° '{task_name}' ä¸åœ¨å·²çŸ¥ä»»åŠ¡åˆ—è¡¨ä¸­")
#             print(f"ğŸ’¡ æ¨èä½¿ç”¨æµ‹è¯•ä»»åŠ¡:")
#             for task in RoboCasaTaskSelector.get_test_tasks()[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
#                 print(f"   - {task}")
#             print(f"ğŸ“‹ è¿è¡Œ RoboCasaTaskSelector.print_available_tasks() æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡")
        
#         self.env = None
#         self.current_step = 0
#         self.last_observation = None
#         self.task_completed = False
        
#         # å°è¯•åˆ›å»ºç¯å¢ƒ
#         self._create_environment()
    
#     def _create_environment(self):
#         """åˆ›å»ºRoboCasaç¯å¢ƒå®ä¾‹"""
#         try:
#             print("ğŸ“¦ æ­£åœ¨åˆ›å»ºRoboCasaç¯å¢ƒ...")
            
#             # åˆ›å»ºç¯å¢ƒçš„å‚æ•°
#             env_kwargs = {
#                 "robots": self.robot_type,
#                 "has_renderer": False,           # è®­ç»ƒæ—¶å…³é—­æ¸²æŸ“
#                 "has_offscreen_renderer": True,  # å¯ç”¨ç¦»å±æ¸²æŸ“è·å–å›¾åƒ
#                 "render_camera": "robot0_frontview",  # ä½¿ç”¨å®é™…å­˜åœ¨çš„ç›¸æœº
#                 "render_collision_mesh": False,
#                 "render_visual_mesh": True,
#                 "control_freq": 20,
#                 "horizon": self.horizon,
#                 "ignore_done": True,
#                 "hard_reset": True,
#                 "camera_names": self.camera_names,
#                 "camera_heights": 480,           # ä½¿ç”¨640x480åˆ†è¾¨ç‡
#                 "camera_widths": 640,
#                 "camera_depths": True,           # å¯ç”¨æ·±åº¦ä¿¡æ¯
#                 "reward_shaping": True,
#                 "use_camera_obs": True,
#                 "use_object_obs": True,
#             }
            
#             # åˆ›å»ºç¯å¢ƒ
#             self.env = robocasa.make(self.task_name, **env_kwargs)
            
#             print(f"âœ… RoboCasaç¯å¢ƒåˆ›å»ºæˆåŠŸ!")
#             print(f"   ä»»åŠ¡: {self.task_name}")
#             print(f"   ç›¸æœº: {self.camera_names}")
            
#             # å®‰å…¨åœ°è·å–åŠ¨ä½œç©ºé—´ä¿¡æ¯
#             try:
#                 if hasattr(self.env, 'action_space'):
#                     print(f"   åŠ¨ä½œç©ºé—´: {self.env.action_space}")
#                 elif hasattr(self.env, 'action_spec'):
#                     action_spec = self.env.action_spec()
#                     print(f"   åŠ¨ä½œè§„æ ¼: {action_spec}")
#                 elif hasattr(self.env, '_get_action_space'):
#                     action_space = self.env._get_action_space()
#                     print(f"   åŠ¨ä½œç©ºé—´: {action_space}")
#                 else:
#                     print(f"   åŠ¨ä½œç©ºé—´: æœªçŸ¥ (ç¯å¢ƒç±»å‹: {type(self.env).__name__})")
#             except Exception as e:
#                 print(f"   åŠ¨ä½œç©ºé—´: è·å–å¤±è´¥ - {e}")
            
#             # å®‰å…¨åœ°è·å–è§‚æµ‹ç©ºé—´ä¿¡æ¯  
#             try:
#                 if hasattr(self.env, 'observation_space'):
#                     obs_keys = list(self.env.observation_space.spaces.keys()) if hasattr(self.env.observation_space, 'spaces') else 'Unknown'
#                     print(f"   è§‚æµ‹ç©ºé—´é”®: {obs_keys}")
#                 elif hasattr(self.env, 'observation_spec'):
#                     obs_spec = self.env.observation_spec()
#                     obs_keys = list(obs_spec.keys()) if isinstance(obs_spec, dict) else 'Unknown'
#                     print(f"   è§‚æµ‹è§„æ ¼é”®: {obs_keys}")
#                 else:
#                     print(f"   è§‚æµ‹ç©ºé—´: æœªçŸ¥")
#             except Exception as e:
#                 print(f"   è§‚æµ‹ç©ºé—´: è·å–å¤±è´¥ - {e}")
            
#             return True
            
#         except Exception as e:
#             print(f"âŒ åˆ›å»ºRoboCasaç¯å¢ƒå¤±è´¥: {e}")
            
#             # å¦‚æœé”™è¯¯ä¿¡æ¯åŒ…å«å¯ç”¨ç¯å¢ƒåˆ—è¡¨ï¼Œæå–å¹¶æ˜¾ç¤º
#             error_str = str(e)
#             if "registered environment among:" in error_str:
#                 # æå–æ³¨å†Œçš„ç¯å¢ƒåˆ—è¡¨
#                 env_list_start = error_str.find("registered environment among:") + 30
#                 env_list = error_str[env_list_start:].strip()
#                 available_envs = [env.strip() for env in env_list.split(",")]
                
#                 print(f"\nğŸ“‹ RoboCasaä¸­å®é™…å¯ç”¨çš„ç¯å¢ƒ:")
#                 print(f"æ€»å…± {len(available_envs)} ä¸ªç¯å¢ƒ")
                
#                 # æŒ‰ç±»åˆ«æ˜¾ç¤ºéƒ¨åˆ†ç¯å¢ƒ
#                 basic_envs = [env for env in available_envs if env in ["Lift", "Stack", "PnP", "Kitchen"]]
#                 pnp_envs = [env for env in available_envs if env.startswith("PnP")]
#                 door_envs = [env for env in available_envs if "Door" in env]
                
#                 if basic_envs:
#                     print(f"ğŸ¯ åŸºç¡€ä»»åŠ¡: {basic_envs}")
#                 if pnp_envs[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
#                     print(f"ğŸ“¦ æŠ“å–ä»»åŠ¡: {pnp_envs[:5]}")
#                 if door_envs[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª  
#                     print(f"ğŸšª å¼€å…³é—¨ä»»åŠ¡: {door_envs[:3]}")
                
#                 print(f"ğŸ’¡ å»ºè®®é¦–å…ˆå°è¯•: Lift (æœ€åŸºç¡€çš„æŠ“å–ä»»åŠ¡)")
            
#             print(f"\né”™è¯¯è¯¦æƒ…:")
#             import traceback
#             traceback.print_exc()
            
#             # æä¾›è§£å†³å»ºè®®
#             print(f"\nğŸ”§ è§£å†³å»ºè®®:")
#             print(f"1. æ£€æŸ¥ä»»åŠ¡åç§°æ˜¯å¦æ­£ç¡®: '{self.task_name}'")
#             print(f"   å¯ç”¨çš„ç®€å•ä»»åŠ¡: {RoboCasaTaskSelector.get_test_tasks()}")
#             print(f"2. æ£€æŸ¥æœºå™¨äººç±»å‹æ˜¯å¦æ”¯æŒ: '{self.robot_type}'")
#             print(f"   å»ºè®®ä½¿ç”¨: PandaMobile æˆ– Panda")
#             print(f"3. ç¡®ä¿RoboCasaæ­£ç¡®å®‰è£…å¹¶ä¸‹è½½äº†å¿…è¦èµ„æº")
#             print(f"4. å°è¯•æœ€ç®€å•çš„ä»»åŠ¡: '{RoboCasaTaskSelector.get_recommended_task()}'")
#             print(f"5. è¿è¡Œ RoboCasaTaskSelector.print_available_tasks() æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡")
            
#             raise
    
#     def reset(self) -> Tuple[Dict[str, Any], Dict[str, Any]]:
#         """é‡ç½®ç¯å¢ƒå¹¶è¿”å›åˆå§‹è§‚æµ‹"""
#         if self.env is None:
#             raise RuntimeError("ç¯å¢ƒæœªæ­£ç¡®åˆå§‹åŒ–")
        
#         try:
#             print(f"ğŸ”„ é‡ç½®RoboCasaç¯å¢ƒ: {self.task_name}")
            
#             obs = self.env.reset()
#             self.current_step = 0
#             self.last_observation = obs
#             self.task_completed = False
            
#             # å¤„ç†è§‚æµ‹æ•°æ®æ ¼å¼
#             processed_obs = self._process_observation(obs)
            
#             info = {
#                 "task_name": self.task_name,
#                 "step": self.current_step,
#                 "max_steps": self.horizon
#             }
            
#             print(f"âœ… ç¯å¢ƒé‡ç½®æˆåŠŸ")
#             print(f"   è§‚æµ‹é”®: {list(processed_obs.keys())}")
#             self._print_observation_info(processed_obs)
            
#             return processed_obs, info
            
#         except Exception as e:
#             print(f"âŒ ç¯å¢ƒé‡ç½®å¤±è´¥: {e}")
#             raise
    
#     def step(self, action: np.ndarray) -> Tuple[Dict[str, Any], float, bool, bool, Dict[str, Any]]:
#         """æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›ä¸‹ä¸€æ­¥ä¿¡æ¯"""
#         if self.env is None:
#             raise RuntimeError("ç¯å¢ƒæœªæ­£ç¡®åˆå§‹åŒ–")
        
#         try:
#             # ç¡®ä¿åŠ¨ä½œæ ¼å¼æ­£ç¡®
#             if not isinstance(action, np.ndarray):
#                 action = np.array(action)
            
#             # æ‰§è¡ŒåŠ¨ä½œ
#             obs, reward, done, info = self.env.step(action)
#             self.current_step += 1
#             self.last_observation = obs
            
#             # å¤„ç†è§‚æµ‹æ•°æ®
#             processed_obs = self._process_observation(obs)
            
#             # æ£€æŸ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€
#             task_success = info.get("success", False) or reward > 0.9
#             if task_success:
#                 self.task_completed = True
#                 done = True
            
#             # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
#             if self.current_step >= self.horizon:
#                 done = True
            
#             # å¢å¼ºinfoä¿¡æ¯
#             enhanced_info = {
#                 **info,
#                 "task_name": self.task_name,
#                 "step": self.current_step,
#                 "max_steps": self.horizon,
#                 "task_success": task_success,
#                 "task_completed": self.task_completed,
#                 "action_taken": action.tolist() if hasattr(action, 'tolist') else action
#             }
            
#             return processed_obs, reward, done, False, enhanced_info
            
#         except Exception as e:
#             print(f"âŒ ç¯å¢ƒæ­¥è¿›å¤±è´¥: {e}")
#             print(f"åŠ¨ä½œå½¢çŠ¶: {action.shape if hasattr(action, 'shape') else 'æœªçŸ¥'}")
#             print(f"åŠ¨ä½œå†…å®¹: {action}")
#             raise
    
#     def _process_observation(self, obs: Dict[str, Any]) -> Dict[str, np.ndarray]:
#         """å¤„ç†è§‚æµ‹æ•°æ®ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®"""
#         processed = {}
        
#         # å¤„ç†å›¾åƒæ•°æ®
#         for camera in self.camera_names:
#             # RGBå›¾åƒ
#             rgb_key = f"{camera}_image"
#             if rgb_key in obs:
#                 img = obs[rgb_key]
#                 # ç¡®ä¿æ˜¯æ­£ç¡®çš„åˆ†è¾¨ç‡ (480, 640, 3)
#                 if img.shape != (480, 640, 3):
#                     img = cv2.resize(img, (640, 480))
#                     if len(img.shape) == 3 and img.shape[2] == 3:
#                         processed[f"frontview_image"] = img.astype(np.uint8)
#                     else:
#                         processed[f"frontview_image"] = np.zeros((480, 640, 3), dtype=np.uint8)
#                 else:
#                     processed[f"frontview_image"] = img.astype(np.uint8)
#                 break  # åªéœ€è¦ä¸€ä¸ªå›¾åƒæº
            
#             # æ·±åº¦å›¾åƒ
#             depth_key = f"{camera}_depth"
#             if depth_key in obs:
#                 depth = obs[depth_key]
#                 if depth.shape != (480, 640):
#                     depth = cv2.resize(depth, (640, 480))
#                 processed[f"frontview_depth"] = depth.astype(np.float32)
#                 break  # åªéœ€è¦ä¸€ä¸ªæ·±åº¦æº
        
#         # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å›¾åƒï¼Œä½¿ç”¨é»˜è®¤å€¼
#         if "frontview_image" not in processed:
#             processed["frontview_image"] = np.zeros((480, 640, 3), dtype=np.uint8)
        
#         # å¤„ç†æœºå™¨äººçŠ¶æ€æ•°æ®
#         robot_keys = [
#             "robot0_joint_pos", "robot0_joint_vel", 
#             "robot0_eef_pos", "robot0_eef_quat",
#             "robot0_gripper_qpos", "robot0_gripper_qvel"
#         ]
        
#         for key in robot_keys:
#             if key in obs:
#                 processed[key] = np.array(obs[key], dtype=np.float32)
        
#         # å¦‚æœç¼ºå°‘å…³é”®æ•°æ®ï¼Œç”Ÿæˆé»˜è®¤å€¼
#         if "robot0_joint_pos" not in processed:
#             processed["robot0_joint_pos"] = np.zeros(7, dtype=np.float32)
#         if "robot0_joint_vel" not in processed:
#             processed["robot0_joint_vel"] = np.zeros(7, dtype=np.float32)
#         if "robot0_eef_pos" not in processed:
#             processed["robot0_eef_pos"] = np.array([0.5, 0.0, 0.8], dtype=np.float32)
#         if "robot0_eef_quat" not in processed:
#             processed["robot0_eef_quat"] = np.array([0, 0, 0, 1], dtype=np.float32)
#         if "robot0_gripper_qpos" not in processed:
#             processed["robot0_gripper_qpos"] = np.zeros(2, dtype=np.float32)
#         if "frontview_image" not in processed:
#             processed["frontview_image"] = np.zeros((480, 640, 3), dtype=np.uint8)
        
#         return processed
    
#     def _print_observation_info(self, obs: Dict[str, Any]):
#         """æ‰“å°è§‚æµ‹ä¿¡æ¯"""
#         print(f"ğŸ“Š è§‚æµ‹æ•°æ®è¯¦æƒ…:")
#         for key, value in obs.items():
#             if isinstance(value, np.ndarray):
#                 print(f"   {key}: shape={value.shape}, dtype={value.dtype}")
#             else:
#                 print(f"   {key}: {type(value)}")
    
#     def get_action_space(self):
#         """è·å–åŠ¨ä½œç©ºé—´ï¼ˆå…¼å®¹å¤šç§RoboCasaç‰ˆæœ¬ï¼‰"""
#         if self.env is None:
#             raise RuntimeError("ç¯å¢ƒæœªåˆå§‹åŒ–")
        
        
#         # ... in RealRoboCasaEnvironment.get_action_space
#         # å°è¯• 'action_spec' å±æ€§
#         if hasattr(self.env, 'action_spec'):
#             action_spec = self.env.action_spec
#             # robosuite 0.3.0 ç‰ˆæœ¬çš„ action_spec æ˜¯ä¸€ä¸ªå…ƒç»„ (low, high)
#             if isinstance(action_spec, tuple) and len(action_spec) == 2:
#                 return action_spec

#         # å°è¯• 'action_space' å±æ€§ (å…¼å®¹gym)
#         if hasattr(self.env, 'action_space'):
#             return self.env.action_space

#         # å°è¯• '_action_space'
#         if hasattr(self.env, '_action_space'):
#             return self.env._action_space

#         # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤çš„åŠ¨ä½œç©ºé—´æè¿°
#         print("âš ï¸ æ— æ³•è·å–æ ‡å‡†åŠ¨ä½œç©ºé—´ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
#         # æ ¹æ®æ—¥å¿—ï¼ŒPandaMobileçš„ç»´åº¦æ˜¯12
#         import gym.spaces
#         return gym.spaces.Box(low=-1.0, high=1.0, shape=(12,), dtype=np.float32)


#         # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤çš„åŠ¨ä½œç©ºé—´æè¿°
#         print("âš ï¸ æ— æ³•è·å–æ ‡å‡†åŠ¨ä½œç©ºé—´ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
#         import gym.spaces
#         return gym.spaces.Box(low=-1.0, high=1.0, shape=(7,), dtype=np.float32)
    
#     def get_observation_space(self):
#         """è·å–è§‚æµ‹ç©ºé—´ï¼ˆå…¼å®¹å¤šç§RoboCasaç‰ˆæœ¬ï¼‰"""
#         if self.env is None:
#             raise RuntimeError("ç¯å¢ƒæœªåˆå§‹åŒ–")
        
#         # å°è¯•å¤šç§å¯èƒ½çš„å±æ€§å
#         for attr_name in ['observation_space', 'observation_spec', '_observation_space']:
#             if hasattr(self.env, attr_name):
#                 attr = getattr(self.env, attr_name)
#                 if callable(attr):
#                     return attr()
#                 else:
#                     return attr
        
#         return None
    
#     def close(self):
#         """å…³é—­ç¯å¢ƒ"""
#         if self.env is not None:
#             try:
#                 self.env.close()
#                 print(f"ğŸ”’ RoboCasaç¯å¢ƒå·²å…³é—­: {self.task_name}")
#             except:
#                 pass
#             finally:
#                 self.env = None

# # ==================== ç›¸æœºé…ç½®æµ‹è¯•å™¨ ====================

# class RoboCasaCameraConfigTester:
#     """RoboCasaç›¸æœºé…ç½®æµ‹è¯•å™¨"""
    
#     def __init__(self):
#         self.available_cameras = [
#             "robot0_frontview",
#             "robot0_agentview_center", 
#             "robot0_agentview_left",
#             "robot0_agentview_right",
#             "robot0_robotview",
#             "robot0_eye_in_hand"
#         ]
        
#         # æ¨èçš„ç›¸æœºé…ç½®ç»„åˆ
#         self.camera_configs = [
#             ["robot0_frontview"],                                    # å•å‰è§†å›¾
#             ["robot0_agentview_center"],                            # å•ä¸­å¿ƒè§†å›¾  
#             ["robot0_robotview"],                                   # å•æœºå™¨äººè§†å›¾
#             ["robot0_frontview", "robot0_eye_in_hand"],             # å‰è§†å›¾+æ‰‹éƒ¨
#             ["robot0_agentview_center", "robot0_eye_in_hand"],      # ä¸­å¿ƒ+æ‰‹éƒ¨
#             ["robot0_robotview", "robot0_eye_in_hand"]              # æœºå™¨äºº+æ‰‹éƒ¨
#         ]
    
#     def find_working_camera_config(self, task_name: str = "Lift") -> Optional[List[str]]:
#         """æ‰¾åˆ°å¯å·¥ä½œçš„ç›¸æœºé…ç½®"""
#         print(f"\nğŸ“· æµ‹è¯•ç›¸æœºé…ç½® (ä»»åŠ¡: {task_name})")
        
#         for i, cam_config in enumerate(self.camera_configs):
#             print(f"   æµ‹è¯•é…ç½® {i+1}: {cam_config}")
            
#             try:
#                 # åŸºç¡€ç¯å¢ƒå‚æ•°
#                 test_kwargs = {
#                     "robots": "PandaMobile",
#                     "has_renderer": False,
#                     "has_offscreen_renderer": True,
#                     "render_camera": cam_config[0],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç›¸æœºä½œä¸ºæ¸²æŸ“ç›¸æœº
#                     "camera_names": cam_config,
#                     "camera_heights": 480,
#                     "camera_widths": 640,
#                     "camera_depths": True,
#                     "horizon": 50,  # çŸ­æ—¶é—´æµ‹è¯•
#                     "use_camera_obs": True,
#                 }
                
#                 # å°è¯•åˆ›å»ºç¯å¢ƒ
#                 test_env = robocasa.make(task_name, **test_kwargs)
                
#                 # å°è¯•reset
#                 obs = test_env.reset()
                
#                 # æ£€æŸ¥ç›¸æœºæ•°æ®æ˜¯å¦å­˜åœ¨
#                 camera_data_found = False
#                 for cam in cam_config:
#                     if f"{cam}_image" in obs:
#                         camera_data_found = True
#                         img_shape = obs[f"{cam}_image"].shape
#                         print(f"     âœ… {cam}: å›¾åƒå½¢çŠ¶ {img_shape}")
                
#                 test_env.close()
                
#                 if camera_data_found:
#                     print(f"   âœ… é…ç½® {i+1} æˆåŠŸ!")
#                     return cam_config
#                 else:
#                     print(f"   âŒ é…ç½® {i+1}: æ— ç›¸æœºæ•°æ®")
                    
#             except Exception as e:
#                 print(f"   âŒ é…ç½® {i+1}: {str(e)[:60]}...")
        
#         print(f"   âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç›¸æœºé…ç½®")
#         return None

# # ==================== ç¯å¢ƒæµ‹è¯•å™¨ ====================

# class RoboCasaEnvironmentTester:
#     """RoboCasaç¯å¢ƒæµ‹è¯•å™¨"""
    
#     def __init__(self):
#         self.test_results = {}
#         self.camera_tester = RoboCasaCameraConfigTester()
    
#     def test_environment_creation(self, task_name: str) -> bool:
#         """æµ‹è¯•ç¯å¢ƒåˆ›å»º"""
#         print(f"\nğŸ§ª æµ‹è¯•ç¯å¢ƒåˆ›å»º: {task_name}")
        
#         # é¦–å…ˆæµ‹è¯•ç›¸æœºé…ç½®
#         working_cameras = self.camera_tester.find_working_camera_config(task_name)
        
#         if not working_cameras:
#             print(f"âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç›¸æœºé…ç½®")
#             return False
        
#         try:
#             env = RealRoboCasaEnvironment(
#                 task_name=task_name,
#                 robot_type="PandaMobile",
#                 horizon=100,  # çŸ­æ—¶é—´æµ‹è¯•
#                 camera_names=working_cameras  # ä½¿ç”¨æµ‹è¯•é€šè¿‡çš„ç›¸æœº
#             )
            
#             print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
            
#             # å®‰å…¨åœ°æµ‹è¯•åŠ¨ä½œç©ºé—´
#             try:
#                 action_space = env.get_action_space()
#                 print(f"ğŸ“ åŠ¨ä½œç©ºé—´: {action_space}")
#             except Exception as e:
#                 print(f"ğŸ“ åŠ¨ä½œç©ºé—´: è·å–å¤±è´¥ - {e}")
            
#             env.close()
#             return True
            
#         except Exception as e:
#             print(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
#             return False
    
#     def test_environment_functionality(self, task_name: str, max_steps: int = 10) -> Dict[str, Any]:
#         """æµ‹è¯•ç¯å¢ƒåŠŸèƒ½"""
#         print(f"\nğŸ§ª æµ‹è¯•ç¯å¢ƒåŠŸèƒ½: {task_name}")
        
#         result = {
#             "task_name": task_name,
#             "creation_success": False,
#             "reset_success": False,
#             "step_success": False,
#             "steps_completed": 0,
#             "error": None,
#             "camera_config": None
#         }
        
#         try:
#             # é¦–å…ˆæµ‹è¯•ç›¸æœºé…ç½®
#             working_cameras = self.camera_tester.find_working_camera_config(task_name)
            
#             if not working_cameras:
#                 result["error"] = "No working camera configuration found"
#                 return result
            
#             result["camera_config"] = working_cameras
            
#             # 1. åˆ›å»ºç¯å¢ƒ
#             env = RealRoboCasaEnvironment(
#                 task_name=task_name, 
#                 horizon=max_steps * 2,
#                 camera_names=working_cameras
#             )
#             result["creation_success"] = True
            
#             # 2. æµ‹è¯•reset
#             obs, info = env.reset()
#             result["reset_success"] = True
#             print(f"âœ… ResetæˆåŠŸï¼Œè§‚æµ‹é”®: {list(obs.keys())}")
            
#             # éªŒè¯ç›¸æœºæ•°æ®
#             for cam in working_cameras:
#                 img_key = f"{cam}_image"
#                 if img_key in obs:
#                     print(f"   ğŸ“· {cam}: {obs[img_key].shape}")
            
#             # 3. æµ‹è¯•step
#             try:
#                 action_space = env.get_action_space()
#                 print(f"ğŸ“ ä½¿ç”¨åŠ¨ä½œç©ºé—´: {action_space}")
#             except Exception as e:
#                 print(f"âš ï¸ åŠ¨ä½œç©ºé—´è·å–å¤±è´¥: {e}")
#                 action_space = None
            
#             for step in range(max_steps):
#                 # ç”ŸæˆéšæœºåŠ¨ä½œ

#                 # ... in RoboCasaEnvironmentTester.test_environment_functionality
#                 try:
#                     if action_space is not None and hasattr(action_space, 'sample'):
#                         action = action_space.sample()
#                     elif action_space is not None:
#                         # è¿™æ˜¯ä¸€ä¸ªæ›´é€šç”¨çš„æ–¹å¼æ¥å¤„ç†ä» robosuite è¿”å›çš„åŠ¨ä½œç©ºé—´
#                         # robosuite çš„ action_spec() è¿”å›çš„æ˜¯ (low, high) å…ƒç»„
#                         low, high = action_space
#                         if isinstance(low, np.ndarray) and isinstance(high, np.ndarray):
#                             action = np.random.uniform(low, high)
#                         else:
#                             # å¦‚æœæ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œæ ¹æ®å½¢çŠ¶ç”Ÿæˆ
#                             action_shape = np.array(low).shape
#                             action = np.random.uniform(-0.1, 0.1, action_shape)
#                     else:
#                         # å¦‚æœè·å–åŠ¨ä½œç©ºé—´å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤ä½†å¯èƒ½é”™è¯¯çš„ç»´åº¦
#                         print("   âš ï¸ æ— æ³•ç¡®å®šåŠ¨ä½œç©ºé—´ï¼Œä½¿ç”¨é»˜è®¤12ç»´åŠ¨ä½œ")
#                         action = np.random.uniform(-0.1, 0.1, 12) # å°†7æ”¹ä¸º12ï¼Œå› ä¸ºæ—¥å¿—æ˜¾ç¤ºæ˜¯12

#                     # ç¡®ä¿åŠ¨ä½œåœ¨[-1, 1]èŒƒå›´å†…ï¼Œè¿™æ˜¯robosuiteçš„æ™®éè¦æ±‚
#                     action = np.clip(action, -1.0, 1.0)
                    
#                     print(f"   ğŸ¯ åŠ¨ä½œå½¢çŠ¶: {action.shape}, èŒƒå›´: [{action.min():.3f}, {action.max():.3f}]")
                    
#                 except Exception as action_e:
#                     print(f"   âš ï¸ åŠ¨ä½œç”Ÿæˆå¤±è´¥: {action_e}")
#                     action = np.random.uniform(-0.1, 0.1, 7)
                
#                 obs, reward, done, _, info = env.step(action)
#                 result["steps_completed"] = step + 1
                
#                 print(f"   æ­¥éª¤ {step+1}: reward={reward:.3f}, done={done}")
                
#                 if done:
#                     if info.get("task_success", False):
#                         print(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
#                     else:
#                         print(f"â±ï¸ ä»»åŠ¡ç»“æŸ")
#                     break
            
#             result["step_success"] = True
#             env.close()
            
#             print(f"âœ… ç¯å¢ƒåŠŸèƒ½æµ‹è¯•å®Œæˆ")
            
#         except Exception as e:
#             result["error"] = str(e)
#             print(f"âŒ ç¯å¢ƒåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        
#         return result
    
#     def find_working_task(self) -> Optional[str]:
#         """æ‰¾åˆ°ä¸€ä¸ªå¯å·¥ä½œçš„ä»»åŠ¡"""
#         print(f"\nğŸ” å¯»æ‰¾å¯ç”¨çš„RoboCasaä»»åŠ¡...")
        
#         # æŒ‰éš¾æ˜“ç¨‹åº¦æµ‹è¯•ä»»åŠ¡
#         test_tasks = RoboCasaTaskSelector.get_test_tasks()
        
#         for task_name in test_tasks:
#             print(f"\nå°è¯•ä»»åŠ¡: {task_name}")
            
#             # é¦–å…ˆæµ‹è¯•ç›¸æœºé…ç½®
#             working_cameras = self.camera_tester.find_working_camera_config(task_name)
            
#             if not working_cameras:
#                 print(f"âŒ ä»»åŠ¡ {task_name} ç›¸æœºé…ç½®å¤±è´¥")
#                 continue
            
#             if self.test_environment_creation(task_name):
#                 result = self.test_environment_functionality(task_name, max_steps=3)
                
#                 if (result["creation_success"] and 
#                     result["reset_success"] and 
#                     result["step_success"]):
#                     print(f"âœ… æ‰¾åˆ°å¯ç”¨ä»»åŠ¡: {task_name}")
#                     print(f"   ğŸ“· ä½¿ç”¨ç›¸æœºé…ç½®: {working_cameras}")
#                     return task_name
#                 else:
#                     print(f"âŒ ä»»åŠ¡ {task_name} åŠŸèƒ½æµ‹è¯•å¤±è´¥")
#             else:
#                 print(f"âŒ ä»»åŠ¡ {task_name} åˆ›å»ºå¤±è´¥")
        
#         print(f"âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ä»»åŠ¡")
#         return None

# # ==================== ä¿®æ”¹FinalGR00TExperimentç±» ====================

# # [ä¿ç•™åŸæœ‰çš„å…¶ä»–ç±»å®šä¹‰: FinalConfig, FixedDataFormatter, FinalGR00TClient, FinalEpisodeResult]

# @dataclass
# class FinalConfig:
#     """æœ€ç»ˆå®éªŒé…ç½®"""
#     # æœåŠ¡è¿æ¥
#     host: str = "localhost"
#     port: int = 5555
    
#     # å®éªŒè®¾ç½®
#     experiment_name: str = "robocasa_groot_experiment"
#     num_episodes: int = 3
#     max_steps_per_episode: int = 60
    
#     # RoboCasaè®¾ç½®
#     robocasa_task: str = "Lift"  # ä½¿ç”¨å®é™…å­˜åœ¨çš„ç®€å•ä»»åŠ¡
#     use_real_robocasa: bool = True  # æ˜¯å¦ä½¿ç”¨çœŸå®RoboCasa
    
#     # å®éªŒæ¨¡å¼
#     run_baseline: bool = True
#     run_metacognitive: bool = True
    
#     # è®¾å¤‡è®¾ç½®
#     device: str = "cuda" if torch.cuda.is_available() else "cpu"

# class FixedDataFormatter:
#     """ä¿®å¤åçš„æ•°æ®æ ¼å¼å™¨ - ä½¿ç”¨æ­£ç¡®çš„è§†é¢‘åˆ†è¾¨ç‡"""
    
#     def __init__(self):
#         self.required_keys = {
#             "video.webcam": (640, 480, 3),
#             "state.single_arm": (1, 5),
#             "state.gripper": (1, 1),
#             "annotation.human.task_description": None
#         }
        
#         print("ğŸ¯ ä½¿ç”¨å¾®è°ƒæ¨¡å‹æœŸæœ›é…ç½®:")
#         for key, shape in self.required_keys.items():
#             if shape:
#                 print(f"   - {key}: {shape}")
#             else:
#                 print(f"   - {key}: [string list]")
    
#     def create_correct_observation(self, base_obs: Dict[str, np.ndarray] = None) -> Dict[str, Any]:
#         """åˆ›å»ºæ­£ç¡®æ ¼å¼çš„è§‚å¯Ÿæ•°æ®"""
#         correct_obs = {}
        
#         # 1. è§†é¢‘æ•°æ® - video.webcam
#         if base_obs and "frontview_image" in base_obs:
#             img = base_obs["frontview_image"]
#             if img.shape[:2] != (480, 640):
#                 import cv2
#                 img = cv2.resize(img, (640, 480))
#             correct_obs["video.webcam"] = img[np.newaxis, :, :, :].astype(np.uint8)
#         else:
#             correct_obs["video.webcam"] = self._generate_correct_image()
        
#         # 2. å•è‡‚çŠ¶æ€ - state.single_arm
#         if base_obs and "robot0_joint_pos" in base_obs:
#             joint_pos = base_obs["robot0_joint_pos"][:5]
#             joint_pos = np.clip(joint_pos, -1.0, 1.0)
#             correct_obs["state.single_arm"] = joint_pos[np.newaxis, :].astype(np.float32)
#         else:
#             joint_data = np.random.uniform(-0.3, 0.3, 5)
#             correct_obs["state.single_arm"] = joint_data[np.newaxis, :].astype(np.float32)
        
#         # 3. å¤¹çˆªçŠ¶æ€ - state.gripper
#         if base_obs and "robot0_gripper_qpos" in base_obs:
#             gripper_pos = base_obs["robot0_gripper_qpos"][:1]
#             correct_obs["state.gripper"] = gripper_pos[np.newaxis, :].astype(np.float32)
#         else:
#             gripper_data = np.random.uniform(-0.1, 0.1, 1)
#             correct_obs["state.gripper"] = gripper_data[np.newaxis, :].astype(np.float32)
        
#         # 4. ä»»åŠ¡æè¿°
#         task_desc = "Execute manipulation task"
#         if base_obs and hasattr(base_obs, 'get'):
#             task_name = base_obs.get('task_name', 'unknown')
#             task_desc = f"Execute {task_name}"
        
#         correct_obs["annotation.human.task_description"] = [task_desc]
        
#         return correct_obs
        
#     def _generate_correct_image(self) -> np.ndarray:
#         """ç”Ÿæˆæ­£ç¡®åˆ†è¾¨ç‡çš„æµ‹è¯•å›¾åƒ"""
#         img = np.zeros((1, 480, 640, 3), dtype=np.uint8)
        
#         for i in range(480):
#             for j in range(640):
#                 img[0, i, j, 0] = (i + j) % 256
#                 img[0, i, j, 1] = (i * 2) % 256
#                 img[0, i, j, 2] = (j * 2) % 256
        
#         return img
    
#     def print_observation_details(self, obs: Dict[str, Any]):
#         """æ‰“å°è§‚å¯Ÿæ•°æ®è¯¦æƒ…"""
#         print("ğŸ“Š å‘é€çš„è§‚å¯Ÿæ•°æ®:")
#         for key, value in obs.items():
#             if isinstance(value, np.ndarray):
#                 print(f"   {key}: shape={value.shape}, dtype={value.dtype}")
#             elif isinstance(value, list):
#                 print(f"   {key}: list[{len(value)}] = {value}")
#             else:
#                 print(f"   {key}: {type(value)} = {value}")

# class FinalGR00TClient:
#     """æœ€ç»ˆGR00Tå®¢æˆ·ç«¯"""
    
#     def __init__(self, config: FinalConfig):
#         self.config = config
#         self.client = None
#         self.formatter = FixedDataFormatter()
#         self.is_connected = False
        
#         # ç»Ÿè®¡ä¿¡æ¯
#         self.total_calls = 0
#         self.total_successes = 0
#         self.total_failures = 0
#         self.total_time = 0.0
    
#     def connect(self) -> bool:
#         """è¿æ¥åˆ°GR00TæœåŠ¡"""
#         if not GROOT_CLIENT_AVAILABLE:
#             print("âŒ GR00Tå®˜æ–¹å®¢æˆ·ç«¯ä¸å¯ç”¨")
#             return False
        
#         try:
#             print(f"ğŸ”— è¿æ¥åˆ°GR00TæœåŠ¡: {self.config.host}:{self.config.port}")
            
#             self.client = RobotInferenceClient(
#                 host=self.config.host, 
#                 port=self.config.port
#             )
            
#             print("ğŸ“‹ éªŒè¯è¿æ¥...")
#             modality_config = self.client.get_modality_config()
            
#             print("âœ… è¿æ¥æˆåŠŸï¼æœåŠ¡ç«¯é…ç½®:")
#             for key, config in modality_config.items():
#                 print(f"   - {key}: {config.modality_keys}")
            
#             # æµ‹è¯•è°ƒç”¨
#             print("\nğŸ§ª è¿›è¡Œè¿æ¥æµ‹è¯•...")
#             test_obs = self.formatter.create_correct_observation()
#             self.formatter.print_observation_details(test_obs)
            
#             print("ğŸš€ å‘é€æµ‹è¯•è¯·æ±‚...")
#             test_result = self.client.get_action(test_obs)
            
#             if test_result is not None:
#                 print("âœ… æµ‹è¯•è°ƒç”¨æˆåŠŸï¼")
#                 print(f"ğŸ“¤ è¿”å›åŠ¨ä½œé”®: {list(test_result.keys())}")
#                 for key, value in test_result.items():
#                     if isinstance(value, np.ndarray):
#                         print(f"   {key}: shape={value.shape}")
#                 self.is_connected = True
#                 return True
#             else:
#                 print("âŒ æµ‹è¯•è°ƒç”¨å¤±è´¥")
#                 return False
                
#         except Exception as e:
#             print(f"âŒ è¿æ¥å¤±è´¥: {e}")
#             import traceback
#             traceback.print_exc()
#             return False
    
#     def predict(self, observation: Dict[str, np.ndarray]) -> Optional[Dict[str, np.ndarray]]:
#         """è¿›è¡Œé¢„æµ‹"""
#         if not self.is_connected:
#             return None
        
#         self.total_calls += 1
#         start_time = time.time()
        
#         try:
#             correct_obs = self.formatter.create_correct_observation(observation)
#             action = self.client.get_action(correct_obs)
            
#             api_time = time.time() - start_time
#             self.total_time += api_time
            
#             if action is not None:
#                 self.total_successes += 1
#                 return action
#             else:
#                 self.total_failures += 1
#                 return None
                
#         except Exception as e:
#             api_time = time.time() - start_time
#             self.total_time += api_time
#             self.total_failures += 1
#             print(f"âš ï¸ é¢„æµ‹å¼‚å¸¸: {e}")
#             return None
    
#     def get_stats(self) -> Dict[str, Any]:
#         """è·å–ç»Ÿè®¡ä¿¡æ¯"""
#         if self.total_calls == 0:
#             return {"calls": 0, "successes": 0, "failures": 0, "success_rate": 0, "avg_time": 0}
        
#         return {
#             "calls": self.total_calls,
#             "successes": self.total_successes,
#             "failures": self.total_failures,
#             "success_rate": self.total_successes / self.total_calls,
#             "avg_time": self.total_time / self.total_calls
#         }

# @dataclass 
# class FinalEpisodeResult:
#     """æœ€ç»ˆEpisodeç»“æœ"""
#     episode_id: int
#     mode: str
#     task_success: bool
#     total_steps: int
#     total_time: float
#     api_calls: int
#     api_successes: int
#     avg_api_time: float
#     metacog_interventions: int = 0
#     groot_actions_received: int = 0

# class FinalGR00TExperiment:
#     """ä¿®æ”¹åçš„GR00Tå®éªŒ - é›†æˆçœŸå®RoboCasa"""
    
#     def __init__(self, config: FinalConfig):
#         self.config = config
#         self.groot_client = FinalGR00TClient(config)
#         self.results = []

#         # è®¾ç½®å…ƒè®¤çŸ¥æ¨¡å—
#         self.metacog_available = False
#         if METACOG_AVAILABLE:
#             try:
#                 self.metacog_module = CompleteMetaCognitiveModule(config.device)
#                 self.robocasa_adapter = RoboCasaToMetacogAdapter(image_size=(480, 640))
#                 self.metacog_to_vla_s2_adapter = MetacogToVLASystem2Adapter()
#                 self.metacog_available = True
#                 print("âœ… å…ƒè®¤çŸ¥æ¨¡å—å·²åŠ è½½")
#             except Exception as e:
#                 print(f"âš ï¸ å…ƒè®¤çŸ¥æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")

#         # åˆ›å»ºç¯å¢ƒï¼ˆè¿™é‡Œæ˜¯å…³é”®ä¿®æ”¹ï¼‰
#         self.environment = self._create_environment()
    


#     # åœ¨ FinalGR00TExperiment ç±»çš„å†…éƒ¨æ·»åŠ è¿™ä¸ªæ–¹æ³•
#     def _convert_groot_action_to_robocasa(self, groot_action: Dict[str, np.ndarray]) -> np.ndarray:
#         """
#         å°†GR00Tæ¨¡å‹çš„è¾“å‡ºåŠ¨ä½œè½¬æ¢ä¸ºRoboCasaç¯å¢ƒæœŸæœ›çš„åŠ¨ä½œæ ¼å¼ã€‚
#         è¿™æ˜¯ä¸€ä¸ªå…³é”®çš„é€‚é…å™¨ã€‚
#         """
#         # GR00Tè¾“å‡ºé€šå¸¸æ˜¯å­—å…¸ï¼ŒåŒ…å« 'world_vector', 'rotation_delta', 'gripper_closedness_action'
#         # å½¢çŠ¶é€šå¸¸æ˜¯ (1, 3), (1, 3), (1, 1)
        
#         # 1. æå–å¹¶å¤„ç†GR00TåŠ¨ä½œ
#         # æœ«ç«¯æ‰§è¡Œå™¨ä½ç§» (dx, dy, dz)
#         world_vector = groot_action.get('world_vector', np.zeros((1, 3)))[0]
#         # æœ«ç«¯æ‰§è¡Œå™¨æ—‹è½¬ (d_roll, d_pitch, d_yaw)
#         rotation_delta = groot_action.get('rotation_delta', np.zeros((1, 3)))[0]
#         # å¤¹çˆªåŠ¨ä½œ (-1 for open, 1 for close)
#         # GR00T çš„ gripper_closedness_action èŒƒå›´æ˜¯ [0, 1] for open, to [1, 1] for closed.
#         # robosuite çš„å¤¹çˆªåŠ¨ä½œé€šå¸¸æ˜¯ -1 (open) to 1 (close). æˆ‘ä»¬éœ€è¦æ˜ å°„ä¸€ä¸‹ã€‚
#         gripper_action_groot = groot_action.get('gripper_closedness_action', np.zeros((1,1)))[0][0]
#         gripper_action_robocasa = (gripper_action_groot - 0.5) * 2.0  # Map [0, 1] to [-1, 1]

#         # 2. æ„é€ RoboCasaçš„12ç»´åŠ¨ä½œå‘é‡
#         # RoboCasaçš„PandaMobileæ§åˆ¶å™¨é€šå¸¸æœŸæœ›ä¸€ä¸ª10ç»´æˆ–12ç»´çš„åŠ¨ä½œã€‚
#         # 12ç»´å¯èƒ½æ˜¯ï¼š[arm_dx, dy, dz, d_roll, dpitch, dyaw] (6) + [gripper] (1) + 
#         # [base_vx, vy, vtheta] (3) + [2 other dims?]
#         # æˆ‘ä»¬å…ˆå‡è®¾ä¸€ä¸ªå¸¸è§çš„10ç»´ç»“æ„ï¼Œå¹¶ç”¨0å¡«å……å‰©ä½™ç»´åº¦
#         # [arm_dx, arm_dy, arm_dz, arm_droll, arm_dpitch, arm_dyaw, gripper, base_x, base_y, base_rot]
        
#         # !! å…³é”®å‡è®¾ !!: GR00Tç›®å‰åªæ§åˆ¶æ‰‹è‡‚ï¼Œä¸æ§åˆ¶åº•ç›˜ã€‚
#         # æ‰€ä»¥åº•ç›˜çš„åŠ¨ä½œæˆ‘ä»¬è®¾ç½®ä¸º0ã€‚
#         base_action = np.zeros(3) # (base_vx, base_vy, base_vtheta)

#         # ç»„åˆæˆä¸€ä¸ª10ç»´åŠ¨ä½œå‘é‡
#         # è‡‚6ç»´ + å¤¹çˆª1ç»´
#         arm_and_gripper_action = np.concatenate([
#             world_vector, 
#             rotation_delta, 
#             [gripper_action_robocasa]
#         ]) # 7ç»´
        
#         # ç»„åˆæ‰‹è‡‚å’Œåº•ç›˜åŠ¨ä½œ
#         # å‡è®¾åŠ¨ä½œæ ¼å¼æ˜¯ï¼š7ç»´è‡‚åŠ¨ä½œ + 3ç»´åº•ç›˜åŠ¨ä½œ + 2ç»´æœªçŸ¥åŠ¨ä½œ(ç”¨0å¡«å……)
#         robocasa_action = np.zeros(12)
#         # å¡«å……æ‰‹è‡‚å’Œå¤¹çˆªéƒ¨åˆ†
#         robocasa_action[0:7] = arm_and_gripper_action
#         # å¡«å……åº•ç›˜éƒ¨åˆ† (è¿™é‡Œæˆ‘ä»¬å‡è®¾å3ç»´æ˜¯åº•ç›˜ï¼Œä½†éœ€è¦éªŒè¯)
#         # robocasa_action[7:10] = base_action 
#         # è®©æˆ‘ä»¬å‡è®¾ robosuite çš„é»˜è®¤ PandaMobile æ§åˆ¶å™¨æ˜¯ OSC_POSE
#         # å®ƒçš„åŠ¨ä½œç©ºé—´æ˜¯ [d_x, d_y, d_z, d_roll, d_pitch, d_yaw, gripper, base_vx, base_vy, base_vtheta] -> 10ç»´
#         # æ—¢ç„¶ç¯å¢ƒæŠ¥12ç»´ï¼Œæˆ‘ä»¬å…ˆå¡«å……å‰7ç»´ï¼Œåé¢ç”¨0ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚
        
#         print(f"ğŸ¤– GR00T->RoboCasa: GR00T action (world_vec, rot_delta, grip): "
#             f"{np.round(world_vector, 2)}, {np.round(rotation_delta, 2)}, {gripper_action_robocasa:.2f}")
        
#         # å°†åŠ¨ä½œè£å‰ªåˆ°[-1, 1]èŒƒå›´
#         return np.clip(robocasa_action, -1.0, 1.0)



#     def _create_environment(self):
#         """åˆ›å»ºç¯å¢ƒ - çœŸå®RoboCasaæˆ–å›é€€åˆ°æ¨¡æ‹Ÿç¯å¢ƒ"""
#         if self.config.use_real_robocasa and ROBOCASA_AVAILABLE:
#             print(f"ğŸ—ï¸ åˆ›å»ºçœŸå®RoboCasaç¯å¢ƒ")
            
#             # é¦–å…ˆæµ‹è¯•ä»»åŠ¡å¯ç”¨æ€§
#             tester = RoboCasaEnvironmentTester()
            
#             # å°è¯•ä½¿ç”¨é…ç½®ä¸­çš„ä»»åŠ¡
#             test_result = tester.test_environment_functionality(
#                 self.config.robocasa_task, max_steps=3
#             )
            
#             if test_result["creation_success"] and test_result["reset_success"]:
#                 print(f"âœ… ä½¿ç”¨ä»»åŠ¡: {self.config.robocasa_task}")
#                 return RealRoboCasaEnvironment(
#                     task_name=self.config.robocasa_task,
#                     robot_type="PandaMobile",
#                     horizon=self.config.max_steps_per_episode * 2
#                 )
#             else:
#                 # å¦‚æœé…ç½®çš„ä»»åŠ¡å¤±è´¥ï¼Œå°è¯•æ‰¾åˆ°å¯ç”¨ä»»åŠ¡
#                 print(f"âš ï¸ é…ç½®ä»»åŠ¡å¤±è´¥ï¼Œå¯»æ‰¾å¯ç”¨ä»»åŠ¡...")
#                 working_task = tester.find_working_task()
                
#                 if working_task:
#                     print(f"âœ… æ‰¾åˆ°å¯ç”¨ä»»åŠ¡: {working_task}")
#                     self.config.robocasa_task = working_task  # æ›´æ–°é…ç½®
#                     return RealRoboCasaEnvironment(
#                         task_name=working_task,
#                         robot_type="PandaMobile", 
#                         horizon=self.config.max_steps_per_episode * 2
#                     )
#                 else:
#                     print(f"âŒ æ— æ³•æ‰¾åˆ°å¯ç”¨çš„RoboCasaä»»åŠ¡ï¼Œå›é€€åˆ°æ¨¡æ‹Ÿç¯å¢ƒ")
#                     return self._create_fallback_environment()
#         else:
#             print(f"ğŸ¤– ä½¿ç”¨æ¨¡æ‹Ÿç¯å¢ƒï¼ˆRoboCasaä¸å¯ç”¨æˆ–å·²ç¦ç”¨ï¼‰")
#             return self._create_fallback_environment()
    
#     def _create_fallback_environment(self):
#         """åˆ›å»ºå›é€€çš„æ¨¡æ‹Ÿç¯å¢ƒ"""
#         class SingleArmTestEnvironment:
#             def __init__(self):
#                 self.step_count = 0
#                 self.max_steps = 60
#                 print("ğŸ¤– åˆå§‹åŒ–æ¨¡æ‹Ÿå•è‡‚ç¯å¢ƒï¼ˆå›é€€æ¨¡å¼ï¼‰")
            
#             def reset(self):
#                 self.step_count = 0
#                 return self._generate_obs(), {"task_name": "simulated_task"}
            
#             def step(self, action):
#                 self.step_count += 1
#                 obs = self._generate_obs()
                
#                 if self.step_count > 25 and np.random.random() < 0.35:
#                     done = True
#                     reward = 1.0
#                 elif self.step_count >= self.max_steps:
#                     done = True
#                     reward = -0.5
#                 else:
#                     done = False
#                     reward = np.random.uniform(-0.01, 0.01)
                
#                 info = {
#                     "task_success": done and reward > 0,
#                     "collision": np.random.random() < 0.003,
#                     "force_violation": np.random.random() < 0.001,
#                     "task_name": "simulated_task"
#                 }
                
#                 return obs, reward, done, False, info
            
#             def _generate_obs(self):
#                 return {
#                     "frontview_image": np.random.randint(50, 200, (480, 640, 3), dtype=np.uint8),
#                     "robot0_joint_pos": np.random.uniform(-0.2, 0.2, 7),
#                     "robot0_joint_vel": np.random.uniform(-0.1, 0.1, 7),
#                     "robot0_gripper_qpos": np.random.uniform(-0.05, 0.05, 2),
#                     "robot0_eef_pos": np.array([0.5, 0.0, 0.8]),
#                     "robot0_eef_quat": np.array([0, 0, 0, 1])
#                 }
            
#             def close(self):
#                 pass
        
#         return SingleArmTestEnvironment()
    
#     def run_experiment(self) -> bool:
#         """è¿è¡Œå®éªŒ"""
#         env_type = "çœŸå®RoboCasa" if isinstance(self.environment, RealRoboCasaEnvironment) else "æ¨¡æ‹Ÿ"
        
#         print(f"\nğŸ¯ å¼€å§‹GR00T + å…ƒè®¤çŸ¥å®éªŒ")
#         print(f"ç¯å¢ƒç±»å‹: {env_type}")
#         if isinstance(self.environment, RealRoboCasaEnvironment):
#             print(f"RoboCasaä»»åŠ¡: {self.config.robocasa_task}")
#         print("=" * 70)
        
#         # è¿æ¥åˆ°GR00TæœåŠ¡
#         if not self.groot_client.connect():
#             print("âŒ æ— æ³•è¿æ¥åˆ°GR00Tæ¨ç†æœåŠ¡")
#             return False
        
#         try:
#             # è¿è¡ŒåŸºçº¿å®éªŒ
#             if self.config.run_baseline:
#                 print(f"\nğŸ¤– åŸºçº¿å®éªŒ (GR00T N1)")
#                 print("-" * 50)
                
#                 for episode in range(self.config.num_episodes):
#                     print(f"\nğŸ“Š åŸºçº¿ Episode {episode + 1}/{self.config.num_episodes}")
#                     result = self._run_episode(episode, "baseline", False)
#                     self.results.append(result)
#                     self._print_episode_summary(result)
            
#             # è¿è¡Œå…ƒè®¤çŸ¥å®éªŒ
#             if self.config.run_metacognitive and self.metacog_available:
#                 print(f"\nğŸ§  å…ƒè®¤çŸ¥å®éªŒ (GR00T N1 + å…ƒè®¤çŸ¥æ¨¡å—)")
#                 print("-" * 50)
                
#                 for episode in range(self.config.num_episodes):
#                     print(f"\nğŸ“Š å…ƒè®¤çŸ¥ Episode {episode + 1}/{self.config.num_episodes}")
#                     result = self._run_episode(episode, "metacognitive", True)
#                     self.results.append(result)
#                     self._print_episode_summary(result)
            
#             # åˆ†æç»“æœ
#             self._analyze_results()
#             self._save_results()
            
#             return True
            
#         finally:
#             # å…³é—­ç¯å¢ƒ
#             if hasattr(self.environment, 'close'):
#                 self.environment.close()
    
#     def _run_episode(self, episode_id: int, mode: str, use_metacognitive: bool) -> FinalEpisodeResult:
#         """è¿è¡Œepisode"""
#         start_time = time.time()
        
#         result = FinalEpisodeResult(
#             episode_id=episode_id,
#             mode=mode,
#             task_success=False,
#             total_steps=0,
#             total_time=0.0,
#             api_calls=0,
#             api_successes=0,
#             avg_api_time=0.0
#         )
        
#         try:
#             obs, info = self.environment.reset()
#             done = False
#             step_count = 0
#             api_times = []
#             current_metacognitive_instruction_for_s2 = None
            
#             print(f"     æ‰§è¡Œä¸­ ({mode}): ", end="", flush=True)
            
#             while not done and step_count < self.config.max_steps_per_episode:
                
#                 # å‡†å¤‡è§‚æµ‹æ•°æ®
#                 observation_for_groot = obs.copy()
#                 if current_metacognitive_instruction_for_s2:
#                     observation_for_groot["metacognitive_instruction"] = [current_metacognitive_instruction_for_s2]
                
#                 # è·å–GR00TåŠ¨ä½œ
#                 api_start = time.time()
#                 groot_action_dict = self.groot_client.predict(observation_for_groot)
#                 api_time = time.time() - api_start
#                 api_times.append(api_time)
                
#                 result.api_calls += 1
#                 current_metacognitive_instruction_for_s2 = None
                
                
#                 # ... in FinalGR00TExperiment._run_episode
#                 env_action_to_execute = None
#                 s1_action_info_for_metacog = None

#                 if groot_action_dict is not None:
#                     result.api_successes += 1
#                     result.groot_actions_received += 1
#                     print(".", end="", flush=True)
                    
#                     # æ ¸å¿ƒä¿®æ”¹ï¼šå°†GR00TåŠ¨ä½œè½¬æ¢ä¸ºRoboCasaåŠ¨ä½œ
#                     if isinstance(self.environment, RealRoboCasaEnvironment):
#                         env_action_to_execute = self._convert_groot_action_to_robocasa(groot_action_dict)
#                         s1_action_info_for_metacog = env_action_to_execute # å…ƒè®¤çŸ¥æ¨¡å—ä½¿ç”¨è½¬æ¢åçš„åŠ¨ä½œ
#                     else: # æ¨¡æ‹Ÿç¯å¢ƒ
#                         # åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥æ¨¡ä»¿è¿™ä¸ªè¿‡ç¨‹
#                         env_action_to_execute = self._convert_groot_action_to_robocasa(groot_action_dict)[:7]
#                         s1_action_info_for_metacog = env_action_to_execute

#                 else:
#                     print("x", end="", flush=True)
#                     # GR00Tè°ƒç”¨å¤±è´¥ï¼Œç”Ÿæˆä¸€ä¸ªé›¶åŠ¨ä½œï¼ˆä¿æŒä¸åŠ¨ï¼‰
#                     if isinstance(self.environment, RealRoboCasaEnvironment):
#                         try:
#                             action_space = self.environment.get_action_space()
#                             if isinstance(action_space, tuple):
#                                 action_shape = action_space[0].shape
#                             else: # gym.Space
#                                 action_shape = action_space.shape
#                             env_action_to_execute = np.zeros(action_shape)
#                         except Exception:
#                             env_action_to_execute = np.zeros(12) # Fallback to 12
#                     else:
#                         env_action_to_execute = np.zeros(7)
#                     s1_action_info_for_metacog = env_action_to_execute

#                 # ç¡®ä¿åŠ¨ä½œéç©º
#                 if env_action_to_execute is None:
#                     print("âš ï¸ åŠ¨ä½œæœªèƒ½ç”Ÿæˆï¼Œä½¿ç”¨é›¶åŠ¨ä½œ")
#                     env_action_to_execute = np.zeros(12) if isinstance(self.environment, RealRoboCasaEnvironment) else np.zeros(7)



                
#                 # ç¯å¢ƒæ­¥è¿›
#                 next_obs, reward, done, _, info = self.environment.step(env_action_to_execute)
                
#                 # å…ƒè®¤çŸ¥å¤„ç†
#                 if use_metacognitive and self.metacog_available:
#                     try:
#                         sensor_data = self.robocasa_adapter.convert_observation(
#                             next_obs,
#                             s1_action_info_for_metacog,
#                             execution_status="normal"
#                         )
#                         metacog_output = self.metacog_module.process_sensor_data(sensor_data)
                        
#                         instruction_for_s2 = self.metacog_to_vla_s2_adapter.convert_to_system2_instruction(metacog_output)
                        
#                         if instruction_for_s2:
#                             current_metacognitive_instruction_for_s2 = instruction_for_s2
#                             if metacog_output.directive != DirectiveType.CONTINUE:
#                                 result.metacog_interventions += 1
#                                 print(f"M[{instruction_for_s2[:10]}...]", end="", flush=True)
                        
#                     except Exception as e:
#                         pass
                
#                 obs = next_obs
#                 step_count += 1
                
#                 if info.get("task_success", False):
#                     result.task_success = True
#                     done = True
#                     print("!", end="", flush=True)
                
#                 if step_count % 10 == 0 and result.api_calls > 0:
#                     success_rate = result.api_successes / result.api_calls
#                     print(f"|{success_rate:.0%}", end="", flush=True)
            
#             result.total_steps = step_count
#             result.total_time = time.time() - start_time
#             result.avg_api_time = np.mean(api_times) if api_times else 0.0
            
#             print()
            
#         except Exception as e:
#             result.total_time = time.time() - start_time
#             print(f" å¼‚å¸¸: {e}")
#             import traceback
#             traceback.print_exc()
        
#         return result
    
#     def _print_episode_summary(self, result: FinalEpisodeResult):
#         """æ‰“å°episodeæ‘˜è¦"""
#         status = "âœ… æˆåŠŸ" if result.task_success else "âŒ å¤±è´¥"
#         api_success_rate = result.api_successes / result.api_calls if result.api_calls > 0 else 0
        
#         print(f"   ç»“æœ: {status}")
#         print(f"   æ‰§è¡Œ: {result.total_steps} æ­¥, {result.total_time:.1f}s")
#         print(f"   API: {result.api_successes}/{result.api_calls} æˆåŠŸ ({api_success_rate:.1%})")
#         print(f"   GR00TåŠ¨ä½œ: {result.groot_actions_received} ä¸ª")
        
#         if result.metacog_interventions > 0:
#             print(f"   å…ƒè®¤çŸ¥: {result.metacog_interventions} æ¬¡å¹²é¢„")
    
#     def _analyze_results(self):
#         """åˆ†æç»“æœ"""
#         print(f"\nğŸ“Š å®éªŒç»“æœåˆ†æ")
#         print("=" * 70)
        
#         env_type = "çœŸå®RoboCasa" if isinstance(self.environment, RealRoboCasaEnvironment) else "æ¨¡æ‹Ÿ"
#         print(f"ç¯å¢ƒç±»å‹: {env_type}")
        
#         baseline_results = [r for r in self.results if r.mode == "baseline"]
#         metacog_results = [r for r in self.results if r.mode == "metacognitive"]
        
#         def analyze_mode(results: List[FinalEpisodeResult], mode_name: str):
#             if not results:
#                 return
            
#             successes = sum(1 for r in results if r.task_success)
#             success_rate = successes / len(results)
#             total_api_calls = sum(r.api_calls for r in results)
#             total_api_successes = sum(r.api_successes for r in results)
#             api_success_rate = total_api_successes / total_api_calls if total_api_calls > 0 else 0
            
#             print(f"\nğŸ” {mode_name} æ¨¡å¼:")
#             print(f"   ä»»åŠ¡æˆåŠŸç‡: {success_rate:.1%} ({successes}/{len(results)})")
#             print(f"   APIæˆåŠŸç‡: {api_success_rate:.1%} ({total_api_successes}/{total_api_calls})")
            
#             if mode_name == "å…ƒè®¤çŸ¥":
#                 total_interventions = sum(r.metacog_interventions for r in results)
#                 print(f"   å…ƒè®¤çŸ¥å¹²é¢„: {total_interventions} æ¬¡")
        
#         analyze_mode(baseline_results, "åŸºçº¿")
#         analyze_mode(metacog_results, "å…ƒè®¤çŸ¥")
        
#         # å¯¹æ¯”åˆ†æ
#         if baseline_results and metacog_results:
#             baseline_success = sum(1 for r in baseline_results if r.task_success) / len(baseline_results)
#             metacog_success = sum(1 for r in metacog_results if r.task_success) / len(metacog_results)
#             improvement = metacog_success - baseline_success
            
#             print(f"\nâš–ï¸ å¯¹æ¯”åˆ†æ:")
#             print(f"   æˆåŠŸç‡å˜åŒ–: {improvement:+.1%}")
            
#             if improvement > 0:
#                 print(f"   âœ… å…ƒè®¤çŸ¥æ¨¡å—æå‡äº†æ€§èƒ½")
#             elif improvement == 0:
#                 print(f"   â¡ï¸ å…ƒè®¤çŸ¥æ¨¡å—ä¿æŒäº†æ€§èƒ½")
#             else:
#                 print(f"   âš ï¸ å…ƒè®¤çŸ¥æ¨¡å—å½±å“äº†æ€§èƒ½")
    
#     def _save_results(self):
#         """ä¿å­˜ç»“æœ"""
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         env_type = "robocasa" if isinstance(self.environment, RealRoboCasaEnvironment) else "simulated"
#         filename = f"groot_experiment_{env_type}_{timestamp}.json"
        
#         data = {
#             "timestamp": timestamp,
#             "experiment_type": f"GR00T_Metacognitive_{env_type}",
#             "environment_type": env_type,
#             "robocasa_task": self.config.robocasa_task if isinstance(self.environment, RealRoboCasaEnvironment) else None,
#             "config": asdict(self.config),
#             "results": [asdict(r) for r in self.results]
#         }
        
#         with open(filename, 'w', encoding='utf-8') as f:
#             json.dump(data, f, indent=2, default=str)
        
#         print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {filename}")

# # ==================== ä¸»å‡½æ•° ====================

# def main():
#     """ä¸»å‡½æ•° - é›†æˆçœŸå®RoboCasaç¯å¢ƒ"""
#     print("ğŸ¯ GR00T + çœŸå®RoboCasaç¯å¢ƒé›†æˆå®éªŒ")
#     print("é˜¶æ®µ1.1: é›†æˆçœŸå®RoboCasaç¯å¢ƒ (å·²ä¿®å¤ç›¸æœºé…ç½®é—®é¢˜)")
#     print("=" * 70)
    
#     # æ£€æŸ¥ä¾èµ–
#     print(f"ğŸ“‹ ä¾èµ–æ£€æŸ¥:")
#     print(f"   RoboCasa: {'âœ…' if ROBOCASA_AVAILABLE else 'âŒ'}")
#     print(f"   GR00T Client: {'âœ…' if GROOT_CLIENT_AVAILABLE else 'âŒ'}")  
#     print(f"   å…ƒè®¤çŸ¥æ¨¡å—: {'âœ…' if METACOG_AVAILABLE else 'âŒ'}")
    
#     if not ROBOCASA_AVAILABLE:
#         print("\nâš ï¸ RoboCasaä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿç¯å¢ƒ")
#         use_robocasa = False
#     else:
#         print("\nğŸ‰ RoboCasaå¯ç”¨ï¼Œå°†å°è¯•ä½¿ç”¨çœŸå®ç¯å¢ƒ")
#         # æ˜¾ç¤ºå¯ç”¨ä»»åŠ¡
#         RoboCasaTaskSelector.print_available_tasks()
#         print(f"\nğŸ’¡ æ¨èæµ‹è¯•ä»»åŠ¡: {RoboCasaTaskSelector.get_recommended_task()}")
#         use_robocasa = True
    
#     # é…ç½®å®éªŒ
#     config = FinalConfig(
#         host="localhost",
#         port=5555,
#         experiment_name="robocasa_integration_test",
#         num_episodes=2,  # å°‘é‡episodeç”¨äºæµ‹è¯•
#         max_steps_per_episode=30,
#         robocasa_task=RoboCasaTaskSelector.get_recommended_task(),  # ä½¿ç”¨æ¨èä»»åŠ¡
#         use_real_robocasa=use_robocasa,
#         run_baseline=True,
#         run_metacognitive=True if METACOG_AVAILABLE else False
#     )
    
#     print(f"\nğŸ› ï¸ å®éªŒé…ç½®:")
#     print(f"   ç¯å¢ƒ: {'çœŸå®RoboCasa' if config.use_real_robocasa else 'æ¨¡æ‹Ÿç¯å¢ƒ'}")
#     if config.use_real_robocasa:
#         print(f"   ä»»åŠ¡: {config.robocasa_task}")
#     print(f"   Episodes: {config.num_episodes}")
#     print(f"   æœ€å¤§æ­¥æ•°: {config.max_steps_per_episode}")
    
#     # è¿è¡Œç¯å¢ƒæµ‹è¯•ï¼ˆå¦‚æœRoboCasaå¯ç”¨ï¼‰
#     if ROBOCASA_AVAILABLE:
#         print(f"\nğŸ§ª RoboCasaç¯å¢ƒæµ‹è¯•")
#         print("-" * 50)
        
#         tester = RoboCasaEnvironmentTester()
#         working_task = tester.find_working_task()
        
#         if working_task:
#             config.robocasa_task = working_task
#             print(f"âœ… ç¡®è®¤ä½¿ç”¨ä»»åŠ¡: {working_task}")
#         else:
#             print(f"âŒ RoboCasaç¯å¢ƒæµ‹è¯•å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿç¯å¢ƒ")
#             config.use_real_robocasa = False
    
#     # è¿è¡Œå®éªŒ
#     print(f"\nğŸš€ å¼€å§‹å®éªŒ")
#     print("-" * 50)
    
#     experiment = FinalGR00TExperiment(config)
    
#     try:
#         success = experiment.run_experiment()
#         if success:
#             env_type = "çœŸå®RoboCasa" if isinstance(experiment.environment, RealRoboCasaEnvironment) else "æ¨¡æ‹Ÿ"
#             print(f"\nğŸ‰ é˜¶æ®µ1.1ä»»åŠ¡å®Œæˆï¼")
#             print(f"âœ… æˆåŠŸé›†æˆ{env_type}ç¯å¢ƒ")
#             print(f"âœ… ä¿®å¤äº†ç›¸æœºé…ç½®é—®é¢˜")
#             print(f"âœ… ç¯å¢ƒreset/stepåŠŸèƒ½æ­£å¸¸")
#             print(f"âœ… GR00Tå®¢æˆ·ç«¯è°ƒç”¨æ­£å¸¸")
#             if METACOG_AVAILABLE:
#                 print(f"âœ… å…ƒè®¤çŸ¥æ¨¡å—é›†æˆæ­£å¸¸")
#             print(f"\nğŸ“ˆ ä¸‹ä¸€æ­¥å¯ä»¥ç»§ç»­é˜¶æ®µ1.2å’Œ1.3ä»»åŠ¡")
#             print(f"ğŸ’¡ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ç›¸æœºåç§°ï¼ˆrobot0_frontviewç­‰ï¼‰")
#         else:
#             print(f"\nâŒ å®éªŒå¤±è´¥")
    
#     except KeyboardInterrupt:
#         print(f"\nâš ï¸ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
#     except Exception as e:
#         print(f"\nâŒ å®éªŒå¼‚å¸¸: {e}")
#         import traceback
#         traceback.print_exc()
    
#     finally:
#         # æ¸…ç†èµ„æº
#         if hasattr(experiment, 'environment') and hasattr(experiment.environment, 'close'):
#             experiment.environment.close()

# if __name__ == "__main__":
#     main()


#!/usr/bin/env python3
"""
é›†æˆå…ƒè®¤çŸ¥æ¨¡å—çš„ç¨³å®šè‰è“ç¯å¢ƒ - åŸºäºå¯å·¥ä½œçš„StableStrawberryEnvironment + è§†é¢‘å½•åˆ¶åŠŸèƒ½
Enhanced Stable Strawberry Environment with Metacognitive Integration + Video Recording
"""

import os
import sys
import time
import json
import numpy as np
import torch
import cv2
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import threading
import queue

# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…æ¸²æŸ“é—®é¢˜
os.environ.setdefault('MUJOCO_GL', 'egl')
os.environ.setdefault('PYOPENGL_PLATFORM', 'egl')

# å¯¼å…¥æ¨¡å—
try:
    import robosuite
    from robosuite.controllers import load_composite_controller_config
    ROBOSUITE_AVAILABLE = True
    print("âœ… RoboSuiteå¯ç”¨")
except ImportError as e:
    print(f"âŒ RoboSuiteä¸å¯ç”¨: {e}")
    ROBOSUITE_AVAILABLE = False

try:
    import robocasa
    ROBOCASA_AVAILABLE = True
    print("âœ… RoboCasaå¯ç”¨")
except ImportError as e:
    print(f"âŒ RoboCasaä¸å¯ç”¨: {e}")
    ROBOCASA_AVAILABLE = False

try:
    from gr00t.eval.robot import RobotInferenceClient
    GROOT_CLIENT_AVAILABLE = True
    print("âœ… GR00Tå®˜æ–¹å®¢æˆ·ç«¯å¯ç”¨")
except ImportError as e:
    print(f"âŒ GR00Tå®˜æ–¹å®¢æˆ·ç«¯ä¸å¯ç”¨: {e}")
    GROOT_CLIENT_AVAILABLE = False

# å¯¼å…¥ä¿®å¤çš„å…ƒè®¤çŸ¥æ¨¡å—
try:
    from metacog_integration import (
        CompleteMetaCognitiveModule,
        RoboCasaToMetacogAdapter,
        MetacogToGR00TAdapter,
        ActionAdjuster,
        SensorData,
        MetaCognitiveOutput,
        DirectiveType,
        MetacogToVLASystem2Adapter
    )
    METACOG_AVAILABLE = True
    print("âœ… ä¿®å¤çš„å…ƒè®¤çŸ¥æ¨¡å—å¯ç”¨")
except ImportError as e:
    print(f"âŒ å…ƒè®¤çŸ¥æ¨¡å—ä¸å¯ç”¨: {e}")
    print("è¯·ç¡®ä¿fixed_metacog_integration.pyåœ¨åŒç›®å½•ä¸‹")
    
    # å°è¯•åŸç‰ˆæ¨¡å—
    try:
        from metacog_integration import (
            CompleteMetaCognitiveModule,
            RoboCasaToMetacogAdapter,
            MetacogToGR00TAdapter,
            ActionAdjuster,
            SensorData,
            MetaCognitiveOutput,
            DirectiveType,
            MetacogToVLASystem2Adapter
        )
        METACOG_AVAILABLE = True
        print("âœ… åŸç‰ˆå…ƒè®¤çŸ¥æ¨¡å—å¯ç”¨")
    except ImportError:
        METACOG_AVAILABLE = False
        
        # åˆ›å»ºå¤‡ç”¨æ•°æ®ç»“æ„
        @dataclass
        class SensorData:
            rgb_image: np.ndarray
            depth_image: np.ndarray
            force_torque: np.ndarray
            contact_detected: bool
            joint_positions: np.ndarray
            joint_velocities: np.ndarray
            end_effector_pose: np.ndarray
            system1_commands: np.ndarray
            execution_status: str
            timestamp: float

# ==================== è§†é¢‘å½•åˆ¶å™¨ ====================

class VideoRecorder:
    """è§†é¢‘å½•åˆ¶å™¨ - ä¸“é—¨ç”¨äºä¿å­˜è®­ç»ƒè¿‡ç¨‹"""
    
    def __init__(self, 
                 output_dir: str = "./experiment_videos",
                 fps: int = 20,
                 video_size: Tuple[int, int] = (640, 480),
                 codec: str = 'mp4v'):
        """
        åˆå§‹åŒ–è§†é¢‘å½•åˆ¶å™¨
        
        Args:
            output_dir: è§†é¢‘ä¿å­˜ç›®å½•
            fps: å¸§ç‡
            video_size: è§†é¢‘å°ºå¯¸ (å®½, é«˜)
            codec: è§†é¢‘ç¼–ç æ ¼å¼
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.fps = fps
        self.video_size = video_size
        self.codec = codec
        
        # å½•åˆ¶çŠ¶æ€
        self.is_recording = False
        self.video_writer = None
        self.current_episode = 0
        self.frame_count = 0
        
        # çº¿ç¨‹å®‰å…¨çš„å¸§é˜Ÿåˆ—
        self.frame_queue = queue.Queue(maxsize=100)
        self.recording_thread = None
        self.stop_recording_flag = threading.Event()
        
        print(f"ğŸ¥ è§†é¢‘å½•åˆ¶å™¨åˆå§‹åŒ–")
        print(f"   ä¿å­˜ç›®å½•: {self.output_dir}")
        print(f"   è§†é¢‘å‚æ•°: {video_size[0]}x{video_size[1]} @ {fps}fps")
        print(f"   ç¼–ç æ ¼å¼: {codec}")
    
    def start_episode_recording(self, episode_id: int, experiment_name: str = "strawberry_experiment"):
        """å¼€å§‹å½•åˆ¶æ–°çš„episode"""
        if self.is_recording:
            self.stop_episode_recording()
        
        self.current_episode = episode_id
        self.frame_count = 0
        
        # ç”Ÿæˆè§†é¢‘æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{experiment_name}_episode_{episode_id:03d}_{timestamp}.mp4"
        self.video_path = self.output_dir / filename
        
        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*self.codec)
        self.video_writer = cv2.VideoWriter(
            str(self.video_path),
            fourcc,
            self.fps,
            self.video_size
        )
        
        if not self.video_writer.isOpened():
            print(f"âŒ æ— æ³•åˆ›å»ºè§†é¢‘æ–‡ä»¶: {self.video_path}")
            return False
        
        # å¯åŠ¨å½•åˆ¶çº¿ç¨‹
        self.is_recording = True
        self.stop_recording_flag.clear()
        self.recording_thread = threading.Thread(target=self._recording_worker)
        self.recording_thread.start()
        
        print(f"ğŸ¬ å¼€å§‹å½•åˆ¶ Episode {episode_id}: {filename}")
        return True
    
    def add_frame(self, image: np.ndarray, step_info: Dict[str, Any] = None):
        """æ·»åŠ ä¸€å¸§åˆ°å½•åˆ¶é˜Ÿåˆ—"""
        if not self.is_recording:
            return
        
        try:
            # å¤„ç†å›¾åƒæ ¼å¼
            processed_image = self._process_image(image, step_info)
            
            # æ·»åŠ åˆ°é˜Ÿåˆ—ï¼ˆéé˜»å¡ï¼‰
            if not self.frame_queue.full():
                self.frame_queue.put(processed_image, block=False)
                self.frame_count += 1
            else:
                print("âš ï¸ å¸§é˜Ÿåˆ—å·²æ»¡ï¼Œè·³è¿‡å¸§")
                
        except Exception as e:
            print(f"âš ï¸ æ·»åŠ å¸§å¤±è´¥: {e}")
    
    def _process_image(self, image: np.ndarray, step_info: Dict[str, Any] = None) -> np.ndarray:
        """å¤„ç†å›¾åƒæ ¼å¼å¹¶æ·»åŠ ä¿¡æ¯å åŠ """
        try:
            # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®
            if image is None:
                image = np.zeros((*self.video_size[::-1], 3), dtype=np.uint8)
            
            # è½¬æ¢æ•°æ®ç±»å‹
            if image.dtype != np.uint8:
                if image.max() <= 1.0:
                    image = (image * 255).astype(np.uint8)
                else:
                    image = image.astype(np.uint8)
            
            # è°ƒæ•´å°ºå¯¸
            if image.shape[:2] != self.video_size[::-1]:
                image = cv2.resize(image, self.video_size)
            
            # ç¡®ä¿æ˜¯3é€šé“
            if len(image.shape) == 2:
                image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
            elif image.shape[2] == 4:
                image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)
            
            # æ·»åŠ ä¿¡æ¯å åŠ 
            if step_info:
                image = self._add_info_overlay(image, step_info)
            
            return image
            
        except Exception as e:
            print(f"âš ï¸ å›¾åƒå¤„ç†å¤±è´¥: {e}")
            return np.zeros((*self.video_size[::-1], 3), dtype=np.uint8)
    
    def _add_info_overlay(self, image: np.ndarray, step_info: Dict[str, Any]) -> np.ndarray:
        """åœ¨å›¾åƒä¸Šæ·»åŠ ä¿¡æ¯å åŠ """
        try:
            overlay_image = image.copy()
            
            # è®¾ç½®å­—ä½“å‚æ•°
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.6
            color = (0, 255, 0)  # ç»¿è‰²
            thickness = 2
            
            # æ·»åŠ åŸºæœ¬ä¿¡æ¯
            y_offset = 30
            
            # Episodeå’ŒStepä¿¡æ¯
            if 'step' in step_info:
                text = f"Episode: {self.current_episode} | Step: {step_info['step']}"
                cv2.putText(overlay_image, text, (10, y_offset), font, font_scale, color, thickness)
                y_offset += 25
            
            # ä»»åŠ¡è¿›åº¦
            if 'strawberry_task_progress' in step_info:
                progress = step_info['strawberry_task_progress']
                picked = progress.get('strawberries_picked', 0)
                placed = progress.get('strawberries_on_plate', 0)
                text = f"Strawberries: Picked {picked}/3 | Placed {placed}/3"
                cv2.putText(overlay_image, text, (10, y_offset), font, font_scale, color, thickness)
                y_offset += 25
            
            # å¥–åŠ±ä¿¡æ¯
            if 'total_reward' in step_info:
                text = f"Reward: {step_info['total_reward']:.2f}"
                cv2.putText(overlay_image, text, (10, y_offset), font, font_scale, color, thickness)
                y_offset += 25
            
            # å…ƒè®¤çŸ¥åé¦ˆ
            if 'metacognitive_feedback' in step_info and step_info['metacognitive_feedback']:
                text = f"Metacog: {step_info['metacognitive_feedback'][:40]}..."
                cv2.putText(overlay_image, text, (10, y_offset), font, font_scale, (255, 255, 0), thickness)
                y_offset += 25
            
            # ä»»åŠ¡æˆåŠŸæ ‡è®°
            if step_info.get('task_success', False):
                text = "TASK SUCCESS!"
                cv2.putText(overlay_image, text, (10, image.shape[0] - 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3)
            
            return overlay_image
            
        except Exception as e:
            print(f"âš ï¸ ä¿¡æ¯å åŠ å¤±è´¥: {e}")
            return image
    
    def _recording_worker(self):
        """å½•åˆ¶å·¥ä½œçº¿ç¨‹"""
        while self.is_recording and not self.stop_recording_flag.is_set():
            try:
                # ä»é˜Ÿåˆ—è·å–å¸§ï¼ˆå¸¦è¶…æ—¶ï¼‰
                frame = self.frame_queue.get(timeout=1.0)
                
                # å†™å…¥è§†é¢‘æ–‡ä»¶
                if self.video_writer and self.video_writer.isOpened():
                    self.video_writer.write(frame)
                
                self.frame_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âš ï¸ å½•åˆ¶çº¿ç¨‹é”™è¯¯: {e}")
                break
    
    def stop_episode_recording(self):
        """åœæ­¢å½“å‰episodeçš„å½•åˆ¶"""
        if not self.is_recording:
            return
        
        print(f"ğŸ¬ åœæ­¢å½•åˆ¶ Episode {self.current_episode} ({self.frame_count} å¸§)")
        
        # åœæ­¢å½•åˆ¶æ ‡å¿—
        self.is_recording = False
        self.stop_recording_flag.set()
        
        # ç­‰å¾…å½•åˆ¶çº¿ç¨‹ç»“æŸ
        if self.recording_thread and self.recording_thread.is_alive():
            self.recording_thread.join(timeout=5.0)
        
        # å¤„ç†å‰©ä½™å¸§
        while not self.frame_queue.empty():
            try:
                frame = self.frame_queue.get_nowait()
                if self.video_writer and self.video_writer.isOpened():
                    self.video_writer.write(frame)
                self.frame_queue.task_done()
            except queue.Empty:
                break
            except Exception as e:
                print(f"âš ï¸ å¤„ç†å‰©ä½™å¸§é”™è¯¯: {e}")
                break
        
        # å…³é—­è§†é¢‘å†™å…¥å™¨
        if self.video_writer:
            self.video_writer.release()
            self.video_writer = None
        
        if hasattr(self, 'video_path') and self.video_path.exists():
            file_size = self.video_path.stat().st_size / (1024 * 1024)  # MB
            print(f"âœ… è§†é¢‘å·²ä¿å­˜: {self.video_path} ({file_size:.1f}MB)")
        
        self.frame_count = 0
    
    def create_summary_video(self, episode_videos: List[str], output_name: str = "experiment_summary"):
        """åˆ›å»ºå®éªŒæ€»ç»“è§†é¢‘"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_path = self.output_dir / f"{output_name}_{timestamp}.mp4"
            
            print(f"ğŸï¸ åˆ›å»ºæ€»ç»“è§†é¢‘: {summary_path}")
            
            # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
            fourcc = cv2.VideoWriter_fourcc(*self.codec)
            summary_writer = cv2.VideoWriter(
                str(summary_path),
                fourcc,
                self.fps,
                self.video_size
            )
            
            if not summary_writer.isOpened():
                print(f"âŒ æ— æ³•åˆ›å»ºæ€»ç»“è§†é¢‘æ–‡ä»¶")
                return None
            
            # åˆå¹¶æ‰€æœ‰episodeè§†é¢‘
            for i, video_path in enumerate(episode_videos):
                if not Path(video_path).exists():
                    continue
                
                print(f"   åˆå¹¶ Episode {i+1}: {Path(video_path).name}")
                
                cap = cv2.VideoCapture(video_path)
                
                # æ·»åŠ episodeæ ‡é¢˜å¸§
                title_frame = np.zeros((*self.video_size[::-1], 3), dtype=np.uint8)
                cv2.putText(title_frame, f"Episode {i+1}", 
                           (self.video_size[0]//4, self.video_size[1]//2),
                           cv2.FONT_HERSHEY_SIMPLEX, 2.0, (255, 255, 255), 3)
                
                # å†™å…¥æ ‡é¢˜å¸§ï¼ˆæŒç»­1ç§’ï¼‰
                for _ in range(self.fps):
                    summary_writer.write(title_frame)
                
                # å†™å…¥episodeå¸§
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    if frame.shape[:2] != self.video_size[::-1]:
                        frame = cv2.resize(frame, self.video_size)
                    
                    summary_writer.write(frame)
                
                cap.release()
            
            summary_writer.release()
            
            if summary_path.exists():
                file_size = summary_path.stat().st_size / (1024 * 1024)
                print(f"âœ… æ€»ç»“è§†é¢‘å·²ä¿å­˜: {summary_path} ({file_size:.1f}MB)")
                return str(summary_path)
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ€»ç»“è§†é¢‘å¤±è´¥: {e}")
            return None
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.is_recording:
            self.stop_episode_recording()
        
        print("ğŸ§¹ è§†é¢‘å½•åˆ¶å™¨èµ„æºå·²æ¸…ç†")

# ==================== ç®€åŒ–çš„æ•°æ®é€‚é…å™¨ - åŸºç¡€ç‰ˆæœ¬ ====================

class SimpleRoboCasaAdapter:
    """ç®€åŒ–çš„æ•°æ®é€‚é…å™¨ - å…ƒè®¤çŸ¥æ¨¡å—ä¼šè‡ªåŠ¨å¤„ç†ç»´åº¦é€‚é…"""
    
    def __init__(self, image_size: Tuple[int, int] = (224, 224)):
        self.image_size = image_size
        
    def convert_observation(self, robocasa_obs: Dict[str, np.ndarray], 
                          action: np.ndarray, 
                          execution_status: str = "normal") -> SensorData:
        """å°†RoboCasaè§‚å¯Ÿè½¬æ¢ä¸ºSensorDataæ ¼å¼ - è®©å…ƒè®¤çŸ¥æ¨¡å—å¤„ç†ç»´åº¦é€‚é…"""
        
        # 1. å¤„ç†è§†è§‰æ•°æ®
        rgb_image = self._extract_rgb_image(robocasa_obs)
        depth_image = self._extract_depth_image(robocasa_obs)
        
        # 2. å¤„ç†åŠ›è§‰æ•°æ®
        force_torque = self._extract_force_data(robocasa_obs, action)
        contact_detected = self._detect_contact(force_torque)
        
        # 3. å¤„ç†æœ¬ä½“æ„Ÿè§‰æ•°æ® - ä¿æŒåŸå§‹æ•°æ®ï¼Œè®©å…ƒè®¤çŸ¥æ¨¡å—é€‚é…
        joint_positions = robocasa_obs.get("robot0_joint_pos", np.zeros(7))
        joint_velocities = robocasa_obs.get("robot0_joint_vel", np.zeros(7))
        end_effector_pose = self._get_ee_pose(robocasa_obs)
        
        # 4. ç³»ç»ŸçŠ¶æ€
        system1_commands = self._process_action(action)
        
        return SensorData(
            rgb_image=rgb_image,
            depth_image=depth_image,
            force_torque=force_torque,
            contact_detected=contact_detected,
            joint_positions=joint_positions,
            joint_velocities=joint_velocities,
            end_effector_pose=end_effector_pose,
            system1_commands=system1_commands,
            execution_status=execution_status,
            timestamp=time.time()
        )
    
    def _extract_rgb_image(self, obs: Dict[str, np.ndarray]) -> np.ndarray:
        """æå–RGBå›¾åƒ"""
        for key in ["frontview_image", "robot0_eye_in_hand_image"]:
            if key in obs and obs[key] is not None:
                img = obs[key]
                if img.dtype == np.uint8:
                    img = img.astype(np.float32) / 255.0
                if img.shape[:2] != self.image_size:
                    img = cv2.resize(img, self.image_size)
                if len(img.shape) == 2:
                    img = np.stack([img] * 3, axis=-1)
                elif img.shape[2] != 3:
                    img = img[:, :, :3]
                return img
        
        return np.zeros((*self.image_size, 3), dtype=np.float32)
    
    def _extract_depth_image(self, obs: Dict[str, np.ndarray]) -> np.ndarray:
        """æå–æ·±åº¦å›¾åƒ"""
        for key in ["frontview_depth", "robot0_eye_in_hand_depth"]:
            if key in obs and obs[key] is not None:
                depth = obs[key]
                if depth.shape != self.image_size:
                    depth = cv2.resize(depth, self.image_size)
                return depth.astype(np.float32)
        
        return np.ones(self.image_size, dtype=np.float32)
    
    def _extract_force_data(self, obs: Dict[str, np.ndarray], action: np.ndarray) -> np.ndarray:
        """æå–æˆ–ä¼°ç®—åŠ›è§‰æ•°æ®"""
        if "force_torque" in obs:
            force_data = obs["force_torque"][:6]
        else:
            force_data = self._estimate_force_from_action(action)
        
        return force_data.astype(np.float32)
    
    def _estimate_force_from_action(self, action: np.ndarray) -> np.ndarray:
        """ä»åŠ¨ä½œä¼°ç®—åŠ›çŸ©"""
        if action is None or len(action) == 0:
            return np.zeros(6, dtype=np.float32)
        
        action_magnitude = np.linalg.norm(action)
        estimated_force = np.random.normal(0, action_magnitude * 0.05, 6)
        estimated_force = np.clip(estimated_force, -5, 5)
        
        return estimated_force.astype(np.float32)
    
    def _detect_contact(self, force_torque: np.ndarray) -> bool:
        """æ£€æµ‹æ¥è§¦"""
        force_magnitude = np.linalg.norm(force_torque[:3])
        return force_magnitude > 0.5
    
    def _get_ee_pose(self, obs: Dict[str, np.ndarray]) -> np.ndarray:
        """è·å–æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€"""
        pose = np.zeros(7, dtype=np.float32)
        
        if "robot0_eef_pos" in obs:
            pos = obs["robot0_eef_pos"][:3]
            pose[:3] = pos
        else:
            pose[:3] = [0.5, 0.0, 0.8]
        
        if "robot0_eef_quat" in obs:
            quat = obs["robot0_eef_quat"][:4]
            pose[3:7] = quat
        else:
            pose[3:7] = [0, 0, 0, 1]
        
        return pose
    
    def _process_action(self, action: np.ndarray) -> np.ndarray:
        """å¤„ç†ç³»ç»ŸåŠ¨ä½œ"""
        if action is None:
            return np.zeros(8, dtype=np.float32)
        
        if len(action) >= 8:
            return action[:8].astype(np.float32)
        else:
            padded = np.zeros(8, dtype=np.float32)
            padded[:len(action)] = action
            return padded

# ==================== ç®€åŒ–åŠ¨ä½œå¤„ç†å™¨ ====================

class SimpleActionProcessor:
    """ç®€åŒ–åŠ¨ä½œå¤„ç†å™¨ - å‡å°‘å¤æ‚æ€§é¿å…å´©æºƒ"""
    
    def __init__(self):
        self.world_vector_scale = 0.02
        self.rotation_scale = 0.02
        self.gripper_scale = 0.15
        self.processed_actions = 0
        
        print("ğŸ”§ ç®€åŒ–åŠ¨ä½œå¤„ç†å™¨åˆå§‹åŒ–")
        print(f"   ä½ç§»ç¼©æ”¾: {self.world_vector_scale}")
        print(f"   æ—‹è½¬ç¼©æ”¾: {self.rotation_scale}")
    
    def process_groot_action(self, groot_action: Dict[str, np.ndarray]) -> np.ndarray:
        """ç®€åŒ–çš„åŠ¨ä½œå¤„ç†"""
        try:
            world_vector = groot_action.get('world_vector', np.zeros((1, 3)))[0]
            rotation_delta = groot_action.get('rotation_delta', np.zeros((1, 3)))[0]
            gripper_action = groot_action.get('gripper_closedness_action', np.zeros((1, 1)))[0][0]
            
            # ç®€åŒ–çš„ç¼©æ”¾
            scaled_world = np.clip(world_vector * self.world_vector_scale, -0.3, 0.3)
            scaled_rotation = np.clip(rotation_delta * self.rotation_scale, -0.3, 0.3)
            scaled_gripper = np.clip(gripper_action * self.gripper_scale, -1.0, 1.0)
            
            # æ„å»ºSO100åŠ¨ä½œ
            so100_action = np.zeros(6)
            so100_action[0:3] = scaled_world
            so100_action[3:5] = scaled_rotation[:2]
            so100_action[5] = scaled_gripper
            
            self.processed_actions += 1
            
            # ç®€åŒ–çš„ç»Ÿè®¡ï¼ˆå‡å°‘æ‰“å°é¢‘ç‡ï¼‰
            if self.processed_actions % 50 == 0:
                print(f"   ğŸ¯ å·²å¤„ç† {self.processed_actions} æ¬¡åŠ¨ä½œ")
            
            return so100_action
            
        except Exception as e:
            print(f"âš ï¸ åŠ¨ä½œå¤„ç†é”™è¯¯: {e}")
            return np.zeros(6)

# ==================== å¢å¼ºè‰è“ç¯å¢ƒ - é›†æˆå…ƒè®¤çŸ¥ ====================

# class EnhancedStrawberryEnvironment:
#     """å¢å¼ºè‰è“ç¯å¢ƒ - åŸºäºå¯å·¥ä½œçš„StableStrawberryEnvironment + å…ƒè®¤çŸ¥é›†æˆ"""
    
#     def __init__(self, 
#                  so100_xml_path: str = None,
#                  horizon: int = 100,
#                  enable_gui: bool = False,
#                  robot: str = "Panda",
#                  enable_metacognitive: bool = True,
#                  device: str = "cuda" if torch.cuda.is_available() else "cpu"):
#         """
#         åˆå§‹åŒ–å¢å¼ºè‰è“ç¯å¢ƒ
        
#         Args:
#             so100_xml_path: SO100 XMLè·¯å¾„
#             horizon: æœ€å¤§æ­¥æ•°
#             enable_gui: æ˜¯å¦å¯ç”¨GUI
#             robot: æœºå™¨äººç±»å‹
#             enable_metacognitive: æ˜¯å¦å¯ç”¨å…ƒè®¤çŸ¥æ¨¡å—
#             device: è®¾å¤‡ç±»å‹
#         """
#         if not ROBOSUITE_AVAILABLE:
#             raise ImportError("RoboSuiteä¸å¯ç”¨")
        
#         self.horizon = horizon
#         self.so100_xml_path = so100_xml_path
#         self.enable_gui = enable_gui
#         self.robot = robot
#         self.enable_metacognitive = enable_metacognitive and METACOG_AVAILABLE
#         self.device = device
        
#         # ç¯å¢ƒçŠ¶æ€
#         self.env = None
#         self.current_step = 0
        
#         # è‰è“ä»»åŠ¡çŠ¶æ€
#         self.strawberry_positions = np.array([
#             [0.6, 0.1, 0.82],   
#             [0.7, 0.15, 0.82],  
#             [0.8, 0.1, 0.82]    
#         ])
#         self.plate_position = np.array([0.5, -0.2, 0.81])
#         self.strawberry_states = [True, True, True]
#         self.strawberry_on_plate = [False, False, False]
        
#         # ç»Ÿè®¡
#         self.strawberries_picked = 0
#         self.strawberries_on_plate = 0
#         self.total_reward = 0.0
#         self.metacog_interventions = 0
#         self.sensor_failures = 0
        
#         # åŠ¨ä½œå¤„ç†å™¨
#         self.action_processor = SimpleActionProcessor()
        
#         print(f"ğŸ“ åˆ›å»ºå¢å¼ºè‰è“ç¯å¢ƒ")
#         print(f"   æœºå™¨äºº: {robot}")
#         print(f"   GUI: {'å¯ç”¨' if enable_gui else 'ç¦ç”¨ (é¿å…å´©æºƒ)'}")
#         print(f"   æœ€å¤§æ­¥æ•°: {horizon}")
#         print(f"   å…ƒè®¤çŸ¥æ¨¡å—: {'å¯ç”¨' if self.enable_metacognitive else 'ç¦ç”¨'}")
#         print(f"   è®¾å¤‡: {device}")
        
#         # åˆå§‹åŒ–å…ƒè®¤çŸ¥æ¨¡å—
#         if self.enable_metacognitive:
#             self._init_metacognitive_modules()
        
#         # åˆ›å»ºç¯å¢ƒ
#         self._create_stable_environment()
    
#     def _init_metacognitive_modules(self):
#         """åˆå§‹åŒ–å…ƒè®¤çŸ¥æ¨¡å—"""
#         try:
#             print("ğŸ§  åˆå§‹åŒ–å…ƒè®¤çŸ¥æ¨¡å—...")
            
#             self.metacog_module = CompleteMetaCognitiveModule(self.device)
#             self.robocasa_adapter = RoboCasaToMetacogAdapter(image_size=(224, 224))  # ä½¿ç”¨æ ‡å‡†é€‚é…å™¨
#             self.metacog_to_vla_adapter = MetacogToVLASystem2Adapter()
#             self.action_adjuster = ActionAdjuster()
            
#             print("âœ… å…ƒè®¤çŸ¥æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
            
#         except Exception as e:
#             print(f"âŒ å…ƒè®¤çŸ¥æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
#             self.enable_metacognitive = False
    
#     def _create_stable_environment(self):
#         """åˆ›å»ºç¨³å®šç¯å¢ƒ - ä½¿ç”¨æœ€ç®€å•çš„é…ç½®"""
#         try:
#             print("ğŸ—ï¸ åˆ›å»ºç¨³å®šç¯å¢ƒ...")
            
#             # æœ€ç®€å•çš„é…ç½® - é¿å…å¤æ‚å‚æ•°
#             config = {
#                 "env_name": "PnPCounterToCab",
#                 "robots": self.robot,
#                 "controller_configs": load_composite_controller_config(robot=self.robot),
#             }
            
#             print(f"   ä½¿ç”¨æœºå™¨äºº: {self.robot}")
#             print(f"   æ§åˆ¶å™¨: å·²åŠ è½½")
            
#             # éå¸¸ä¿å®ˆçš„ç¯å¢ƒé…ç½®
#             self.env = robosuite.make(
#                 **config,
#                 has_renderer=False,  # å¼ºåˆ¶å…³é—­æ¸²æŸ“å™¨é¿å…å´©æºƒ
#                 has_offscreen_renderer=True,  # ä¿æŒç¦»å±æ¸²æŸ“
#                 render_camera=None,
#                 ignore_done=True,
#                 use_camera_obs=True,
#                 control_freq=20,
#                 camera_names=["robot0_eye_in_hand"],  # åªä½¿ç”¨ä¸€ä¸ªç›¸æœº
#                 camera_heights=480,
#                 camera_widths=640,
#                 initialization_noise=None,  # å…³é—­å™ªå£°
#             )
            
#             print("âœ… ç¨³å®šç¯å¢ƒåˆ›å»ºæˆåŠŸ")
            
#             # ç®€å•éªŒè¯
#             if hasattr(self.env, 'action_space'):
#                 print(f"   åŠ¨ä½œç©ºé—´: {getattr(self.env.action_space, 'shape', 'Unknown')}")
            
#             print("ğŸ‰ ç¨³å®šç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼")
            
#         except Exception as e:
#             print(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
#             import traceback
#             traceback.print_exc()
#             raise
    
#     def reset(self) -> Tuple[Dict[str, Any], Dict[str, Any]]:
#         """é‡ç½®ç¯å¢ƒ"""
#         try:
#             print("ğŸ”„ é‡ç½®ç¨³å®šç¯å¢ƒ...")
            
#             obs = self.env.reset()
#             self.current_step = 0
            
#             # é‡ç½®çŠ¶æ€
#             self.strawberries_picked = 0
#             self.strawberries_on_plate = 0
#             self.total_reward = 0.0
#             self.strawberry_states = [True, True, True]
#             self.strawberry_on_plate = [False, False, False]
#             self.metacog_interventions = 0
#             self.sensor_failures = 0
            
#             # å¤„ç†è§‚æµ‹æ•°æ® - ä½¿ç”¨çœŸå®æ•°æ®
#             processed_obs = self._process_real_observation(obs)
            
#             # å®‰å…¨çš„ä½ç½®è°ƒæ•´
#             robot_pos = processed_obs.get("robot0_eef_pos", np.array([0.5, 0.0, 0.8]))
#             print(f"   æœºå™¨äººä½ç½®: {robot_pos}")
            
#             # ç®€å•çš„ä½ç½®è°ƒæ•´
#             if abs(robot_pos[0]) > 1.5 or abs(robot_pos[1]) > 1.5:
#                 print("   âš ï¸ è°ƒæ•´ç‰©ä½“ä½ç½®")
#                 self.strawberry_positions += robot_pos[:3] * 0.5
#                 self.plate_position += robot_pos[:3] * 0.5
            
#             # æ„å»ºä¿¡æ¯
#             info = {
#                 "task_name": "Enhanced Strawberry Pick and Place",
#                 "task_description": "Pick up strawberries and place them in the target location",
#                 "step": self.current_step,
#                 "max_steps": self.horizon,
#                 "metacognitive_enabled": self.enable_metacognitive,
#                 "strawberry_task_progress": {
#                     "strawberries_picked": self.strawberries_picked,
#                     "strawberries_on_plate": self.strawberries_on_plate,
#                     "total_strawberries": 3,
#                 }
#             }
            
#             print("âœ… ç¨³å®šç¯å¢ƒé‡ç½®æˆåŠŸ")
            
#             return processed_obs, info
            
#         except Exception as e:
#             print(f"âŒ ç¯å¢ƒé‡ç½®å¤±è´¥: {e}")
#             # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
#             return self._get_safe_default_obs(), {"step": 0, "task_name": "Safe Default"}
    
#     def step(self, action: np.ndarray) -> Tuple[Dict[str, Any], float, bool, bool, Dict[str, Any]]:
#         """å®‰å…¨çš„æ­¥è¿›"""
#         try:
#             # å®‰å…¨çš„åŠ¨ä½œé€‚é…
#             adapted_action = self._safe_adapt_action(action)
            
#             # ç¯å¢ƒæ­¥è¿›
#             obs, reward, done, info = self.env.step(adapted_action)
#             self.current_step += 1
            
#             # å¤„ç†è§‚æµ‹æ•°æ® - ä½¿ç”¨çœŸå®æ•°æ®
#             processed_obs = self._process_real_observation(obs)
            
#             # ä»»åŠ¡å¥–åŠ±è¯„ä¼°
#             task_reward, task_success = self._safe_evaluate_task(processed_obs, action)
#             reward += task_reward
#             self.total_reward += reward
            
#             # å…ƒè®¤çŸ¥å¤„ç†
#             metacog_feedback = None
#             if self.enable_metacognitive:
#                 metacog_feedback = self._process_metacognitive_feedback(processed_obs, adapted_action)
            
#             # ä»»åŠ¡å®Œæˆ
#             if task_success:
#                 done = True
#                 reward += 10.0
#                 print(f"ğŸ‰ è‰è“ä»»åŠ¡å®Œæˆï¼")
            
#             # è¶…æ—¶
#             if self.current_step >= self.horizon:
#                 done = True
            
#             # å¢å¼ºä¿¡æ¯
#             enhanced_info = {
#                 **info,
#                 "task_name": "Enhanced Strawberry Pick and Place",
#                 "task_description": "Pick up strawberries and place them in the target location",
#                 "step": self.current_step,
#                 "max_steps": self.horizon,
#                 "task_success": task_success,
#                 "total_reward": self.total_reward,
#                 "metacog_interventions": self.metacog_interventions,
#                 "sensor_failures": self.sensor_failures,
#                 "metacognitive_feedback": metacog_feedback,
#                 "strawberry_task_progress": {
#                     "strawberries_picked": self.strawberries_picked,
#                     "strawberries_on_plate": self.strawberries_on_plate,
#                     "total_strawberries": 3,
#                 }
#             }
            
#             # ç®€åŒ–çš„è¿›åº¦æ˜¾ç¤º
#             if self.current_step % 30 == 0:
#                 print(f"   ğŸ“Š æ­¥éª¤ {self.current_step}: æ‹£é€‰={self.strawberries_picked}, æ”¾ç½®={self.strawberries_on_plate}, å¥–åŠ±={self.total_reward:.2f}")
#                 if self.enable_metacognitive:
#                     print(f"   ğŸ§  å…ƒè®¤çŸ¥å¹²é¢„: {self.metacog_interventions}")
            
#             return processed_obs, reward, done, False, enhanced_info
            
#         except Exception as e:
#             print(f"âŒ æ­¥è¿›å¤±è´¥: {e}")
#             self.sensor_failures += 1
#             # è¿”å›å®‰å…¨å€¼
#             return self._get_safe_default_obs(), 0.0, True, False, {"step": self.current_step}
    
#     def _process_metacognitive_feedback(self, obs: Dict[str, np.ndarray], action: np.ndarray) -> Optional[str]:
#         """å¤„ç†å…ƒè®¤çŸ¥åé¦ˆ - ä½¿ç”¨ä¿®å¤çš„å…ƒè®¤çŸ¥æ¨¡å—"""
#         if not self.enable_metacognitive:
#             return None
        
#         try:
#             # è½¬æ¢è§‚æµ‹æ•°æ®ä¸ºä¼ æ„Ÿå™¨æ•°æ®æ ¼å¼ - å…ƒè®¤çŸ¥æ¨¡å—ä¼šè‡ªåŠ¨é€‚é…ç»´åº¦
#             sensor_data = self.robocasa_adapter.convert_observation(
#                 obs, action, execution_status="normal"
#             )
            
#             # è·å–å…ƒè®¤çŸ¥è¾“å‡º
#             metacog_output = self.metacog_module.process_sensor_data(sensor_data)
            
#             # è½¬æ¢ä¸ºVLA System2æŒ‡ä»¤
#             instruction = self.metacog_to_vla_adapter.convert_to_system2_instruction(metacog_output)
            
#             # è®°å½•å¹²é¢„
#             if instruction and metacog_output.directive != DirectiveType.CONTINUE:
#                 self.metacog_interventions += 1
#                 if self.current_step % 30 == 0:  # é€‚åº¦çš„æ‰“å°é¢‘ç‡
#                     print(f"   ğŸ§  å…ƒè®¤çŸ¥å¹²é¢„: {instruction}")
            
#             return instruction
            
#         except Exception as e:
#             if self.current_step % 40 == 0:  # è¿›ä¸€æ­¥å‡å°‘é”™è¯¯æ‰“å°é¢‘ç‡
#                 print(f"âš ï¸ å…ƒè®¤çŸ¥å¤„ç†é”™è¯¯: {e}")
#             return None
    
#     def _process_real_observation(self, obs: Dict[str, Any]) -> Dict[str, np.ndarray]:
#         """å¤„ç†çœŸå®è§‚æµ‹æ•°æ® - æ›¿æ¢éšæœºæ•°æ®"""
#         processed = {}
        
#         try:
#             # å¤„ç†å›¾åƒæ•°æ® - ä½¿ç”¨çœŸå®ç›¸æœºæ•°æ®
#             image_found = False
#             for camera in ["robot0_robotview", "robot0_eye_in_hand"]:
#                 rgb_key = f"{camera}_image"
#                 if rgb_key in obs and obs[rgb_key] is not None:
#                     try:
#                         img = obs[rgb_key]
#                         if img is not None and img.size > 0:
#                             if img.shape[:2] != (480, 640):
#                                 img = cv2.resize(img, (640, 480))
#                             processed["frontview_image"] = img.astype(np.uint8)
#                             image_found = True
#                             break
#                     except Exception as e:
#                         print(f"âš ï¸ å¤„ç†{camera}å›¾åƒå¤±è´¥: {e}")
#                         continue
            
#             if not image_found:
#                 processed["frontview_image"] = np.zeros((480, 640, 3), dtype=np.uint8)
#                 self.sensor_failures += 1
            
#             # å¤„ç†æ·±åº¦æ•°æ®
#             depth_found = False
#             for camera in ["robot0_robotview", "robot0_eye_in_hand"]:
#                 depth_key = f"{camera}_depth"
#                 if depth_key in obs and obs[depth_key] is not None:
#                     try:
#                         depth = obs[depth_key]
#                         if depth.size > 0:
#                             if depth.shape != (480, 640):
#                                 depth = cv2.resize(depth, (640, 480))
#                             processed["frontview_depth"] = depth.astype(np.float32)
#                             depth_found = True
#                             break
#                     except Exception:
#                         continue
            
#             if not depth_found:
#                 processed["frontview_depth"] = np.ones((480, 640), dtype=np.float32)
            
#             # å¤„ç†æœºå™¨äººçŠ¶æ€æ•°æ® - ä½¿ç”¨çœŸå®ä¼ æ„Ÿå™¨æ•°æ®
#             robot_keys = ["robot0_joint_pos", "robot0_eef_pos", "robot0_eef_quat", "robot0_gripper_qpos"]
            
#             for key in robot_keys:
#                 if key in obs and obs[key] is not None:
#                     try:
#                         data = np.array(obs[key], dtype=np.float32)
#                         # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
#                         if np.any(np.isnan(data)) or np.any(np.isinf(data)):
#                             print(f"âš ï¸ {key} åŒ…å«æ— æ•ˆæ•°æ®")
#                             data = np.nan_to_num(data, nan=0.0, posinf=1.0, neginf=-1.0)
                        
#                         if "joint" in key:
#                             processed[key] = data[:5] if len(data) > 5 else data
#                         else:
#                             processed[key] = data
#                     except Exception as e:
#                         print(f"âš ï¸ å¤„ç†{key}å¤±è´¥: {e}")
#                         self.sensor_failures += 1
            
#             # æä¾›å®‰å…¨çš„é»˜è®¤å€¼ï¼ˆåŸºäºç‰©ç†çº¦æŸè€Œä¸æ˜¯éšæœºï¼‰
#             if "robot0_eef_pos" not in processed:
#                 processed["robot0_eef_pos"] = np.array([0.5, 0.0, 0.8], dtype=np.float32)
#             if "robot0_joint_pos" not in processed:
#                 processed["robot0_joint_pos"] = np.zeros(5, dtype=np.float32)
#             if "robot0_eef_quat" not in processed:
#                 processed["robot0_eef_quat"] = np.array([0, 0, 0, 1], dtype=np.float32)
#             if "robot0_gripper_qpos" not in processed:
#                 processed["robot0_gripper_qpos"] = np.zeros(2, dtype=np.float32)
            
#             # æ·»åŠ ä»»åŠ¡ä¿¡æ¯
#             processed["robot_type"] = "SO100"
#             processed["task_description"] = "Enhanced: Pick strawberries and place them carefully"
#             processed["current_step"] = self.current_step
            
#             return processed
            
#         except Exception as e:
#             print(f"âš ï¸ è§‚æµ‹æ•°æ®å¤„ç†é”™è¯¯: {e}")
#             self.sensor_failures += 1
#             return self._get_safe_default_obs()
    
#     def _safe_adapt_action(self, action: np.ndarray) -> np.ndarray:
#         """å®‰å…¨çš„åŠ¨ä½œé€‚é…"""
#         try:
#             if not isinstance(action, np.ndarray):
#                 action = np.array(action)
            
#             # ç¡®ä¿åŠ¨ä½œæ˜¯æœ‰é™çš„
#             action = np.nan_to_num(action, nan=0.0, posinf=0.1, neginf=-0.1)
            
#             if len(action) == 6:
#                 adapted = np.zeros(7)
#                 adapted[0:3] = action[0:3]
#                 adapted[3:5] = action[3:5]
#                 adapted[5] = 0.0
#                 adapted[6] = action[5]
#                 return np.clip(adapted, -1.0, 1.0)
#             elif len(action) == 7:
#                 return np.clip(action, -1.0, 1.0)
#             else:
#                 adapted = np.zeros(7)
#                 adapted[:min(len(action), 7)] = action[:7]
#                 return np.clip(adapted, -1.0, 1.0)
                
#         except Exception as e:
#             print(f"âš ï¸ åŠ¨ä½œé€‚é…é”™è¯¯: {e}")
#             return np.zeros(7)
    
#     def _safe_evaluate_task(self, obs: Dict[str, Any], action: np.ndarray) -> Tuple[float, bool]:
#         """å®‰å…¨çš„è‰è“ä»»åŠ¡è¯„ä¼° - åŸºäºçœŸå®ä¼ æ„Ÿå™¨æ•°æ®"""
#         try:
#             reward = 0.0
#             task_success = False
            
#             robot_pos = obs.get("robot0_eef_pos")
#             if robot_pos is None:
#                 return reward, task_success
            
#             gripper_action = action[-1] if len(action) > 0 else 0.0
            
#             # è‰è“æ£€æµ‹ - åŸºäºçœŸå®ä½ç½®æ•°æ®
#             for i, (strawberry_pos, is_available) in enumerate(zip(self.strawberry_positions, self.strawberry_states)):
#                 if not is_available:
#                     continue
                
#                 try:
#                     distance = np.linalg.norm(robot_pos - strawberry_pos)
                    
#                     if distance < 0.3:  # æ¥è¿‘è‰è“
#                         reward += 0.5
                        
#                         if distance < 0.2 and gripper_action > 0.2:  # æŠ“å–åŠ¨ä½œ
#                             if self.strawberry_states[i]:
#                                 self.strawberry_states[i] = False
#                                 self.strawberries_picked += 1
#                                 reward += 2.0
#                                 print(f"   ğŸ“ æ‹£é€‰è‰è“{i+1}!")
                                
#                 except Exception:
#                     continue
            
#             # ç›˜å­æ£€æµ‹ - åŸºäºçœŸå®ä½ç½®æ•°æ®
#             try:
#                 plate_distance = np.linalg.norm(robot_pos - self.plate_position)
                
#                 if plate_distance < 0.25:  # æ¥è¿‘ç›˜å­
#                     reward += 0.5
                    
#                     if plate_distance < 0.15 and gripper_action < -0.2:  # æ”¾ç½®åŠ¨ä½œ
#                         picked = sum(1 for state in self.strawberry_states if not state)
#                         on_plate = sum(1 for state in self.strawberry_on_plate if state)
                        
#                         if picked > on_plate:
#                             for i, on_plate_state in enumerate(self.strawberry_on_plate):
#                                 if not on_plate_state and not self.strawberry_states[i]:
#                                     self.strawberry_on_plate[i] = True
#                                     self.strawberries_on_plate += 1
#                                     reward += 3.0
#                                     print(f"   ğŸ½ï¸ æ”¾ç½®è‰è“{i+1}!")
#                                     break
#             except Exception:
#                 pass
            
#             # ä»»åŠ¡å®Œæˆåˆ¤æ–­
#             if self.strawberries_on_plate >= 3:
#                 task_success = True
            
#             return reward, task_success
            
#         except Exception as e:
#             print(f"âš ï¸ ä»»åŠ¡è¯„ä¼°é”™è¯¯: {e}")
#             return 0.0, False
    
#     def _get_safe_default_obs(self) -> Dict[str, np.ndarray]:
#         """è·å–å®‰å…¨çš„é»˜è®¤è§‚æµ‹"""
#         return {
#             "frontview_image": np.zeros((480, 640, 3), dtype=np.uint8),
#             "frontview_depth": np.ones((480, 640), dtype=np.float32),
#             "robot0_joint_pos": np.zeros(5, dtype=np.float32),
#             "robot0_eef_pos": np.array([0.5, 0.0, 0.8], dtype=np.float32),
#             "robot0_eef_quat": np.array([0, 0, 0, 1], dtype=np.float32),
#             "robot0_gripper_qpos": np.zeros(2, dtype=np.float32),
#             "robot_type": "SO100",
#             "task_description": "Safe default observation",
#             "current_step": self.current_step
#         }
    
#     def get_action_space(self):
#         """è·å–åŠ¨ä½œç©ºé—´"""
#         if self.env is None:
#             raise RuntimeError("ç¯å¢ƒæœªåˆå§‹åŒ–")
        
#         if hasattr(self.env, 'action_space'):
#             return self.env.action_space
#         else:
#             import gym.spaces
#             return gym.spaces.Box(low=-1.0, high=1.0, shape=(7,), dtype=np.float32)
    
#     def close(self):
#         """å®‰å…¨å…³é—­ç¯å¢ƒ"""
#         if self.env is not None:
#             try:
#                 self.env.close()
#                 print("ğŸ”’ å¢å¼ºè‰è“ç¯å¢ƒå·²å…³é—­")
#                 print(f"ğŸ“Š æœ€ç»ˆç»“æœ: æ‹£é€‰={self.strawberries_picked}/3, æ”¾ç½®={self.strawberries_on_plate}/3")
#                 print(f"   æ€»å¥–åŠ±={self.total_reward:.2f}, å…ƒè®¤çŸ¥å¹²é¢„={self.metacog_interventions}, ä¼ æ„Ÿå™¨å¤±è´¥={self.sensor_failures}")
#             except Exception as e:
#                 print(f"âš ï¸ å…³é—­ç¯å¢ƒé”™è¯¯: {e}")
#             finally:
#                 self.env = None




from robosuite.models.objects import BoxObject, CylinderObject, CanObject
from robosuite.utils.placement_samplers import UniformRandomSampler

class EnhancedStrawberryEnvironment:
    """
    å¢å¼ºçš„æ¡Œé¢è‰è“ç¯å¢ƒ - ä½¿ç”¨è‡ªå®šä¹‰çš„æ¡Œé¢åœºæ™¯æ›¿æ¢å¨æˆ¿ç¯å¢ƒã€‚
    (æœ€ç»ˆå…¼å®¹ç‰ˆï¼Œä¿®å¤äº†sizeå‚æ•°é—®é¢˜)
    """
    
    def __init__(self, 
                 so100_xml_path: str = None, 
                 horizon: int = 100,
                 enable_gui: bool = False,
                 robot: str = "Panda",
                 enable_metacognitive: bool = True,
                 device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        
        if not ROBOSUITE_AVAILABLE:
            raise ImportError("RoboSuiteä¸å¯ç”¨")
        
        self.horizon = horizon
        self.enable_gui = enable_gui
        self.robot = robot
        self.enable_metacognitive = enable_metacognitive and METACOG_AVAILABLE
        self.device = device
        
        self.env = None
        self.current_step = 0
        self.table_top_offset = None
        self.plate_pos = None
        self.object_names = ["strawberry1", "strawberry2", "strawberry3", "grape1", "grape2", "grape3"]
        self.held_object = None
        self.placed_strawberries = set()
        
        self.total_reward = 0.0
        self.metacog_interventions = 0
        self.sensor_failures = 0
        
        self.action_processor = SimpleActionProcessor()
        
        print(f"ğŸ“ åˆ›å»ºå¢å¼ºçš„ã€æ¡Œé¢ã€‘è‰è“ç¯å¢ƒ")
        print(f"   æœºå™¨äºº: {robot}")
        print(f"   GUI: {'å¯ç”¨' if enable_gui else 'ç¦ç”¨'}")
        print(f"   æœ€å¤§æ­¥æ•°: {horizon}")
        
        if self.enable_metacognitive:
            self._init_metacognitive_modules()
        
        self._create_tabletop_environment()

    def _init_metacognitive_modules(self):
        try:
            print("ğŸ§  åˆå§‹åŒ–å…ƒè®¤çŸ¥æ¨¡å—...")
            self.metacog_module = CompleteMetaCognitiveModule(self.device)
            self.robocasa_adapter = RoboCasaToMetacogAdapter(image_size=(224, 224))
            self.metacog_to_vla_adapter = MetacogToVLASystem2Adapter()
            self.action_adjuster = ActionAdjuster()
            print("âœ… å…ƒè®¤çŸ¥æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ å…ƒè®¤çŸ¥æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enable_metacognitive = False

    def _create_tabletop_environment(self):
        """
        åˆ›å»ºè‡ªå®šä¹‰çš„æ¡Œé¢ç¯å¢ƒ - å…¼å®¹æ—§ç‰ˆRobosuite API (sizeå‚æ•°)
        """
        try:
            print("ğŸ—ï¸ åˆ›å»ºè‡ªå®šä¹‰æ¡Œé¢ç¯å¢ƒ...")
            
            # ã€ã€ã€ å·²ä¿®å¤ ã€‘ã€‘ã€‘
            # 1. å®šä¹‰æˆ‘ä»¬çš„ç‰©ä½“ï¼Œä½¿ç”¨ size_min å’Œ size_max
            strawberry_size = [0.02, 0.025] # [radius, half_height]
            strawberries = [
                CanObject(
                    name=f"strawberry{i+1}", 
                    size_min=strawberry_size, # ä½¿ç”¨æ—§ç‰ˆAPI
                    size_max=strawberry_size, # ä½¿ç”¨æ—§ç‰ˆAPI
                    rgba=[1, 0, 0, 1]
                ) for i in range(3)
            ]
            
            grape_size = [0.018, 0.018] # [radius, half_height]
            grapes = [
                CylinderObject(
                    name=f"grape{i+1}", 
                    size_min=grape_size, # ä½¿ç”¨æ—§ç‰ˆAPI
                    size_max=grape_size, # ä½¿ç”¨æ—§ç‰ˆAPI
                    rgba=[0.5, 1, 0.5, 1]
                ) for i in range(3)
            ]
            
            plate_size = [0.12, 0.01] # [radius, half_height]
            plate = CylinderObject(
                name="plate",
                size_min=plate_size, # ä½¿ç”¨æ—§ç‰ˆAPI
                size_max=plate_size, # ä½¿ç”¨æ—§ç‰ˆAPI
                rgba=[1, 1, 1, 1],
                solimp=[0.998, 0.998, 0.001],
                solref=[0.001, 1]
            )
            
            # 2. é…ç½®ç¯å¢ƒ (ä¸ä¸Šä¸€ç‰ˆç›¸åŒ)
            config = {
                "env_name": "PickPlace",
                "robots": self.robot,
                "controller_configs": load_composite_controller_config(robot=self.robot),
                "placement_initializer": None,
                "single_object_mode": 2,
                "has_renderer": self.enable_gui,
                "has_offscreen_renderer": True,
                "ignore_done": True,
                "use_camera_obs": True,
                "control_freq": 20,
                "camera_configs": {
                    "type": "Camera",
                    "name": "worldview",
                    "pos": np.array([0.6, 0.0, 1.4]),
                    "quat": np.array([0.653, 0.271, 0.653, -0.271]),
                    "camera_fovy": 50
                },
                "camera_names": "worldview",
                "camera_heights": 480,
                "camera_widths": 640,
                "mujoco_objects": strawberries + grapes + [plate]
            }
            
            # 3. åˆ›å»ºç¯å¢ƒ
            self.env = robosuite.make(**config)

            # 4. è·å–æ¡Œé¢ä¿¡æ¯
            self.table_top_offset = self.env.table_top_offset
            
            print("âœ… è‡ªå®šä¹‰æ¡Œé¢ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼")
            
        except Exception as e:
            print(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise


    def reset(self) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """é‡ç½®ç¯å¢ƒï¼Œå¹¶æ‰‹åŠ¨æ”¾ç½®æ‰€æœ‰ç‰©ä½“"""
        try:
            print("ğŸ”„ é‡ç½®æ¡Œé¢ç¯å¢ƒ...")
            obs = self.env.reset()
            self.current_step = 0
            
            # é‡ç½®çŠ¶æ€
            self.total_reward = 0.0
            self.held_object = None
            self.placed_strawberries.clear()
            
            # --- æ‰‹åŠ¨æ”¾ç½®ç‰©ä½“ ---
            # å®šä¹‰æ¡Œé¢ä¸Šå¯æ”¾ç½®ç‰©ä½“çš„åŒºåŸŸ
            table_pos = self.table_top_offset
            x_range = [-0.15, 0.15]
            y_range = [-0.25, 0.25]
            
            # æ”¾ç½®ç›˜å­
            self.plate_pos = np.array([table_pos[0] - 0.25, table_pos[1], table_pos[2]])
            self.env.sim.data.set_joint_qpos(
                "plate_joint0",
                np.concatenate([self.plate_pos, [1, 0, 0, 0]])
            )
            
            # éšæœºæ”¾ç½®è‰è“å’Œè‘¡è„
            for obj_name in self.object_names:
                while True:
                    # åœ¨æ¡Œé¢ä¸Šéšæœºé€‰ä¸€ä¸ªç‚¹
                    random_pos = table_pos + np.array([
                        np.random.uniform(*x_range),
                        np.random.uniform(*y_range),
                        0.02 # ç‰©ä½“é«˜åº¦åç§»
                    ])
                    
                    # ç¡®ä¿ä¸ä¼šå’Œç›˜å­é‡å 
                    if np.linalg.norm(random_pos[:2] - self.plate_pos[:2]) > 0.15:
                        self.env.sim.data.set_joint_qpos(
                            f"{obj_name}_joint0",
                            np.concatenate([random_pos, [1, 0, 0, 0]])
                        )
                        break
            
            # --- ç»“æŸæ‰‹åŠ¨æ”¾ç½® ---
            
            processed_obs = self._process_real_observation(obs)
            info = self._get_current_info()
            
            print("âœ… æ¡Œé¢ç¯å¢ƒé‡ç½®æˆåŠŸï¼Œç‰©ä½“å·²æ”¾ç½®ã€‚")
            
            return processed_obs, info
            
        except Exception as e:
            print(f"âŒ ç¯å¢ƒé‡ç½®å¤±è´¥: {e}")
            return self._get_safe_default_obs(), {"step": 0, "task_name": "Safe Default"}

    def step(self, action: np.ndarray) -> Tuple[Dict[str, Any], float, bool, bool, Dict[str, Any]]:
        """å®‰å…¨çš„æ­¥è¿›"""
        try:
            adapted_action = self._safe_adapt_action(action)
            obs, reward, done, info = self.env.step(adapted_action)
            self.current_step += 1
            
            processed_obs = self._process_real_observation(obs)
            
            # ä½¿ç”¨å…¨æ–°çš„æ¡Œé¢ä»»åŠ¡è¯„ä¼°å‡½æ•°
            task_reward, task_success = self._evaluate_tabletop_task(processed_obs, adapted_action)
            reward = task_reward # æˆ‘ä»¬åªå…³å¿ƒæˆ‘ä»¬çš„ä»»åŠ¡å¥–åŠ±
            self.total_reward += reward
            
            metacog_feedback = None
            if self.enable_metacognitive:
                metacog_feedback = self._process_metacognitive_feedback(processed_obs, adapted_action)
            
            if task_success:
                done = True
                self.total_reward += 10.0 # æˆåŠŸæ—¶ç»™äºˆå·¨å¤§å¥–åŠ±
                print(f"ğŸ‰ è‰è“ä»»åŠ¡å®Œæˆï¼")
            
            if self.current_step >= self.horizon:
                done = True

            enhanced_info = self._get_current_info()
            enhanced_info['task_success'] = task_success
            enhanced_info['metacognitive_feedback'] = metacog_feedback
            
            if self.enable_gui:
                self.env.render()

            return processed_obs, reward, done, False, enhanced_info
            
        except Exception as e:
            print(f"âŒ æ­¥è¿›å¤±è´¥: {e}")
            self.sensor_failures += 1
            return self._get_safe_default_obs(), 0.0, True, False, {"step": self.current_step}

    def _evaluate_tabletop_task(self, obs: Dict[str, Any], action: np.ndarray) -> Tuple[float, bool]:
        """
        å…¨æ–°çš„ä»»åŠ¡è¯„ä¼°å‡½æ•°ï¼ŒåŸºäº3Dåæ ‡ã€‚
        """
        reward = 0.0
        eef_pos = obs.get("robot0_eef_pos")
        gripper_openness = obs.get("robot0_gripper_qpos")[0] # å‡è®¾å€¼è¶Šå¤§è¶Šå¼€
        
        # 1. æŠ“å–é€»è¾‘
        if self.held_object is None:
            # å¯»æ‰¾æœ€è¿‘çš„ã€å°šæœªæ”¾ç½®çš„è‰è“
            min_dist = float('inf')
            target_strawberry = None
            for i in range(3):
                s_name = f"strawberry{i+1}"
                if s_name not in self.placed_strawberries:
                    s_pos = self.env.sim.data.body_xpos[self.env.sim.model.body_name2id(f"{s_name}_main")]
                    dist = np.linalg.norm(eef_pos - s_pos)
                    if dist < min_dist:
                        min_dist = dist
                        target_strawberry = s_name
            
            if target_strawberry:
                # å¥–åŠ±ï¼šé è¿‘ç›®æ ‡è‰è“
                reward += 1.0 - np.tanh(10.0 * min_dist)
                
                # æ£€æŸ¥æ˜¯å¦æŠ“å–æˆåŠŸ
                if min_dist < 0.05 and gripper_openness < 0.01: # Gripper is closed
                    self.held_object = target_strawberry
                    reward += 5.0 # æŠ“å–æˆåŠŸå¥–åŠ±
                    print(f"   ğŸ“ æŠ“å– {self.held_object}!")

        # 2. æ”¾ç½®é€»è¾‘
        else:
            # å¥–åŠ±ï¼šé è¿‘ç›˜å­
            dist_to_plate = np.linalg.norm(eef_pos[:2] - self.plate_pos[:2])
            reward += 1.0 - np.tanh(10.0 * dist_to_plate)
            
            # è·å–å½“å‰æŠ“ç€ç‰©ä½“çš„å®æ—¶ä½ç½®
            held_obj_pos = self.env.sim.data.body_xpos[self.env.sim.model.body_name2id(f"{self.held_object}_main")]

            # æ£€æŸ¥æ˜¯å¦æ”¾ç½®æˆåŠŸ
            dist_on_plate = np.linalg.norm(held_obj_pos[:2] - self.plate_pos[:2])
            is_over_plate = dist_on_plate < 0.1 # ç›˜å­åŠå¾„
            is_low_enough = held_obj_pos[2] < self.table_top_offset[2] + 0.05
            
            if is_over_plate and is_low_enough and gripper_openness > 0.02: # Gripper is opening
                print(f"   ğŸ½ï¸ æ”¾ç½® {self.held_object}!")
                self.placed_strawberries.add(self.held_object)
                self.held_object = None
                reward += 10.0 # æ”¾ç½®æˆåŠŸå¥–åŠ±

        # 3. ä»»åŠ¡æˆåŠŸåˆ¤æ–­
        task_success = len(self.placed_strawberries) == 3
        return reward, task_success
    
    # --- Helper and Unchanged Methods ---
    
    def _get_current_info(self) -> Dict[str, Any]:
        return {
            "task_name": "Tabletop Strawberry Pick and Place",
            "task_description": "Pick up red strawberries and place them on the white plate.",
            "step": self.current_step,
            "max_steps": self.horizon,
            "total_reward": self.total_reward,
            "metacog_interventions": self.metacog_interventions,
            "sensor_failures": self.sensor_failures,
            "strawberry_task_progress": {
                "strawberries_picked": 1 if self.held_object else 0,
                "strawberries_on_plate": len(self.placed_strawberries),
                "total_strawberries": 3,
            }
        }

    def _process_real_observation(self, obs: Dict[str, Any]) -> Dict[str, np.ndarray]:
        processed = {}
        try:
            # ä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„ 'worldview' ç›¸æœº
            rgb_key = "worldview_image"
            if rgb_key in obs and obs[rgb_key] is not None:
                # Robosuiteè¿”å›çš„å›¾åƒæ˜¯ä¸Šä¸‹é¢ å€’çš„ï¼Œéœ€è¦ç¿»è½¬
                img = obs[rgb_key][::-1] 
                processed["frontview_image"] = img.astype(np.uint8)
            else:
                processed["frontview_image"] = np.zeros((480, 640, 3), dtype=np.uint8)
                self.sensor_failures += 1
            
            depth_key = "worldview_depth"
            if depth_key in obs and obs[depth_key] is not None:
                depth = obs[depth_key][::-1]
                processed["frontview_depth"] = depth.astype(np.float32)
            else:
                processed["frontview_depth"] = np.ones((480, 640), dtype=np.float32)
            
            robot_keys = ["robot0_joint_pos", "robot0_eef_pos", "robot0_eef_quat", "robot0_gripper_qpos"]
            for key in robot_keys:
                if key in obs and obs[key] is not None:
                    processed[key] = np.array(obs[key], dtype=np.float32)

            return processed
        except Exception as e:
            print(f"âš ï¸ è§‚æµ‹æ•°æ®å¤„ç†é”™è¯¯: {e}")
            self.sensor_failures += 1
            return self._get_safe_default_obs()

    def close(self):
        """å®‰å…¨å…³é—­ç¯å¢ƒ"""
        if self.env is not None:
            try:
                self.env.close()
                print("ğŸ”’ å¢å¼ºæ¡Œé¢ç¯å¢ƒå·²å…³é—­")
                print(f"ğŸ“Š æœ€ç»ˆç»“æœ: æ”¾ç½®={len(self.placed_strawberries)}/3")
            except Exception as e:
                print(f"âš ï¸ å…³é—­ç¯å¢ƒé”™è¯¯: {e}")
            finally:
                self.env = None

    def _safe_adapt_action(self, action: np.ndarray) -> np.ndarray:
        """å®‰å…¨çš„åŠ¨ä½œé€‚é…"""
        try:
            if not isinstance(action, np.ndarray):
                action = np.array(action)
            
            action = np.nan_to_num(action, nan=0.0, posinf=0.1, neginf=-0.1)
            
            if len(action) == 6:
                adapted = np.zeros(7)
                adapted[0:3] = action[0:3]
                adapted[3:5] = action[3:5]
                adapted[5] = 0.0
                adapted[6] = action[5]
                return np.clip(adapted, -1.0, 1.0)
            elif len(action) == 7:
                return np.clip(action, -1.0, 1.0)
            else:
                adapted = np.zeros(7)
                adapted[:min(len(action), 7)] = action[:7]
                return np.clip(adapted, -1.0, 1.0)
                
        except Exception as e:
            print(f"âš ï¸ åŠ¨ä½œé€‚é…é”™è¯¯: {e}")
            return np.zeros(7)
    
    def _get_safe_default_obs(self) -> Dict[str, np.ndarray]:
        """è·å–å®‰å…¨çš„é»˜è®¤è§‚æµ‹"""
        return {
            "frontview_image": np.zeros((480, 640, 3), dtype=np.uint8),
            "frontview_depth": np.ones((480, 640), dtype=np.float32),
            "robot0_joint_pos": np.zeros(5, dtype=np.float32),
            "robot0_eef_pos": np.array([0.5, 0.0, 0.8], dtype=np.float32),
            "robot0_eef_quat": np.array([0, 0, 0, 1], dtype=np.float32),
            "robot0_gripper_qpos": np.zeros(2, dtype=np.float32),
            "robot_type": "SO100",
            "task_description": "Safe default observation",
            "current_step": self.current_step
        }
    
    def get_action_space(self):
        """è·å–åŠ¨ä½œç©ºé—´"""
        if self.env is None:
            raise RuntimeError("ç¯å¢ƒæœªåˆå§‹åŒ–")
        return self.env.action_space
    
    def _process_metacognitive_feedback(self, obs: Dict[str, np.ndarray], action: np.ndarray) -> Optional[str]:
        """å¤„ç†å…ƒè®¤çŸ¥åé¦ˆ - ä½¿ç”¨ä¿®å¤çš„å…ƒè®¤çŸ¥æ¨¡å—"""
        if not self.enable_metacognitive:
            return None
        
        try:
            sensor_data = self.robocasa_adapter.convert_observation(
                obs, action, execution_status="normal"
            )
            metacog_output = self.metacog_module.process_sensor_data(sensor_data)
            instruction = self.metacog_to_vla_adapter.convert_to_system2_instruction(metacog_output)
            
            if instruction and metacog_output.directive != DirectiveType.CONTINUE:
                self.metacog_interventions += 1
            
            return instruction
            
        except Exception as e:
            return None

# ==================== æ”¯æŒè§†é¢‘å½•åˆ¶çš„å¢å¼ºè‰è“ç¯å¢ƒ ====================

# class EnhancedStrawberryEnvironmentWithVideo(EnhancedStrawberryEnvironment):
#     """å¢å¼ºè‰è“ç¯å¢ƒ - æ”¯æŒè§†é¢‘å½•åˆ¶"""
    
#     def __init__(self, 
#                  so100_xml_path: str = None,
#                  horizon: int = 100,
#                  enable_gui: bool = False,
#                  robot: str = "Panda",
#                  enable_metacognitive: bool = True,
#                  device: str = "cuda" if torch.cuda.is_available() else "cpu",
#                  # æ–°å¢è§†é¢‘å½•åˆ¶å‚æ•°
#                  enable_video_recording: bool = True,
#                  video_output_dir: str = "./experiment_videos",
#                  video_fps: int = 20):
#         """
#         åˆå§‹åŒ–æ”¯æŒè§†é¢‘å½•åˆ¶çš„å¢å¼ºè‰è“ç¯å¢ƒ
        
#         Args:
#             enable_video_recording: æ˜¯å¦å¯ç”¨è§†é¢‘å½•åˆ¶
#             video_output_dir: è§†é¢‘ä¿å­˜ç›®å½•
#             video_fps: è§†é¢‘å¸§ç‡
#         """
        
#         # åˆå§‹åŒ–çˆ¶ç±»
#         super().__init__(so100_xml_path, horizon, enable_gui, robot, 
#                         enable_metacognitive, device)
        
#         # è§†é¢‘å½•åˆ¶è®¾ç½®
#         self.enable_video_recording = enable_video_recording
#         self.video_recorder = None
        
#         if self.enable_video_recording:
#             self.video_recorder = VideoRecorder(
#                 output_dir=video_output_dir,
#                 fps=video_fps,
#                 video_size=(640, 480)
#             )
#             print(f"ğŸ¥ è§†é¢‘å½•åˆ¶å·²å¯ç”¨")
#         else:
#             print(f"ğŸ“· è§†é¢‘å½•åˆ¶å·²ç¦ç”¨")
    
#     def reset(self) -> Tuple[Dict[str, Any], Dict[str, Any]]:
#         """é‡ç½®ç¯å¢ƒå¹¶å¼€å§‹æ–°çš„è§†é¢‘å½•åˆ¶"""
#         obs, info = super().reset()
        
#         # å¼€å§‹æ–°çš„episodeå½•åˆ¶
#         if self.enable_video_recording and self.video_recorder:
#             episode_id = info.get('episode_id', 0)
#             self.video_recorder.start_episode_recording(episode_id, "enhanced_strawberry")
            
#             # å½•åˆ¶ç¬¬ä¸€å¸§
#             self._record_current_frame(obs, info)
        
#         return obs, info
    
#     def step(self, action: np.ndarray) -> Tuple[Dict[str, Any], float, bool, bool, Dict[str, Any]]:
#         """ç¯å¢ƒæ­¥è¿›å¹¶å½•åˆ¶è§†é¢‘å¸§"""
#         obs, reward, done, truncated, info = super().step(action)
        
#         # å½•åˆ¶å½“å‰å¸§
#         if self.enable_video_recording and self.video_recorder:
#             self._record_current_frame(obs, info)
            
#             # å¦‚æœepisodeç»“æŸï¼Œåœæ­¢å½•åˆ¶
#             if done:
#                 self.video_recorder.stop_episode_recording()
        
#         return obs, reward, done, truncated, info
    
#     def _record_current_frame(self, obs: Dict[str, Any], info: Dict[str, Any]):
#         """å½•åˆ¶å½“å‰å¸§"""
#         try:
#             # æå–RGBå›¾åƒ
#             rgb_image = None
            
#             # å°è¯•ä»è§‚æµ‹ä¸­è·å–å›¾åƒ
#             if "frontview_image" in obs and obs["frontview_image"] is not None:
#                 rgb_image = obs["frontview_image"]
#             elif "robot0_robotview_image" in obs and obs["robot0_robotview_image"] is not None:
#                 rgb_image = obs["robot0_robotview_image"]
            
#             if rgb_image is not None:
#                 # å‡†å¤‡å¸§ä¿¡æ¯
#                 frame_info = {
#                     'step': info.get('step', self.current_step),
#                     'total_reward': info.get('total_reward', self.total_reward),
#                     'task_success': info.get('task_success', False),
#                     'strawberry_task_progress': info.get('strawberry_task_progress', {}),
#                     'metacognitive_feedback': info.get('metacognitive_feedback')
#                 }
                
#                 # æ·»åŠ å¸§åˆ°å½•åˆ¶å™¨
#                 self.video_recorder.add_frame(rgb_image, frame_info)
            
#         except Exception as e:
#             print(f"âš ï¸ å½•åˆ¶å¸§å¤±è´¥: {e}")
    
#     def close(self):
#         """å…³é—­ç¯å¢ƒå¹¶æ¸…ç†è§†é¢‘å½•åˆ¶å™¨"""
#         # åœæ­¢è§†é¢‘å½•åˆ¶
#         if self.enable_video_recording and self.video_recorder:
#             self.video_recorder.cleanup()
        
#         # è°ƒç”¨çˆ¶ç±»å…³é—­æ–¹æ³•
#         super().close()


class EnhancedStrawberryEnvironmentWithVideo(EnhancedStrawberryEnvironment):
    """
    æ”¯æŒè§†é¢‘å½•åˆ¶çš„å¢å¼ºæ¡Œé¢ç¯å¢ƒ (æ­¤ç±»ä»£ç åŸºæœ¬ä¸å˜, ä»…ç»§æ‰¿æ–°çš„çˆ¶ç±»)
    """
    
    def __init__(self, 
                 so100_xml_path: str = None,
                 horizon: int = 100,
                 enable_gui: bool = False,
                 robot: str = "Panda",
                 enable_metacognitive: bool = True,
                 device: str = "cuda" if torch.cuda.is_available() else "cpu",
                 enable_video_recording: bool = True,
                 video_output_dir: str = "./experiment_videos",
                 video_fps: int = 20):
        
        # åˆå§‹åŒ–çˆ¶ç±» (ç°åœ¨æ˜¯æ–°çš„æ¡Œé¢ç¯å¢ƒçˆ¶ç±»)
        super().__init__(so100_xml_path, horizon, enable_gui, robot, 
                        enable_metacognitive, device)
        
        self.enable_video_recording = enable_video_recording
        self.video_recorder = None
        
        if self.enable_video_recording:
            self.video_recorder = VideoRecorder(
                output_dir=video_output_dir,
                fps=video_fps,
                video_size=(640, 480)
            )
            print(f"ğŸ¥ è§†é¢‘å½•åˆ¶å·²å¯ç”¨")
        else:
            print(f"ğŸ“· è§†é¢‘å½•åˆ¶å·²ç¦ç”¨")
    
    def reset(self) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        obs, info = super().reset()
        if self.enable_video_recording and self.video_recorder:
            episode_id = info.get('episode_id', 0)
            self.video_recorder.start_episode_recording(episode_id, "tabletop_strawberry")
            self._record_current_frame(obs, info)
        return obs, info
    
    def step(self, action: np.ndarray) -> Tuple[Dict[str, Any], float, bool, bool, Dict[str, Any]]:
        obs, reward, done, truncated, info = super().step(action)
        if self.enable_video_recording and self.video_recorder:
            self._record_current_frame(obs, info)
            if done:
                self.video_recorder.stop_episode_recording()
        return obs, reward, done, truncated, info
    
    def _record_current_frame(self, obs: Dict[str, Any], info: Dict[str, Any]):
        try:
            rgb_image = obs.get("frontview_image")
            if rgb_image is not None:
                frame_info = {
                    'step': info.get('step', self.current_step),
                    'total_reward': info.get('total_reward', self.total_reward),
                    'task_success': info.get('task_success', False),
                    'strawberry_task_progress': info.get('strawberry_task_progress', {}),
                    'metacognitive_feedback': info.get('metacognitive_feedback')
                }
                self.video_recorder.add_frame(rgb_image, frame_info)
        except Exception as e:
            print(f"âš ï¸ å½•åˆ¶å¸§å¤±è´¥: {e}")
    
    def close(self):
        if self.enable_video_recording and self.video_recorder:
            self.video_recorder.cleanup()
        super().close()

# ==================== å¢å¼ºGR00Tå®¢æˆ·ç«¯ ====================

class EnhancedGR00TClient:
    """å¢å¼ºGR00Tå®¢æˆ·ç«¯ - æ”¯æŒå…ƒè®¤çŸ¥åé¦ˆ"""
    
    def __init__(self, host: str = "localhost", port: int = 5555):
        self.host = host
        self.port = port
        self.client = None
        self.is_connected = False
        self.action_processor = SimpleActionProcessor()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_calls = 0
        self.total_successes = 0
        self.total_time = 0.0
    
    def connect(self) -> bool:
        """è¿æ¥åˆ°GR00TæœåŠ¡"""
        if not GROOT_CLIENT_AVAILABLE:
            return False
        
        try:
            print(f"ğŸ”— è¿æ¥åˆ°GR00T: {self.host}:{self.port}")
            
            self.client = RobotInferenceClient(host=self.host, port=self.port)
            
            # éªŒè¯è¿æ¥
            modality_config = self.client.get_modality_config()
            print("âœ… GR00Tè¿æ¥æˆåŠŸï¼")
            
            # è¿æ¥æµ‹è¯•
            test_obs = self._create_test_observation()
            test_result = self.client.get_action(test_obs)
            
            if test_result is not None:
                print("âœ… GR00Tæµ‹è¯•æˆåŠŸï¼")
                self.is_connected = True
                return True
            else:
                print("âŒ GR00Tæµ‹è¯•å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ GR00Tè¿æ¥å¤±è´¥: {e}")
            return False
    
    def predict_action(self, observation: Dict[str, np.ndarray], 
                      task_description: str = None,
                      metacognitive_feedback: str = None) -> Optional[np.ndarray]:
        """é¢„æµ‹åŠ¨ä½œ - æ”¯æŒå…ƒè®¤çŸ¥åé¦ˆ"""
        if not self.is_connected:
            return None
        
        self.total_calls += 1
        start_time = time.time()
        
        try:
            # è½¬æ¢è§‚æµ‹æ ¼å¼
            groot_obs = self._convert_observation(observation, task_description, metacognitive_feedback)
            
            # è·å–åŠ¨ä½œ
            groot_action = self.client.get_action(groot_obs)
            
            api_time = time.time() - start_time
            self.total_time += api_time
            
            if groot_action is not None:
                self.total_successes += 1
                # ä½¿ç”¨ç®€åŒ–çš„åŠ¨ä½œå¤„ç†å™¨
                so100_action = self.action_processor.process_groot_action(groot_action)
                return so100_action
            else:
                return None
                
        except Exception as e:
            api_time = time.time() - start_time
            self.total_time += api_time
            if self.total_calls % 20 == 0:
                print(f"âš ï¸ é¢„æµ‹é”™è¯¯: {e}")
            return None
    
    def _convert_observation(self, obs: Dict[str, np.ndarray], 
                           task_description: str = None,
                           metacognitive_feedback: str = None) -> Dict[str, Any]:
        """è½¬æ¢è§‚æµ‹æ ¼å¼"""
        try:
            groot_obs = {}
            
            # è§†é¢‘æ•°æ®
            if "frontview_image" in obs and obs["frontview_image"] is not None:
                img = obs["frontview_image"]
                if img.shape[:2] != (480, 640):
                    img = cv2.resize(img, (640, 480))
                groot_obs["video.webcam"] = img[np.newaxis, :, :, :].astype(np.uint8)
            else:
                groot_obs["video.webcam"] = np.zeros((1, 480, 640, 3), dtype=np.uint8)
            
            # æœºå™¨äººçŠ¶æ€
            if "robot0_joint_pos" in obs and obs["robot0_joint_pos"] is not None:
                joint_pos = obs["robot0_joint_pos"][:5]  # ä½¿ç”¨å‰5ä¸ªå…³èŠ‚
                joint_pos = np.clip(joint_pos, -2.0, 2.0)
                groot_obs["state.single_arm"] = joint_pos[np.newaxis, :].astype(np.float32)
            else:
                groot_obs["state.single_arm"] = np.zeros((1, 5), dtype=np.float32)
            
            # å¤¹çˆªçŠ¶æ€
            if "robot0_gripper_qpos" in obs and obs["robot0_gripper_qpos"] is not None:
                gripper_pos = obs["robot0_gripper_qpos"][:1]
                groot_obs["state.gripper"] = gripper_pos[np.newaxis, :].astype(np.float32)
            else:
                groot_obs["state.gripper"] = np.zeros((1, 1), dtype=np.float32)
            
            # ä»»åŠ¡æè¿° - é›†æˆå…ƒè®¤çŸ¥åé¦ˆ
            desc_parts = []
            if task_description:
                desc_parts.append(task_description)
            if metacognitive_feedback:
                desc_parts.append(f"å»ºè®®: {metacognitive_feedback}")
            
            if desc_parts:
                full_description = ". ".join(desc_parts)
            else:
                full_description = obs.get("task_description", "Pick strawberries and place them carefully")
            
            groot_obs["annotation.human.task_description"] = [full_description]
            
            return groot_obs
            
        except Exception as e:
            print(f"âš ï¸ è§‚æµ‹è½¬æ¢é”™è¯¯: {e}")
            return self._create_test_observation()
    
    def _create_test_observation(self) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•è§‚æµ‹"""
        return {
            "video.webcam": np.zeros((1, 480, 640, 3), dtype=np.uint8),
            "state.single_arm": np.zeros((1, 5), dtype=np.float32),
            "state.gripper": np.zeros((1, 1), dtype=np.float32),
            "annotation.human.task_description": ["Pick strawberries and place them carefully"]
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "calls": self.total_calls,
            "successes": self.total_successes,
            "success_rate": self.total_successes / self.total_calls if self.total_calls > 0 else 0,
            "avg_time": self.total_time / self.total_calls if self.total_calls > 0 else 0,
        }

# ==================== ä¸»å®éªŒç±» ====================

@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½®"""
    # è¿æ¥è®¾ç½®
    host: str = "localhost"
    port: int = 5555
    
    # å®éªŒè®¾ç½®
    experiment_name: str = "enhanced_strawberry_experiment"
    num_episodes: int = 3
    max_steps_per_episode: int = 80
    
    # ç¯å¢ƒè®¾ç½®
    so100_xml_path: str = None
    
    # æ¨¡å—å¯ç”¨
    enable_metacognitive: bool = True
    
    # è®¾å¤‡è®¾ç½®
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

@dataclass
class EpisodeResult:
    """Episodeç»“æœ"""
    episode_id: int
    task_success: bool
    total_steps: int
    total_time: float
    total_reward: float
    api_calls: int
    api_successes: int
    metacog_interventions: int
    sensor_failures: int
    strawberries_picked: int
    strawberries_on_plate: int

class EnhancedStrawberryExperiment:
    """å¢å¼ºè‰è“å®éªŒ - é›†æˆæ‰€æœ‰åŠŸèƒ½"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.results = []
        
        print(f"ğŸ¯ åˆå§‹åŒ–å¢å¼ºè‰è“å®éªŒ")
        print(f"   å…ƒè®¤çŸ¥: {'å¯ç”¨' if config.enable_metacognitive else 'ç¦ç”¨'}")
        print(f"   è®¾å¤‡: {config.device}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.groot_client = EnhancedGR00TClient(config.host, config.port)
        self.environment = None
        
        # åˆ›å»ºç¯å¢ƒ
        self._create_environment()
    
    def _create_environment(self):
        """åˆ›å»ºå¢å¼ºè‰è“ç¯å¢ƒ"""
        print(f"ğŸ—ï¸ åˆ›å»ºå¢å¼ºè‰è“ç¯å¢ƒ")
        
        self.environment = EnhancedStrawberryEnvironment(
            so100_xml_path=self.config.so100_xml_path,
            horizon=self.config.max_steps_per_episode * 2,
            enable_gui=False,  # é¿å…å´©æºƒ
            robot="Panda",
            enable_metacognitive=self.config.enable_metacognitive,
            device=self.config.device
        )
    
    def run_experiment(self) -> bool:
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print(f"\nğŸ¯ å¼€å§‹å¢å¼ºè‰è“å®éªŒ")
        print(f"ä»»åŠ¡: è‰è“æ‹£é€‰å’Œæ”¾ç½®")
        print("=" * 70)
        
        # è¿æ¥GR00T
        if not self.groot_client.connect():
            print("âŒ æ— æ³•è¿æ¥åˆ°GR00TæœåŠ¡")
            return False
        
        try:
            # è¿è¡Œepisodes
            for episode in range(self.config.num_episodes):
                print(f"\nğŸ“Š Episode {episode + 1}/{self.config.num_episodes}")
                result = self._run_episode(episode)
                self.results.append(result)
                self._print_episode_summary(result)
            
            # åˆ†æç»“æœ
            self._analyze_results()
            self._save_results()
            
            return True
            
        finally:
            if hasattr(self.environment, 'close'):
                self.environment.close()
    
    def _run_episode(self, episode_id: int) -> EpisodeResult:
        """è¿è¡Œå•ä¸ªepisode"""
        start_time = time.time()
        
        result = EpisodeResult(
            episode_id=episode_id,
            task_success=False,
            total_steps=0,
            total_time=0.0,
            total_reward=0.0,
            api_calls=0,
            api_successes=0,
            metacog_interventions=0,
            sensor_failures=0,
            strawberries_picked=0,
            strawberries_on_plate=0
        )
        
        try:
            obs, info = self.environment.reset()
            # æ·»åŠ episode_idåˆ°infoä¸­
            info['episode_id'] = episode_id
            
            done = False
            step_count = 0
            
            print(f"     æ‰§è¡Œä¸­: ", end="", flush=True)
            
            while not done and step_count < self.config.max_steps_per_episode:
                # è·å–ä»»åŠ¡æè¿°å’Œå…ƒè®¤çŸ¥åé¦ˆ
                task_description = obs.get("task_description", "Pick strawberries and place them carefully")
                metacognitive_feedback = info.get("metacognitive_feedback") if 'info' in locals() else None
                
                # è·å–GR00TåŠ¨ä½œ
                action = self.groot_client.predict_action(
                    obs, task_description, metacognitive_feedback
                )
                
                result.api_calls += 1
                
                if action is not None:
                    result.api_successes += 1
                    print(".", end="", flush=True)
                else:
                    # ä½¿ç”¨é›¶åŠ¨ä½œä½œä¸ºå›é€€
                    action = np.zeros(6)
                    print("x", end="", flush=True)
                
                # ç¯å¢ƒæ­¥è¿›
                next_obs, reward, done, _, info = self.environment.step(action)
                
                result.total_reward += reward
                obs = next_obs
                step_count += 1
                
                # è®°å½•ç»Ÿè®¡
                result.metacog_interventions = info.get("metacog_interventions", 0)
                result.sensor_failures = info.get("sensor_failures", 0)
                
                # è®°å½•è‰è“è¿›åº¦
                progress = info.get("strawberry_task_progress", {})
                result.strawberries_picked = progress.get("strawberries_picked", 0)
                result.strawberries_on_plate = progress.get("strawberries_on_plate", 0)
                
                # æ£€æŸ¥ä»»åŠ¡å®Œæˆ
                if info.get("task_success", False):
                    result.task_success = True
                    done = True
                    print("!", end="", flush=True)
                
                # è¿›åº¦æ˜¾ç¤º
                if step_count % 15 == 0:
                    success_rate = result.api_successes / result.api_calls if result.api_calls > 0 else 0
                    print(f"|{success_rate:.0%}", end="", flush=True)
                
                # å…ƒè®¤çŸ¥å¹²é¢„æ˜¾ç¤º
                if result.metacog_interventions > 0 and step_count % 10 == 0:
                    print("M", end="", flush=True)
            
            result.total_steps = step_count
            result.total_time = time.time() - start_time
            
            print()
            
        except Exception as e:
            result.total_time = time.time() - start_time
            print(f" å¼‚å¸¸: {e}")
        
        return result
    
    def _print_episode_summary(self, result: EpisodeResult):
        """æ‰“å°episodeæ‘˜è¦"""
        status = "âœ… æˆåŠŸ" if result.task_success else "âŒ å¤±è´¥"
        api_success_rate = result.api_successes / result.api_calls if result.api_calls > 0 else 0
        
        print(f"   ç»“æœ: {status}")
        print(f"   æ‰§è¡Œ: {result.total_steps} æ­¥, {result.total_time:.1f}s")
        print(f"   å¥–åŠ±: {result.total_reward:.2f}")
        print(f"   API: {result.api_successes}/{result.api_calls} æˆåŠŸ ({api_success_rate:.1%})")
        print(f"   è‰è“: æ‹£é€‰={result.strawberries_picked}/3, æ”¾ç½®={result.strawberries_on_plate}/3")
        
        if result.metacog_interventions > 0:
            print(f"   å…ƒè®¤çŸ¥: {result.metacog_interventions} æ¬¡å¹²é¢„")
        if result.sensor_failures > 0:
            print(f"   ä¼ æ„Ÿå™¨: {result.sensor_failures} æ¬¡å¤±è´¥")
    
    def _analyze_results(self):
        """åˆ†æå®éªŒç»“æœ"""
        print(f"\nğŸ“Š å®éªŒç»“æœåˆ†æ")
        print("=" * 70)
        
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœæ•°æ®")
            return
        
        # æ€»ä½“ç»Ÿè®¡
        total_episodes = len(self.results)
        successful_episodes = sum(1 for r in self.results if r.task_success)
        success_rate = successful_episodes / total_episodes
        
        total_api_calls = sum(r.api_calls for r in self.results)
        total_api_successes = sum(r.api_successes for r in self.results)
        api_success_rate = total_api_successes / total_api_calls if total_api_calls > 0 else 0
        
        avg_reward = np.mean([r.total_reward for r in self.results])
        avg_steps = np.mean([r.total_steps for r in self.results])
        
        # è‰è“ä»»åŠ¡ç»Ÿè®¡
        total_picked = sum(r.strawberries_picked for r in self.results)
        total_placed = sum(r.strawberries_on_plate for r in self.results)
        avg_picked = total_picked / total_episodes
        avg_placed = total_placed / total_episodes
        
        print(f"ğŸ” æ€»ä½“è¡¨ç°:")
        print(f"   ä»»åŠ¡æˆåŠŸç‡: {success_rate:.1%} ({successful_episodes}/{total_episodes})")
        print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"   å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
        print(f"   APIæˆåŠŸç‡: {api_success_rate:.1%} ({total_api_successes}/{total_api_calls})")
        print(f"   è‰è“æ‹£é€‰: å¹³å‡ {avg_picked:.1f}/3 ä¸ª/episode")
        print(f"   è‰è“æ”¾ç½®: å¹³å‡ {avg_placed:.1f}/3 ä¸ª/episode")
        
        # å…ƒè®¤çŸ¥åˆ†æ
        if self.config.enable_metacognitive:
            total_interventions = sum(r.metacog_interventions for r in self.results)
            avg_interventions = total_interventions / total_episodes
            print(f"   å…ƒè®¤çŸ¥å¹²é¢„: å¹³å‡ {avg_interventions:.1f} æ¬¡/episode")
        
        # ä¼ æ„Ÿå™¨åˆ†æ
        total_sensor_failures = sum(r.sensor_failures for r in self.results)
        if total_sensor_failures > 0:
            print(f"   ä¼ æ„Ÿå™¨å¤±è´¥: æ€»è®¡ {total_sensor_failures} æ¬¡")
    
    def _save_results(self):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"enhanced_strawberry_experiment_{timestamp}.json"
        
        data = {
            "timestamp": timestamp,
            "experiment_type": "Enhanced_Strawberry_Pickplace",
            "config": asdict(self.config),
            "results": [asdict(r) for r in self.results],
            "summary": {
                "total_episodes": len(self.results),
                "success_rate": sum(1 for r in self.results if r.task_success) / len(self.results) if self.results else 0,
                "avg_reward": np.mean([r.total_reward for r in self.results]) if self.results else 0,
                "total_strawberries_picked": sum(r.strawberries_picked for r in self.results) if self.results else 0,
                "total_strawberries_placed": sum(r.strawberries_on_plate for r in self.results) if self.results else 0,
                "total_metacog_interventions": sum(r.metacog_interventions for r in self.results) if self.results else 0
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, default=str)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {filename}")

# ==================== æ”¯æŒè§†é¢‘å½•åˆ¶çš„å®éªŒç±» ====================

class EnhancedStrawberryExperimentWithVideo(EnhancedStrawberryExperiment):
    """å¢å¼ºè‰è“å®éªŒ - æ”¯æŒè§†é¢‘å½•åˆ¶"""
    
    def __init__(self, config: ExperimentConfig, 
                 enable_video_recording: bool = True,
                 video_output_dir: str = "./experiment_videos"):
        
        self.enable_video_recording = enable_video_recording
        self.video_output_dir = video_output_dir
        self.episode_videos = []
        
        # ä½¿ç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œä½†ä¸åˆ›å»ºç¯å¢ƒ
        self.config = config
        self.results = []
        
        print(f"ğŸ¯ åˆå§‹åŒ–æ”¯æŒè§†é¢‘å½•åˆ¶çš„å¢å¼ºè‰è“å®éªŒ")
        print(f"   è§†é¢‘å½•åˆ¶: {'å¯ç”¨' if enable_video_recording else 'ç¦ç”¨'}")
        print(f"   è§†é¢‘ç›®å½•: {video_output_dir}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.groot_client = EnhancedGR00TClient(config.host, config.port)
        self.environment = None
        
        # åˆ›å»ºæ”¯æŒè§†é¢‘å½•åˆ¶çš„ç¯å¢ƒ
        self._create_environment_with_video()
    
    def _create_environment_with_video(self):
        """åˆ›å»ºæ”¯æŒè§†é¢‘å½•åˆ¶çš„å¢å¼ºè‰è“ç¯å¢ƒ"""
        print(f"ğŸ—ï¸ åˆ›å»ºæ”¯æŒè§†é¢‘å½•åˆ¶çš„å¢å¼ºè‰è“ç¯å¢ƒ")
        
        self.environment = EnhancedStrawberryEnvironmentWithVideo(
            so100_xml_path=self.config.so100_xml_path,
            horizon=self.config.max_steps_per_episode * 2,
            enable_gui=False,
            robot="Panda",
            enable_metacognitive=self.config.enable_metacognitive,
            device=self.config.device,
            # è§†é¢‘å½•åˆ¶å‚æ•°
            enable_video_recording=self.enable_video_recording,
            video_output_dir=self.video_output_dir,
            video_fps=20
        )
    
    def _run_episode(self, episode_id: int) -> EpisodeResult:
        """è¿è¡Œå•ä¸ªepisodeå¹¶å½•åˆ¶è§†é¢‘"""
        # åœ¨infoä¸­æ·»åŠ episode_idä»¥ä¾¿è§†é¢‘å½•åˆ¶å™¨ä½¿ç”¨
        result = super()._run_episode(episode_id)
        
        # è®°å½•è§†é¢‘è·¯å¾„ï¼ˆå¦‚æœå¯ç”¨äº†å½•åˆ¶ï¼‰
        if self.enable_video_recording and hasattr(self.environment, 'video_recorder'):
            video_recorder = self.environment.video_recorder
            if hasattr(video_recorder, 'video_path') and video_recorder.video_path:
                self.episode_videos.append(str(video_recorder.video_path))
        
        return result
    
    def run_experiment(self) -> bool:
        """è¿è¡Œå®Œæ•´å®éªŒå¹¶ç”Ÿæˆè§†é¢‘æ€»ç»“"""
        success = super().run_experiment()
        
        if success and self.enable_video_recording and self.episode_videos:
            # åˆ›å»ºå®éªŒæ€»ç»“è§†é¢‘
            print(f"\nğŸï¸ åˆ›å»ºå®éªŒæ€»ç»“è§†é¢‘...")
            
            if hasattr(self.environment, 'video_recorder'):
                summary_video = self.environment.video_recorder.create_summary_video(
                    self.episode_videos, 
                    f"enhanced_strawberry_experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                )
                
                if summary_video:
                    print(f"âœ… å®éªŒæ€»ç»“è§†é¢‘å·²åˆ›å»º: {summary_video}")
        
        return success

# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•° - å¢å¼ºè‰è“å®éªŒ"""
    print("ğŸ¯ å¢å¼ºè‰è“ç¯å¢ƒé›†æˆå®éªŒ")
    print("é˜¶æ®µ1: è®©ç³»ç»Ÿè·‘èµ·æ¥ - åŸºäºå¯å·¥ä½œçš„StableStrawberryEnvironment")
    print("=" * 70)
    
    # æ£€æŸ¥ä¾èµ–
    print(f"ğŸ“‹ ä¾èµ–æ£€æŸ¥:")
    print(f"   RoboSuite: {'âœ…' if ROBOSUITE_AVAILABLE else 'âŒ'}")
    print(f"   RoboCasa: {'âœ…' if ROBOCASA_AVAILABLE else 'âŒ'}")
    print(f"   GR00T Client: {'âœ…' if GROOT_CLIENT_AVAILABLE else 'âŒ'}")
    print(f"   å…ƒè®¤çŸ¥æ¨¡å—: {'âœ…' if METACOG_AVAILABLE else 'âŒ'}")
    
    if not ROBOSUITE_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼šéœ€è¦RoboSuite")
        return
    
    # é…ç½®å®éªŒ
    so100_xml_path = "/root/autodl-tmp/gr00t/SO-ARM100/Simulation/URDF_SO100/SO_5DOF_ARM100_05d.SLDASM/urdf/SO_5DOF_ARM100_05d.SLDASM.xml"
    
    config = ExperimentConfig(
        host="localhost",
        port=5555,
        experiment_name="enhanced_strawberry_integration",
        num_episodes=3,
        max_steps_per_episode=80,
        so100_xml_path=so100_xml_path,
        enable_metacognitive=METACOG_AVAILABLE
    )
    
    print(f"\nğŸ› ï¸ å®éªŒé…ç½®:")
    print(f"   ç¯å¢ƒ: å¢å¼ºè‰è“ç¯å¢ƒ (åŸºäºStableStrawberryEnvironment)")
    print(f"   ä»»åŠ¡: è‰è“æ‹£é€‰å’Œæ”¾ç½®")
    print(f"   Episodes: {config.num_episodes}")
    print(f"   æœ€å¤§æ­¥æ•°: {config.max_steps_per_episode}")
    print(f"   å…ƒè®¤çŸ¥æ¨¡å—: {'å¯ç”¨' if config.enable_metacognitive else 'ç¦ç”¨'}")
    
    # è¿è¡Œå®éªŒ
    print(f"\nğŸš€ å¼€å§‹å®éªŒ")
    print("-" * 50)
    
    experiment = EnhancedStrawberryExperiment(config)
    
    try:
        success = experiment.run_experiment()
        
        if success:
            print(f"\nğŸ‰ é˜¶æ®µ1ä»»åŠ¡å®Œæˆï¼")
            print(f"âœ… 1.1 é›†æˆå¢å¼ºè‰è“ç¯å¢ƒ (åŸºäºå¯å·¥ä½œçš„StableStrawberryEnvironment)")
            print(f"âœ… 1.2 å®ç°ä»»åŠ¡å›ºå®šè¾“å…¥ (è‰è“æ‹£é€‰å’Œæ”¾ç½®)")
            print(f"âœ… 1.3 ä¿®å¤ä¼ æ„Ÿå™¨æ•°æ®è½¬æ¢ (ä½¿ç”¨çœŸå®æ•°æ®)")
            print(f"âœ… ç¯å¢ƒreset/stepåŠŸèƒ½æ­£å¸¸")
            print(f"âœ… GR00Tå®¢æˆ·ç«¯è°ƒç”¨æ­£å¸¸")
            if METACOG_AVAILABLE:
                print(f"âœ… å…ƒè®¤çŸ¥æ¨¡å—é›†æˆæ­£å¸¸")
            
            print(f"\nğŸ“ˆ è‰è“æ‹£é€‰ä»»åŠ¡ç³»ç»Ÿå·²æˆåŠŸè·‘èµ·æ¥ï¼")
            print(f"ğŸ’¡ å…³é”®æ”¹è¿›:")
            print(f"   - åŸºäºå¯å·¥ä½œçš„StableStrawberryEnvironment")
            print(f"   - ä½¿ç”¨çœŸå®ä¼ æ„Ÿå™¨æ•°æ®æ›¿ä»£éšæœºæ•°æ®")
            print(f"   - é›†æˆå®Œæ•´å…ƒè®¤çŸ¥åé¦ˆé“¾è·¯")
            print(f"   - é¿å…Segmentation Faultçš„ç¨³å®šè®¾è®¡")
        else:
            print(f"\nâŒ å®éªŒå¤±è´¥")
    
    except KeyboardInterrupt:
        print(f"\nâš ï¸ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®éªŒå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        if hasattr(experiment, 'environment') and hasattr(experiment.environment, 'close'):
            experiment.environment.close()

def main_with_video():
    """æ”¯æŒè§†é¢‘å½•åˆ¶çš„ä¸»å‡½æ•°"""
    print("ğŸ¯ å¢å¼ºè‰è“ç¯å¢ƒé›†æˆå®éªŒ - æ”¯æŒè§†é¢‘å½•åˆ¶")
    print("=" * 70)
    
    # æ£€æŸ¥ä¾èµ–
    print(f"ğŸ“‹ ä¾èµ–æ£€æŸ¥:")
    print(f"   RoboSuite: {'âœ…' if ROBOSUITE_AVAILABLE else 'âŒ'}")
    print(f"   RoboCasa: {'âœ…' if ROBOCASA_AVAILABLE else 'âŒ'}")
    print(f"   GR00T Client: {'âœ…' if GROOT_CLIENT_AVAILABLE else 'âŒ'}")
    print(f"   å…ƒè®¤çŸ¥æ¨¡å—: {'âœ…' if METACOG_AVAILABLE else 'âŒ'}")
    print(f"   OpenCV: âœ…")  # OpenCVåº”è¯¥å·²ç»å¯ç”¨
    
    if not ROBOSUITE_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼šéœ€è¦RoboSuite")
        return
    
    # é…ç½®å®éªŒï¼ˆä¸åŸæ¥ç›¸åŒï¼‰
    so100_xml_path = "/root/autodl-tmp/gr00t/SO-ARM100/Simulation/URDF_SO100/SO_5DOF_ARM100_05d.SLDASM/urdf/SO_5DOF_ARM100_05d.SLDASM.xml"
    
    config = ExperimentConfig(
        host="localhost",
        port=5555,
        experiment_name="enhanced_strawberry_integration_with_video",
        num_episodes=3,
        max_steps_per_episode=80,
        so100_xml_path=so100_xml_path,
        enable_metacognitive=METACOG_AVAILABLE
    )
    
    print(f"\nğŸ› ï¸ å®éªŒé…ç½®:")
    print(f"   ç¯å¢ƒ: å¢å¼ºè‰è“ç¯å¢ƒ + è§†é¢‘å½•åˆ¶")
    print(f"   ä»»åŠ¡: è‰è“æ‹£é€‰å’Œæ”¾ç½®")
    print(f"   Episodes: {config.num_episodes}")
    print(f"   è§†é¢‘ä¿å­˜: ./strawberry_experiment_videos/")
    
    # è¿è¡Œæ”¯æŒè§†é¢‘å½•åˆ¶çš„å®éªŒ
    experiment = EnhancedStrawberryExperimentWithVideo(
        config, 
        enable_video_recording=True,
        video_output_dir="./strawberry_experiment_videos"
    )
    
    try:
        success = experiment.run_experiment()
        
        if success:
            print(f"\nğŸ‰ å®éªŒå®Œæˆï¼è§†é¢‘å·²ä¿å­˜")
            print(f"ğŸ“ è§†é¢‘æ–‡ä»¶ä¿å­˜åœ¨: ./strawberry_experiment_videos/")
            print(f"   - æ¯ä¸ªepisodeçš„å•ç‹¬è§†é¢‘")
            print(f"   - å®Œæ•´å®éªŒçš„æ€»ç»“è§†é¢‘")
            print(f"ğŸ¥ å¯ä»¥ä½¿ç”¨ä»»ä½•è§†é¢‘æ’­æ”¾å™¨æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹")
            
            # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
            video_dir = Path("./strawberry_experiment_videos")
            if video_dir.exists():
                videos = list(video_dir.glob("*.mp4"))
                if videos:
                    print(f"\nğŸ“¹ ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶:")
                    for video in sorted(videos):
                        file_size = video.stat().st_size / (1024 * 1024)
                        print(f"   - {video.name} ({file_size:.1f}MB)")
        
    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        if hasattr(experiment, 'environment'):
            experiment.environment.close()

if __name__ == "__main__":
    # é€‰æ‹©æ˜¯å¦å¯ç”¨è§†é¢‘å½•åˆ¶
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--video":
        main_with_video()
    else:
        print("ä½¿ç”¨ --video å‚æ•°å¯ç”¨è§†é¢‘å½•åˆ¶åŠŸèƒ½")
        print("ä¾‹å¦‚: python script.py --video")
        print("æˆ–è€…ç›´æ¥è¿è¡Œä¸å¸¦å‚æ•°ç‰ˆæœ¬ï¼ˆä¸å½•åˆ¶è§†é¢‘ï¼‰")
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦å¯ç”¨è§†é¢‘å½•åˆ¶
        try:
            choice = input("\næ˜¯å¦å¯ç”¨è§†é¢‘å½•åˆ¶ï¼Ÿ(y/n): ").lower().strip()
            if choice in ['y', 'yes', 'æ˜¯']:
                main_with_video()
            else:
                main()
        except KeyboardInterrupt:
            print("\nç¨‹åºå·²é€€å‡º")
        except:
            # å¦‚æœè¾“å…¥æœ‰é—®é¢˜ï¼Œé»˜è®¤è¿è¡Œä¸å¸¦è§†é¢‘çš„ç‰ˆæœ¬
            main()