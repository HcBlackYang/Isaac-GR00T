# #!/usr/bin/env python3
# """
# RoboSuite-GR00Tè‰è“æ‹£é€‰ç¯å¢ƒæ¥å£
# å®Œå…¨å¤ç°è®­ç»ƒæ•°æ®é›†ä¸­çš„ç‰©ä½“ï¼š3ä¸ªè‰è“ + 4ä¸ªç»¿çƒ + 1ä¸ªç›˜å­
# """

# import os
# import sys
# import time
# import json
# import numpy as np
# import torch
# import cv2
# from pathlib import Path
# from typing import Dict, List, Any, Tuple, Optional
# from dataclasses import dataclass
# from datetime import datetime

# # è®¾ç½®ç¯å¢ƒå˜é‡
# os.environ.setdefault('MUJOCO_GL', 'egl')
# os.environ.setdefault('PYOPENGL_PLATFORM', 'egl')

# # å¯¼å…¥æ£€æŸ¥
# try:
#     import robosuite
#     from robosuite.controllers import load_composite_controller_config
#     ROBOSUITE_AVAILABLE = True
#     print("âœ… RoboSuiteå¯ç”¨")
# except ImportError as e:
#     print(f"âŒ RoboSuiteä¸å¯ç”¨: {e}")
#     ROBOSUITE_AVAILABLE = False

# try:
#     from gr00t.eval.robot import RobotInferenceClient
#     GROOT_CLIENT_AVAILABLE = True
#     print("âœ… GR00Tå®¢æˆ·ç«¯å¯ç”¨")
# except ImportError as e:
#     print(f"âŒ GR00Tå®¢æˆ·ç«¯ä¸å¯ç”¨: {e}")
#     GROOT_CLIENT_AVAILABLE = False

# # ==================== è§†é¢‘å½•åˆ¶å™¨ ====================

# class VideoRecorder:
#     """è§†é¢‘å½•åˆ¶å™¨ - è®°å½•æœºå™¨äººæ‰§è¡Œä»»åŠ¡è¿‡ç¨‹"""
    
#     def __init__(self, 
#                  output_dir: str = "./strawberry_experiment_videos",
#                  fps: int = 20,
#                  video_size: Tuple[int, int] = (640, 480),
#                  codec: str = 'mp4v'):
#         """
#         åˆå§‹åŒ–è§†é¢‘å½•åˆ¶å™¨
        
#         Args:
#             output_dir: è§†é¢‘ä¿å­˜ç›®å½•
#             fps: å¸§ç‡
#             video_size: è§†é¢‘å°ºå¯¸ (å®½, é«˜)
#             codec: è§†é¢‘ç¼–ç æ ¼å¼
#         """
#         self.output_dir = Path(output_dir)
#         self.output_dir.mkdir(parents=True, exist_ok=True)
        
#         self.fps = fps
#         self.video_size = video_size
#         self.codec = codec
        
#         # å½•åˆ¶çŠ¶æ€
#         self.is_recording = False
#         self.video_writer = None
#         self.current_episode = 0
#         self.frame_count = 0
        
#         print(f"ğŸ¥ è§†é¢‘å½•åˆ¶å™¨åˆå§‹åŒ–")
#         print(f"   ä¿å­˜ç›®å½•: {self.output_dir}")
#         print(f"   è§†é¢‘å‚æ•°: {video_size[0]}x{video_size[1]} @ {fps}fps")
    
#     def start_episode_recording(self, episode_id: int, experiment_name: str = "strawberry_experiment"):
#         """å¼€å§‹å½•åˆ¶æ–°çš„episode"""
#         if self.is_recording:
#             self.stop_episode_recording()
        
#         self.current_episode = episode_id
#         self.frame_count = 0
        
#         # ç”Ÿæˆè§†é¢‘æ–‡ä»¶å
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         filename = f"{experiment_name}_episode_{episode_id:03d}_{timestamp}.mp4"
#         self.video_path = self.output_dir / filename
        
#         # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
#         fourcc = cv2.VideoWriter_fourcc(*self.codec)
#         self.video_writer = cv2.VideoWriter(
#             str(self.video_path),
#             fourcc,
#             self.fps,
#             self.video_size
#         )
        
#         if not self.video_writer.isOpened():
#             print(f"âŒ æ— æ³•åˆ›å»ºè§†é¢‘æ–‡ä»¶: {self.video_path}")
#             return False
        
#         self.is_recording = True
#         print(f"ğŸ¬ å¼€å§‹å½•åˆ¶ Episode {episode_id}: {filename}")
#         return True
    
#     def add_frame(self, image: np.ndarray, step_info: Dict[str, Any] = None):
#         """æ·»åŠ ä¸€å¸§åˆ°å½•åˆ¶"""
#         if not self.is_recording:
#             return
        
#         try:
#             # å¤„ç†å›¾åƒæ ¼å¼
#             processed_image = self._process_image(image, step_info)
            
#             # å†™å…¥è§†é¢‘æ–‡ä»¶
#             if self.video_writer and self.video_writer.isOpened():
#                 self.video_writer.write(processed_image)
#                 self.frame_count += 1
                
#         except Exception as e:
#             print(f"âš ï¸ æ·»åŠ å¸§å¤±è´¥: {e}")
    
#     def _process_image(self, image: np.ndarray, step_info: Dict[str, Any] = None) -> np.ndarray:
#         """å¤„ç†å›¾åƒæ ¼å¼å¹¶æ·»åŠ ä¿¡æ¯å åŠ """
#         try:
#             # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®
#             if image is None:
#                 image = np.zeros((*self.video_size[::-1], 3), dtype=np.uint8)
            
#             # è½¬æ¢æ•°æ®ç±»å‹
#             if image.dtype != np.uint8:
#                 if image.max() <= 1.0:
#                     image = (image * 255).astype(np.uint8)
#                 else:
#                     image = image.astype(np.uint8)
            
#             # è°ƒæ•´å°ºå¯¸
#             if image.shape[:2] != self.video_size[::-1]:
#                 image = cv2.resize(image, self.video_size)
            
#             # ç¡®ä¿æ˜¯3é€šé“
#             if len(image.shape) == 2:
#                 image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
#             elif len(image.shape) == 3 and image.shape[2] == 4:
#                 image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)
            
#             # æ·»åŠ ä¿¡æ¯å åŠ 
#             if step_info:
#                 image = self._add_info_overlay(image, step_info)
            
#             return image
            
#         except Exception as e:
#             print(f"âš ï¸ å›¾åƒå¤„ç†å¤±è´¥: {e}")
#             return np.zeros((*self.video_size[::-1], 3), dtype=np.uint8)
    
#     def _add_info_overlay(self, image: np.ndarray, step_info: Dict[str, Any]) -> np.ndarray:
#         """åœ¨å›¾åƒä¸Šæ·»åŠ ä¿¡æ¯å åŠ """
#         try:
#             overlay_image = image.copy()
            
#             # è®¾ç½®å­—ä½“å‚æ•°
#             font = cv2.FONT_HERSHEY_SIMPLEX
#             font_scale = 0.6
#             color = (0, 255, 0)  # ç»¿è‰²
#             thickness = 2
            
#             # æ·»åŠ åŸºæœ¬ä¿¡æ¯
#             y_offset = 30
            
#             # Episodeå’ŒStepä¿¡æ¯
#             if 'step' in step_info:
#                 text = f"Episode: {self.current_episode} | Step: {step_info['step']}"
#                 cv2.putText(overlay_image, text, (10, y_offset), font, font_scale, color, thickness)
#                 y_offset += 25
            
#             # ä»»åŠ¡è¿›åº¦
#             if 'task_progress' in step_info:
#                 progress = step_info['task_progress']
#                 strawberries_on_plate = step_info.get('strawberries_on_plate', 0)
#                 text = f"Progress: {progress:.1%} | Strawberries: {strawberries_on_plate}/3"
#                 cv2.putText(overlay_image, text, (10, y_offset), font, font_scale, color, thickness)
#                 y_offset += 25
            
#             # å¥–åŠ±ä¿¡æ¯
#             if 'reward' in step_info:
#                 text = f"Reward: {step_info['reward']:.2f}"
#                 cv2.putText(overlay_image, text, (10, y_offset), font, font_scale, color, thickness)
#                 y_offset += 25
            
#             # ä»»åŠ¡æˆåŠŸæ ‡è®°
#             if step_info.get('task_success', False):
#                 text = "TASK SUCCESS!"
#                 cv2.putText(overlay_image, text, (10, image.shape[0] - 30), 
#                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3)
            
#             return overlay_image
            
#         except Exception as e:
#             print(f"âš ï¸ ä¿¡æ¯å åŠ å¤±è´¥: {e}")
#             return image
    
#     def stop_episode_recording(self):
#         """åœæ­¢å½“å‰episodeçš„å½•åˆ¶"""
#         if not self.is_recording:
#             return
        
#         print(f"ğŸ¬ åœæ­¢å½•åˆ¶ Episode {self.current_episode} ({self.frame_count} å¸§)")
        
#         self.is_recording = False
        
#         # å…³é—­è§†é¢‘å†™å…¥å™¨
#         if self.video_writer:
#             self.video_writer.release()
#             self.video_writer = None
        
#         if hasattr(self, 'video_path') and self.video_path.exists():
#             file_size = self.video_path.stat().st_size / (1024 * 1024)  # MB
#             print(f"âœ… è§†é¢‘å·²ä¿å­˜: {self.video_path} ({file_size:.1f}MB)")
        
#         self.frame_count = 0
    
#     def cleanup(self):
#         """æ¸…ç†èµ„æº"""
#         if self.is_recording:
#             self.stop_episode_recording()
        
#         print("ğŸ§¹ è§†é¢‘å½•åˆ¶å™¨èµ„æºå·²æ¸…ç†")

# # ==================== é…ç½®å’Œå·¥å…·å‡½æ•° ====================

# @dataclass
# class ExperimentConfig:
#     """å®éªŒé…ç½®"""
#     robot: str = "Panda"
#     robot_xml_path: Optional[str] = None
#     num_episodes: int = 3
#     max_steps_per_episode: int = 200
#     enable_gui: bool = False
#     enable_video_recording: bool = False
#     video_output_dir: str = "./strawberry_experiment_videos"
#     groot_host: str = "localhost"
#     groot_port: int = 5555

# def create_so100_robot_registration():
#     """
#     åˆ›å»ºSO100æœºå™¨äººæ³¨å†Œä¿¡æ¯
#     è¿™ä¸ªå‡½æ•°å±•ç¤ºå¦‚ä½•æ³¨å†Œè‡ªå®šä¹‰æœºå™¨äººåˆ°RoboSuite
#     """
#     print("\nğŸ”§ SO100æœºå™¨äººæ³¨å†Œè¯´æ˜ï¼š")
#     print("=" * 50)
    
#     print("ğŸ“ è¦åœ¨RoboSuiteä¸­ä½¿ç”¨SO100æœºå™¨äººï¼Œéœ€è¦ï¼š")
#     print("   1. å°†æœºå™¨äººXMLæ–‡ä»¶æ”¾åœ¨æ­£ç¡®çš„ç›®å½•")
#     print("   2. åˆ›å»ºæ§åˆ¶å™¨é…ç½®æ–‡ä»¶")
#     print("   3. æ³¨å†Œæœºå™¨äººåˆ°RoboSuiteç³»ç»Ÿ")
    
#     print("\nğŸ› ï¸ å½“å‰å®ç°æ–¹æ¡ˆï¼š")
#     print("   1. å°è¯•ç›´æ¥ä½¿ç”¨SO100 XMLè·¯å¾„")
#     print("   2. å¦‚æœå¤±è´¥ï¼Œå›é€€åˆ°Pandaæœºå™¨äºº")
#     print("   3. ä¿æŒGR00Tæ¥å£å…¼å®¹æ€§")
    
#     return True

# def create_so100_controller_config():
#     """ä¸ºSO100æœºå™¨äººåˆ›å»ºæ§åˆ¶å™¨é…ç½®"""
#     try:
#         # åŸºäºPandaé…ç½®ä¿®æ”¹ä¸º5DOF
#         base_config = load_composite_controller_config(robot="Panda")
        
#         # ä¿®æ”¹è‡‚éƒ¨æ§åˆ¶å™¨é…ç½®ä¸º5DOF
#         so100_config = base_config.copy()
        
#         if "arm" in so100_config:
#             arm_config = so100_config["arm"]
#             # è°ƒæ•´æ§åˆ¶å‚æ•°é€‚é…5DOF
#             if "input_max" in arm_config:
#                 arm_config["input_max"] = 1.0
#             if "input_min" in arm_config:
#                 arm_config["input_min"] = -1.0
#             if "output_max" in arm_config:
#                 arm_config["output_max"] = 0.05  # é™ä½è¾“å‡ºå¹…åº¦
#             if "output_min" in arm_config:
#                 arm_config["output_min"] = -0.05
        
#         print("âœ… SO100æ§åˆ¶å™¨é…ç½®åˆ›å»ºæˆåŠŸ")
#         return so100_config
        
#     except Exception as e:
#         print(f"âš ï¸ SO100æ§åˆ¶å™¨é…ç½®å¤±è´¥: {e}")
#         return load_composite_controller_config(robot="Panda")

# # ==================== è‡ªå®šä¹‰è‰è“æ‹£é€‰ç¯å¢ƒ ====================

# class StrawberryPickPlaceEnvironment:
#     """
#     è‰è“æ‹£é€‰ç¯å¢ƒåŒ…è£…å™¨
#     æ”¯æŒSO100æœºå™¨äºº + è§†é¢‘å½•åˆ¶ + è‡ªå®šä¹‰ç‰©ä½“å’Œä»»åŠ¡é€»è¾‘
#     """
    
#     def __init__(
#         self,
#         robots="Panda",
#         robot_xml_path=None,
#         has_renderer=False,
#         has_offscreen_renderer=True,
#         use_camera_obs=True,
#         camera_names="frontview",
#         camera_heights=480,
#         camera_widths=640,
#         control_freq=20,
#         horizon=1000,
#         ignore_done=True,
#         enable_video_recording=False,
#         video_output_dir="./strawberry_experiment_videos"
#     ):
        
#         # å­˜å‚¨é…ç½®
#         self.robots = robots if isinstance(robots, list) else [robots]
#         self.robot_xml_path = robot_xml_path
#         self.enable_video_recording = enable_video_recording
        
#         # è§†é¢‘å½•åˆ¶å™¨
#         self.video_recorder = None
#         if enable_video_recording:
#             self.video_recorder = VideoRecorder(
#                 output_dir=video_output_dir,
#                 fps=20,
#                 video_size=(camera_widths, camera_heights)
#             )
        
#         # ä»»åŠ¡ç›¸å…³çŠ¶æ€
#         self.strawberry_names = ["strawberry_0", "strawberry_1", "strawberry_2"]
#         self.green_ball_names = ["green_ball_0", "green_ball_1", "green_ball_2", "green_ball_3"]
#         self.plate_name = "plate"
        
#         self.held_object = None
#         self.placed_strawberries = set()
#         self.task_complete = False
#         self.current_step = 0
#         self.max_steps = horizon
        
#         # åˆ›å»ºç‰©ä½“åˆ—è¡¨
#         self.objects = self._create_objects()
        
#         # å°è¯•ä½¿ç”¨SO100æœºå™¨äºº
#         actual_robot = self._setup_robot_configuration(robots, robot_xml_path)
        
#         # åˆ›å»ºåŸºç¡€ç¯å¢ƒ
#         try:
#             print(f"ğŸ”§ åˆ›å»ºåŸºç¡€PickPlaceç¯å¢ƒ...")
#             print(f"   ç›®æ ‡æœºå™¨äºº: {robots}")
#             print(f"   å®é™…ä½¿ç”¨: {actual_robot}")
            
#             # ä½¿ç”¨æ ‡å‡†çš„robosuite.makeæ–¹æ³•
#             self.env = robosuite.make(
#                 "PickPlace",
#                 robots=actual_robot,
#                 controller_configs=self._get_controller_config(actual_robot),
#                 has_renderer=has_renderer,
#                 has_offscreen_renderer=has_offscreen_renderer,
#                 use_camera_obs=use_camera_obs,
#                 camera_names=camera_names,
#                 camera_heights=camera_heights,
#                 camera_widths=camera_widths,
#                 control_freq=control_freq,
#                 horizon=horizon,
#                 ignore_done=ignore_done,
#                 single_object_mode=2,  # å•ç‰©ä½“æ¨¡å¼
#                 object_type="can"      # é»˜è®¤ä½¿ç”¨canç‰©ä½“
#             )
            
#             print(f"âœ… åŸºç¡€ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
            
#             # è·å–ç¯å¢ƒä¿¡æ¯ - ä¿®å¤å±æ€§è®¿é—®
#             try:
#                 # å°è¯•è·å–æ¡Œé¢ä¿¡æ¯
#                 if hasattr(self.env, 'table_full_size'):
#                     self.table_full_size = self.env.table_full_size
#                 else:
#                     self.table_full_size = (1.0, 1.0, 0.05)  # é»˜è®¤æ¡Œé¢å°ºå¯¸
                
#                 if hasattr(self.env, 'table_top_offset'):
#                     self.table_offset = self.env.table_top_offset
#                 elif hasattr(self.env, 'table_offset'):
#                     self.table_offset = self.env.table_offset
#                 else:
#                     # ä½¿ç”¨é»˜è®¤æ¡Œé¢ä½ç½®
#                     self.table_offset = np.array([0.0, 0.0, 0.8])
#                     print(f"âš ï¸ ä½¿ç”¨é»˜è®¤æ¡Œé¢ä½ç½®: {self.table_offset}")
                
#                 print(f"ğŸ“ æ¡Œé¢ä¿¡æ¯: ä½ç½®={self.table_offset}, å°ºå¯¸={self.table_full_size}")
                
#             except Exception as e:
#                 print(f"âš ï¸ è·å–æ¡Œé¢ä¿¡æ¯å¤±è´¥: {e}")
#                 # ä½¿ç”¨é»˜è®¤å€¼
#                 self.table_full_size = (1.0, 1.0, 0.05)
#                 self.table_offset = np.array([0.0, 0.0, 0.8])
            
#             print(f"âœ… åˆ›å»ºäº†è™šæ‹Ÿç‰©ä½“å¸ƒå±€")
#             print(f"   - 3ä¸ªçº¢è‰²è‰è“: {self.strawberry_names}")
#             print(f"   - 4ä¸ªç»¿è‰²å°çƒ: {self.green_ball_names}")
#             print(f"   - 1ä¸ªç™½è‰²ç›˜å­: {self.plate_name}")
#             if enable_video_recording:
#                 print(f"   ğŸ¥ è§†é¢‘å½•åˆ¶: å¯ç”¨")
#             print(f"   è¯´æ˜ï¼šç”±äºRoboSuiteé™åˆ¶ï¼Œä½¿ç”¨é€»è¾‘æ¨¡æ‹Ÿå®ç°å¤šç‰©ä½“åœºæ™¯")
            
#         except Exception as e:
#             print(f"âŒ åŸºç¡€ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
#             raise
        
#         # éªŒè¯ç¯å¢ƒåˆ›å»ºæ˜¯å¦æˆåŠŸ
#         try:
#             print(f"ğŸ” éªŒè¯ç¯å¢ƒ...")
#             test_obs = self.env.reset()
#             print(f"   âœ… ç¯å¢ƒé‡ç½®æˆåŠŸ")
#             print(f"   ğŸ“Š è§‚æµ‹é”®: {list(test_obs.keys())}")
            
#             # æ£€æŸ¥å…³é”®è§‚æµ‹æ•°æ®
#             if any(key in test_obs for key in ["frontview_image", "agentview_image", "image"]):
#                 print(f"   âœ… å›¾åƒæ•°æ®å¯ç”¨")
#             else:
#                 print(f"   âš ï¸ æœªæ‰¾åˆ°å›¾åƒæ•°æ®")
            
#             if any(key in test_obs for key in ["robot0_eef_pos", "eef_pos"]):
#                 print(f"   âœ… æœºå™¨äººçŠ¶æ€å¯ç”¨")
#             else:
#                 print(f"   âš ï¸ æœªæ‰¾åˆ°æœºå™¨äººçŠ¶æ€")
            
#         except Exception as e:
#             print(f"âš ï¸ ç¯å¢ƒéªŒè¯å¤±è´¥: {e}")
#             print(f"   ç»§ç»­ä½¿ç”¨å½“å‰ç¯å¢ƒé…ç½®")
    
#     def _setup_robot_configuration(self, target_robot: str, robot_xml_path: str) -> str:
#         """è®¾ç½®æœºå™¨äººé…ç½®ï¼Œå°è¯•ä½¿ç”¨SO100"""
        
#         if robot_xml_path and os.path.exists(robot_xml_path):
#             print(f"ğŸ“ æ£€æµ‹åˆ°SO100æœºå™¨äººXML: {robot_xml_path}")
            
#             try:
#                 # å°è¯•æ³¨å†ŒSO100æœºå™¨äºº
#                 print(f"ğŸ”§ å°è¯•é…ç½®SO100æœºå™¨äºº...")
                
#                 # ç®€å•çš„SO100æ”¯æŒ - ç›´æ¥å°è¯•ä½¿ç”¨
#                 if target_robot == "SO100":
#                     print(f"   å°è¯•ç›´æ¥ä½¿ç”¨SO100...")
#                     return "SO100"
                
#             except Exception as e:
#                 print(f"âš ï¸ SO100é…ç½®å¤±è´¥: {e}")
        
#         print(f"   å›é€€åˆ°Pandaæœºå™¨äººï¼ˆç¨³å®šå¯é ï¼‰")
#         return "Panda"
    
#     def _get_controller_config(self, robot_name: str):
#         """è·å–æ§åˆ¶å™¨é…ç½®"""
#         try:
#             if robot_name == "SO100":
#                 print(f"ğŸ›ï¸ ä½¿ç”¨SO100æ§åˆ¶å™¨é…ç½®")
#                 return create_so100_controller_config()
#             else:
#                 print(f"ğŸ›ï¸ ä½¿ç”¨{robot_name}æ ‡å‡†æ§åˆ¶å™¨é…ç½®")
#                 return load_composite_controller_config(robot=robot_name)
#         except Exception as e:
#             print(f"âš ï¸ æ§åˆ¶å™¨é…ç½®å¤±è´¥: {e}")
#             return load_composite_controller_config(robot="Panda")
    
#     def _create_objects(self):
#         """åˆ›å»ºè™šæ‹Ÿç‰©ä½“å®šä¹‰ï¼ˆç”¨äºé€»è¾‘æ¨¡æ‹Ÿï¼‰"""
#         objects = []
        
#         # å®šä¹‰è™šæ‹Ÿç‰©ä½“ä¿¡æ¯
#         for i in range(3):
#             objects.append({
#                 "name": f"strawberry_{i}",
#                 "type": "strawberry",
#                 "color": [0.8, 0.2, 0.2],
#                 "size": [0.02, 0.025],
#                 "target": True  # è¿™æ˜¯ä»»åŠ¡ç›®æ ‡ç‰©ä½“
#             })
        
#         for i in range(4):
#             objects.append({
#                 "name": f"green_ball_{i}",
#                 "type": "green_ball", 
#                 "color": [0.3, 0.8, 0.3],
#                 "size": [0.015],
#                 "target": False  # è¿™æ˜¯å¹²æ‰°ç‰©ä½“
#             })
        
#         objects.append({
#             "name": "plate",
#             "type": "plate",
#             "color": [0.95, 0.95, 0.95],
#             "size": [0.12, 0.008],
#             "target": False  # è¿™æ˜¯æ”¾ç½®ç›®æ ‡
#         })
        
#         return objects
    
#     def reset(self):
#         """é‡ç½®ç¯å¢ƒå¹¶å¼€å§‹è§†é¢‘å½•åˆ¶"""
#         # é‡ç½®åŸºç¡€ç¯å¢ƒ
#         obs = self.env.reset()
        
#         # é‡ç½®ä»»åŠ¡çŠ¶æ€
#         self.current_step = 0
#         self.held_object = None
#         self.placed_strawberries.clear()
#         self.task_complete = False
        
#         # è®¾ç½®è™šæ‹Ÿç‰©ä½“ä½ç½®ï¼ˆç”¨äºä»»åŠ¡é€»è¾‘ï¼‰
#         self._setup_virtual_object_positions()
        
#         return self._process_observation(obs)
    
#     def step(self, action):
#         """ç¯å¢ƒæ­¥è¿›å¹¶å½•åˆ¶è§†é¢‘"""
#         # åŸºç¡€ç¯å¢ƒæ­¥è¿›
#         obs, reward, done, info = self.env.step(action)
#         self.current_step += 1
        
#         # è®¡ç®—ä»»åŠ¡å¥–åŠ±
#         task_reward = self._calculate_task_reward(obs, action)
        
#         # æ£€æŸ¥ä»»åŠ¡å®Œæˆ
#         task_success = self._check_task_success()
        
#         # æ›´æ–°doneçŠ¶æ€
#         if task_success:
#             done = True
#         elif self.current_step >= self.max_steps:
#             done = True
        
#         # å¤„ç†è§‚æµ‹
#         processed_obs = self._process_observation(obs)
        
#         # æ›´æ–°info
#         task_info = self.get_task_info()
#         info.update(task_info)
        
#         # è§†é¢‘å½•åˆ¶
#         if self.video_recorder and self.video_recorder.is_recording:
#             step_info = {
#                 'step': self.current_step,
#                 'reward': task_reward,
#                 'task_progress': task_info.get('task_progress', 0.0),
#                 'strawberries_on_plate': task_info.get('strawberries_on_plate', 0),
#                 'task_success': task_success
#             }
            
#             # å½•åˆ¶å½“å‰å¸§
#             if "frontview_image" in processed_obs:
#                 self.video_recorder.add_frame(processed_obs["frontview_image"], step_info)
        
#         return processed_obs, task_reward, done, info
    
#     def start_episode_recording(self, episode_id: int):
#         """å¼€å§‹episodeå½•åˆ¶"""
#         if self.video_recorder:
#             return self.video_recorder.start_episode_recording(episode_id, "strawberry_so100")
#         return False
    
#     def stop_episode_recording(self):
#         """åœæ­¢episodeå½•åˆ¶"""
#         if self.video_recorder:
#             self.video_recorder.stop_episode_recording()
    
#     def _setup_virtual_object_positions(self):
#         """è®¾ç½®è™šæ‹Ÿç‰©ä½“ä½ç½®ï¼ˆåŸºäºæ¡Œé¢åæ ‡ï¼‰"""
#         try:
#             # è·å–æ¡Œé¢ä¸­å¿ƒä½ç½®
#             table_center = self.table_offset
            
#             # è™šæ‹Ÿç›˜å­ä½ç½®ï¼ˆæ¡Œé¢åº•éƒ¨ä¸­å¤®ï¼‰
#             self.virtual_plate_pos = np.array([
#                 table_center[0], 
#                 table_center[1] - 0.25, 
#                 table_center[2] + 0.01  # ç¨å¾®æŠ¬é«˜é¿å…ç©¿é€
#             ])
            
#             # è™šæ‹Ÿè‰è“ä½ç½®ï¼ˆæ¡Œé¢ä¸Šæ–¹ï¼Œåˆ†æ•£åˆ†å¸ƒï¼‰
#             self.virtual_strawberry_positions = [
#                 table_center + np.array([-0.15, 0.1, 0.03]),   # å·¦ä¸Š
#                 table_center + np.array([0.15, 0.15, 0.03]),   # å³ä¸Š  
#                 table_center + np.array([0.0, 0.05, 0.03])     # ä¸­é—´
#             ]
            
#             # è™šæ‹Ÿç»¿çƒä½ç½®ï¼ˆæ¡Œé¢å››å‘¨ï¼‰
#             self.virtual_green_ball_positions = [
#                 table_center + np.array([-0.2, -0.1, 0.03]),   # å·¦
#                 table_center + np.array([0.2, -0.05, 0.03]),   # å³
#                 table_center + np.array([-0.1, 0.25, 0.03]),   # ä¸Š
#                 table_center + np.array([0.1, -0.15, 0.03])    # ä¸‹
#             ]
            
#             print(f"ğŸ“ è™šæ‹Ÿç‰©ä½“ä½ç½®å·²è®¾ç½®ï¼ŒåŸºäºæ¡Œé¢ä¸­å¿ƒ: {table_center}")
#             print(f"   ç›˜å­åŒºåŸŸ: {self.virtual_plate_pos}")
#             print(f"   è‰è“åŒºåŸŸ: æ¡Œé¢ä¸Šæ–¹ï¼ˆ3ä¸ªä½ç½®ï¼‰")
#             print(f"   ç»¿çƒåŒºåŸŸ: æ¡Œé¢å››å‘¨ï¼ˆ4ä¸ªä½ç½®ï¼‰")
            
#         except Exception as e:
#             print(f"âš ï¸ è™šæ‹Ÿç‰©ä½“ä½ç½®è®¾ç½®å¤±è´¥: {e}")
#             # ä½¿ç”¨é»˜è®¤ä½ç½®
#             self.virtual_plate_pos = np.array([0.0, -0.25, 0.81])
#             self.virtual_strawberry_positions = [
#                 np.array([-0.1, 0.1, 0.83]),
#                 np.array([0.1, 0.1, 0.83]),
#                 np.array([0.0, 0.2, 0.83])
#             ]
#             self.virtual_green_ball_positions = [
#                 np.array([-0.2, 0.0, 0.83]),
#                 np.array([0.2, 0.0, 0.83]),
#                 np.array([0.0, -0.1, 0.83]),
#                 np.array([0.0, 0.3, 0.83])
#             ]
#             print(f"   ä½¿ç”¨é»˜è®¤è™šæ‹Ÿä½ç½®")
    
#     def _calculate_task_reward(self, obs, action):
#         """è®¡ç®—è‰è“æ‹£é€‰ä»»åŠ¡å¥–åŠ± - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ›´å®¹æ˜“æˆåŠŸ"""
#         reward = 0.0
        
#         try:
#             # è·å–æœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨ä½ç½®
#             eef_pos = obs.get("robot0_eef_pos", np.array([0.5, 0.0, 0.8]))
#             gripper_qpos = obs.get("robot0_gripper_qpos", np.array([0.0, 0.0]))
#             gripper_openness = gripper_qpos[0] if len(gripper_qpos) > 0 else 0.0
            
#             # æ ‡å‡†åŒ–å¤¹çˆªçŠ¶æ€ (0=å…³é—­, 1=å¼€å¯)
#             gripper_normalized = np.clip(gripper_openness, 0.0, 1.0)
            
#             # åŸºäºè™šæ‹Ÿç‰©ä½“ä½ç½®çš„ä»»åŠ¡é€»è¾‘
#             if self.held_object is None:
#                 # å¯»æ‰¾æœ€è¿‘çš„æœªæ”¾ç½®è‰è“
#                 min_dist = float('inf')
#                 target_strawberry_idx = -1
                
#                 for i, strawberry_pos in enumerate(self.virtual_strawberry_positions):
#                     if i not in self.placed_strawberries:
#                         dist = np.linalg.norm(eef_pos - strawberry_pos)
#                         if dist < min_dist:
#                             min_dist = dist
#                             target_strawberry_idx = i
                
#                 if target_strawberry_idx >= 0:
#                     # å¥–åŠ±æ¥è¿‘ç›®æ ‡è‰è“ - å¢åŠ æƒé‡
#                     approach_reward = 5.0 * (1.0 - np.tanh(3.0 * min_dist))
#                     reward += approach_reward
                    
#                     # æ£€æŸ¥"æŠ“å–"æˆåŠŸ - æ”¾å®½æ¡ä»¶
#                     grab_distance_threshold = 0.08  # å¢åŠ åˆ°8cm
#                     grab_gripper_threshold = 0.3    # æ”¾å®½å¤¹çˆªé˜ˆå€¼
                    
#                     if min_dist < grab_distance_threshold and gripper_normalized < grab_gripper_threshold:
#                         self.held_object = f"strawberry_{target_strawberry_idx}"
#                         reward += 20.0  # å¢åŠ æŠ“å–å¥–åŠ±
#                         print(f"   ğŸ“ æŠ“å–è‰è“ {target_strawberry_idx}! (è·ç¦»: {min_dist:.3f}m)")
            
#             else:
#                 # å·²ç»"æŠ“ç€"è‰è“ï¼Œå¥–åŠ±æ¥è¿‘ç›˜å­
#                 dist_to_plate = np.linalg.norm(eef_pos[:2] - self.virtual_plate_pos[:2])
#                 approach_reward = 5.0 * (1.0 - np.tanh(3.0 * dist_to_plate))
#                 reward += approach_reward
                
#                 # æ£€æŸ¥"æ”¾ç½®"æˆåŠŸ - æ”¾å®½æ¡ä»¶
#                 place_distance_threshold = 0.15  # å¢åŠ åˆ°15cm
#                 place_height_threshold = 0.12    # æ”¾å®½é«˜åº¦é™åˆ¶
#                 place_gripper_threshold = 0.5    # æ”¾å®½å¤¹çˆªå¼€å¯é˜ˆå€¼
                
#                 height_diff = eef_pos[2] - self.virtual_plate_pos[2]
                
#                 if (dist_to_plate < place_distance_threshold and 
#                     height_diff < place_height_threshold and 
#                     gripper_normalized > place_gripper_threshold):
                    
#                     # è§£æheld_objectè·å–è‰è“ç´¢å¼•
#                     strawberry_idx = int(self.held_object.split('_')[-1])
#                     self.placed_strawberries.add(strawberry_idx)
#                     self.held_object = None
#                     reward += 30.0  # å¢åŠ æ”¾ç½®å¥–åŠ±
#                     print(f"   ğŸ½ï¸ æ”¾ç½®è‰è“åˆ°ç›˜å­ä¸Š! ({len(self.placed_strawberries)}/3)")
#                     print(f"      è·ç¦»: {dist_to_plate:.3f}m, é«˜åº¦å·®: {height_diff:.3f}m")
            
#             # ä»»åŠ¡å®Œæˆé¢å¤–å¥–åŠ±
#             if len(self.placed_strawberries) == 3 and not self.task_complete:
#                 reward += 100.0
#                 self.task_complete = True
#                 print("ğŸ‰ æ‰€æœ‰è‰è“éƒ½å·²æ”¾ç½®å®Œæˆ!")
            
#             # æ·»åŠ åŸºäºRGBå›¾åƒçš„è§†è§‰å¥–åŠ±
#             if "frontview_image" in obs:
#                 visual_reward = self._calculate_visual_reward(obs["frontview_image"])
#                 reward += visual_reward
            
#             # æ·»åŠ å¹³æ»‘çš„è¿åŠ¨å¥–åŠ±ï¼Œé¿å…æœºå™¨äººåœæ»
#             action_magnitude = np.linalg.norm(action) if action is not None else 0.0
#             if action_magnitude > 0.01:  # é¼“åŠ±æœ‰æ„ä¹‰çš„è¿åŠ¨
#                 reward += 0.1
            
#             return reward
            
#         except Exception as e:
#             print(f"âš ï¸ å¥–åŠ±è®¡ç®—é”™è¯¯: {e}")
#             return 0.0
    
#     def _calculate_visual_reward(self, rgb_image):
#         """åŸºäºè§†è§‰ä¿¡æ¯è®¡ç®—å¥–åŠ±ï¼ˆæ£€æµ‹çº¢è‰²åŒºåŸŸæ¨¡æ‹Ÿè‰è“è¯†åˆ«ï¼‰"""
#         try:
#             if rgb_image is None or len(rgb_image.shape) != 3:
#                 return 0.0
            
#             # è½¬æ¢ä¸ºHSVè¿›è¡Œçº¢è‰²æ£€æµ‹
#             hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)
            
#             # çº¢è‰²èŒƒå›´ï¼ˆæ¨¡æ‹Ÿè‰è“é¢œè‰²ï¼‰
#             lower_red1 = np.array([0, 50, 50])
#             upper_red1 = np.array([10, 255, 255])
#             lower_red2 = np.array([170, 50, 50])
#             upper_red2 = np.array([180, 255, 255])
            
#             mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
#             mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
#             red_mask = mask1 + mask2
            
#             # è®¡ç®—çº¢è‰²åƒç´ æ¯”ä¾‹
#             red_pixels = np.sum(red_mask > 0)
#             total_pixels = rgb_image.shape[0] * rgb_image.shape[1]
#             red_ratio = red_pixels / total_pixels
            
#             # å¦‚æœæ£€æµ‹åˆ°çº¢è‰²åŒºåŸŸï¼Œç»™äºˆå°é¢è§†è§‰å¥–åŠ±
#             if red_ratio > 0.005:  # è‡³å°‘0.5%çš„çº¢è‰²åƒç´ 
#                 return 0.2 * min(red_ratio * 10, 1.0)
            
#             return 0.0
            
#         except Exception as e:
#             return 0.0
    
#     def _check_task_success(self):
#         """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æˆåŠŸå®Œæˆ"""
#         return len(self.placed_strawberries) == 3
    
#     def _process_observation(self, obs):
#         """å¤„ç†è§‚æµ‹æ•°æ®ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®"""
#         processed = {}
        
#         try:
#             # å›¾åƒæ•°æ® - å°è¯•å¤šç§å¯èƒ½çš„é”®å
#             image_found = False
#             for img_key in ["frontview_image", "agentview_image", "image"]:
#                 if img_key in obs and obs[img_key] is not None:
#                     img = obs[img_key]
#                     # RoboSuiteå›¾åƒå¯èƒ½éœ€è¦ç¿»è½¬
#                     if len(img.shape) == 3:
#                         img = img[::-1]  
#                     processed["frontview_image"] = img.astype(np.uint8)
#                     image_found = True
#                     break
            
#             if not image_found:
#                 processed["frontview_image"] = np.zeros((480, 640, 3), dtype=np.uint8)
#                 print("âš ï¸ æœªæ‰¾åˆ°å›¾åƒæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å›¾åƒ")
            
#             # æœºå™¨äººçŠ¶æ€ - å®‰å…¨åœ°è·å–å„ç§çŠ¶æ€ä¿¡æ¯
#             self._process_robot_state(obs, processed)
            
#             return processed
            
#         except Exception as e:
#             print(f"âš ï¸ è§‚æµ‹å¤„ç†é”™è¯¯: {e}")
#             return self._get_default_observation()
    
#     def _process_robot_state(self, obs, processed):
#         """å®‰å…¨åœ°å¤„ç†æœºå™¨äººçŠ¶æ€æ•°æ®"""
#         try:
#             # å…³èŠ‚ä½ç½®
#             joint_keys = ["robot0_joint_pos", "joint_pos", "qpos"]
#             for key in joint_keys:
#                 if key in obs and obs[key] is not None:
#                     processed["robot0_joint_pos"] = np.array(obs[key], dtype=np.float32)
#                     break
#             else:
#                 processed["robot0_joint_pos"] = np.zeros(7, dtype=np.float32)
            
#             # æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®
#             eef_keys = ["robot0_eef_pos", "eef_pos", "end_effector_pos"]
#             for key in eef_keys:
#                 if key in obs and obs[key] is not None:
#                     processed["robot0_eef_pos"] = np.array(obs[key], dtype=np.float32)
#                     break
#             else:
#                 processed["robot0_eef_pos"] = np.array([0.5, 0.0, 0.8], dtype=np.float32)
            
#             # æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€
#             eef_quat_keys = ["robot0_eef_quat", "eef_quat", "end_effector_quat"]
#             for key in eef_quat_keys:
#                 if key in obs and obs[key] is not None:
#                     processed["robot0_eef_quat"] = np.array(obs[key], dtype=np.float32)
#                     break
#             else:
#                 processed["robot0_eef_quat"] = np.array([0, 0, 0, 1], dtype=np.float32)
            
#             # å¤¹çˆªçŠ¶æ€
#             gripper_keys = ["robot0_gripper_qpos", "gripper_qpos", "gripper_pos"]
#             for key in gripper_keys:
#                 if key in obs and obs[key] is not None:
#                     processed["robot0_gripper_qpos"] = np.array(obs[key], dtype=np.float32)
#                     break
#             else:
#                 processed["robot0_gripper_qpos"] = np.zeros(2, dtype=np.float32)
            
#         except Exception as e:
#             print(f"âš ï¸ æœºå™¨äººçŠ¶æ€å¤„ç†é”™è¯¯: {e}")
#             # æä¾›é»˜è®¤å€¼
#             processed.update({
#                 "robot0_joint_pos": np.zeros(7, dtype=np.float32),
#                 "robot0_eef_pos": np.array([0.5, 0.0, 0.8], dtype=np.float32),
#                 "robot0_eef_quat": np.array([0, 0, 0, 1], dtype=np.float32),
#                 "robot0_gripper_qpos": np.zeros(2, dtype=np.float32)
#             })
    
#     def get_task_info(self):
#         """è·å–ä»»åŠ¡ä¿¡æ¯"""
#         return {
#             "strawberries_picked": 1 if self.held_object and "strawberry" in self.held_object else 0,
#             "strawberries_on_plate": len(self.placed_strawberries),
#             "total_strawberries": 3,
#             "task_success": self.task_complete,
#             "task_progress": len(self.placed_strawberries) / 3.0,
#             "current_step": self.current_step,
#             "max_steps": self.max_steps,
#             "held_object": self.held_object
#         }
    
#     def _get_default_observation(self):
#         """è·å–é»˜è®¤è§‚æµ‹"""
#         return {
#             "frontview_image": np.zeros((480, 640, 3), dtype=np.uint8),
#             "robot0_joint_pos": np.zeros(7, dtype=np.float32),
#             "robot0_eef_pos": np.array([0.5, 0.0, 0.8], dtype=np.float32),
#             "robot0_eef_quat": np.array([0, 0, 0, 1], dtype=np.float32),
#             "robot0_gripper_qpos": np.zeros(2, dtype=np.float32)
#         }
    
#     def close(self):
#         """å…³é—­ç¯å¢ƒå’Œæ¸…ç†èµ„æº"""
#         # åœæ­¢è§†é¢‘å½•åˆ¶
#         if self.video_recorder:
#             self.video_recorder.cleanup()
        
#         # å…³é—­åŸºç¡€ç¯å¢ƒ
#         if hasattr(self, 'env') and self.env is not None:
#             try:
#                 self.env.close()
#                 print("ğŸ”’ è‰è“æ‹£é€‰ç¯å¢ƒå·²å…³é—­")
#             except:
#                 pass

# # ==================== æ•°æ®é€‚é…å™¨ ====================

# class RoboSuiteGR00TAdapter:
#     """RoboSuiteä¸GR00Tä¹‹é—´çš„æ•°æ®é€‚é…å™¨"""
    
#     def __init__(self):
#         self.processed_observations = 0
#         self.processed_actions = 0
        
#     def robosuite_to_groot_obs(self, obs: Dict[str, np.ndarray], 
#                               task_description: str = "Pick red strawberries and place them on the white plate") -> Dict[str, Any]:
#         """å°†RoboSuiteè§‚æµ‹è½¬æ¢ä¸ºGR00Tæ ¼å¼"""
#         try:
#             groot_obs = {}
            
#             # 1. è§†è§‰æ•°æ®
#             camera_key = "frontview_image"
#             if camera_key in obs and obs[camera_key] is not None:
#                 img = obs[camera_key]
#                 if img.shape[:2] != (480, 640):
#                     img = cv2.resize(img, (640, 480))
#                 if len(img.shape) == 3 and img.shape[2] == 3:
#                     groot_obs["video.webcam"] = img[np.newaxis, :, :, :].astype(np.uint8)
#                 else:
#                     groot_obs["video.webcam"] = np.zeros((1, 480, 640, 3), dtype=np.uint8)
#             else:
#                 groot_obs["video.webcam"] = np.zeros((1, 480, 640, 3), dtype=np.uint8)
            
#             # 2. æœºå™¨äººå…³èŠ‚çŠ¶æ€
#             if "robot0_joint_pos" in obs and obs["robot0_joint_pos"] is not None:
#                 joint_pos = obs["robot0_joint_pos"]
#                 if len(joint_pos) >= 5:
#                     groot_obs["state.single_arm"] = joint_pos[:5][np.newaxis, :].astype(np.float32)
#                 else:
#                     groot_obs["state.single_arm"] = np.zeros((1, 5), dtype=np.float32)
#             else:
#                 groot_obs["state.single_arm"] = np.zeros((1, 5), dtype=np.float32)
            
#             # 3. å¤¹çˆªçŠ¶æ€
#             if "robot0_gripper_qpos" in obs and obs["robot0_gripper_qpos"] is not None:
#                 gripper_pos = obs["robot0_gripper_qpos"]
#                 if len(gripper_pos) > 0:
#                     normalized_gripper = np.clip(gripper_pos[0], 0.0, 1.0)
#                     groot_obs["state.gripper"] = np.array([[normalized_gripper]], dtype=np.float32)
#                 else:
#                     groot_obs["state.gripper"] = np.zeros((1, 1), dtype=np.float32)
#             else:
#                 groot_obs["state.gripper"] = np.zeros((1, 1), dtype=np.float32)
            
#             # 4. ä»»åŠ¡æè¿°
#             groot_obs["annotation.human.task_description"] = [task_description]
            
#             self.processed_observations += 1
#             return groot_obs
            
#         except Exception as e:
#             print(f"âš ï¸ è§‚æµ‹è½¬æ¢é”™è¯¯: {e}")
#             return self._get_default_groot_obs()
    
#     def groot_to_robosuite_action(self, groot_action: Dict[str, np.ndarray]) -> np.ndarray:
#         """å°†GR00TåŠ¨ä½œè½¬æ¢ä¸ºRoboSuiteæ ¼å¼"""
#         try:
#             world_vector = groot_action.get('world_vector', np.zeros((1, 3)))[0]
#             rotation_delta = groot_action.get('rotation_delta', np.zeros((1, 3)))[0]
#             gripper_action = groot_action.get('gripper_closedness_action', np.zeros((1, 1)))[0][0]
            
#             # æ„å»ºRoboSuiteåŠ¨ä½œ [x, y, z, rx, ry, rz, gripper]
#             action = np.zeros(7, dtype=np.float32)
#             action[0:3] = np.clip(world_vector * 0.02, -0.1, 0.1)
#             action[3:5] = np.clip(rotation_delta[:2] * 0.01, -0.1, 0.1)
#             action[5] = 0.0
#             action[6] = np.clip(gripper_action, -1.0, 1.0)
            
#             self.processed_actions += 1
#             return action
            
#         except Exception as e:
#             print(f"âš ï¸ åŠ¨ä½œè½¬æ¢é”™è¯¯: {e}")
#             return np.zeros(7, dtype=np.float32)
    
#     def _get_default_groot_obs(self) -> Dict[str, Any]:
#         return {
#             "video.webcam": np.zeros((1, 480, 640, 3), dtype=np.uint8),
#             "state.single_arm": np.zeros((1, 5), dtype=np.float32),
#             "state.gripper": np.zeros((1, 1), dtype=np.float32),
#             "annotation.human.task_description": ["Pick red strawberries and place them on the white plate"]
#         }
    
#     def get_stats(self) -> Dict[str, int]:
#         return {
#             "observations_processed": self.processed_observations,
#             "actions_processed": self.processed_actions
#         }

# # ==================== GR00Tå®¢æˆ·ç«¯ ====================

# class GR00TClient:
#     """GR00Tå®¢æˆ·ç«¯"""
    
#     def __init__(self, host: str = "localhost", port: int = 5555):
#         self.host = host
#         self.port = port
#         self.client = None
#         self.adapter = RoboSuiteGR00TAdapter()
#         self.is_connected = False
        
#         self.total_calls = 0
#         self.successful_calls = 0
#         self.total_latency = 0.0
    
#     def connect(self) -> bool:
#         """è¿æ¥åˆ°GR00TæœåŠ¡"""
#         if not GROOT_CLIENT_AVAILABLE:
#             print("âŒ GR00Tå®¢æˆ·ç«¯åº“ä¸å¯ç”¨")
#             return False
        
#         try:
#             print(f"ğŸ”— è¿æ¥GR00TæœåŠ¡: {self.host}:{self.port}")
            
#             self.client = RobotInferenceClient(host=self.host, port=self.port)
            
#             # è¿æ¥æµ‹è¯•
#             test_obs = self.adapter._get_default_groot_obs()
#             start_time = time.time()
#             test_result = self.client.get_action(test_obs)
#             latency = time.time() - start_time
            
#             if test_result is not None:
#                 self.is_connected = True
#                 print(f"âœ… GR00Tè¿æ¥æˆåŠŸï¼å»¶è¿Ÿ: {latency:.3f}s")
#                 return True
#             else:
#                 print("âŒ GR00Tæµ‹è¯•å¤±è´¥")
#                 return False
                
#         except Exception as e:
#             print(f"âŒ GR00Tè¿æ¥å¤±è´¥: {e}")
#             return False
    
#     def get_action(self, observation: Dict[str, np.ndarray], 
#                    task_description: str = "Pick red strawberries and place them on the white plate") -> Optional[np.ndarray]:
#         """è·å–åŠ¨ä½œ"""
#         if not self.is_connected:
#             return None
        
#         self.total_calls += 1
#         start_time = time.time()
        
#         try:
#             groot_obs = self.adapter.robosuite_to_groot_obs(observation, task_description)
#             groot_action = self.client.get_action(groot_obs)
            
#             latency = time.time() - start_time
#             self.total_latency += latency
            
#             if groot_action is not None:
#                 self.successful_calls += 1
#                 robosuite_action = self.adapter.groot_to_robosuite_action(groot_action)
#                 return robosuite_action
#             else:
#                 return None
                
#         except Exception as e:
#             latency = time.time() - start_time
#             self.total_latency += latency
#             if self.total_calls % 10 == 0:
#                 print(f"âš ï¸ åŠ¨ä½œé¢„æµ‹é”™è¯¯: {e}")
#             return None
    
#     def get_stats(self) -> Dict[str, Any]:
#         return {
#             "total_calls": self.total_calls,
#             "successful_calls": self.successful_calls,
#             "success_rate": self.successful_calls / self.total_calls if self.total_calls > 0 else 0,
#             "average_latency": self.total_latency / self.total_calls if self.total_calls > 0 else 0,
#             "adapter_stats": self.adapter.get_stats()
#         }

# # ==================== ä¸»æ¥å£ç±» ====================

# @dataclass
# class ExperimentConfig:
#     """å®éªŒé…ç½®"""
#     robot: str = "Panda"
#     robot_xml_path: Optional[str] = None
#     num_episodes: int = 3
#     max_steps_per_episode: int = 200
#     enable_gui: bool = False
#     enable_video_recording: bool = False
#     video_output_dir: str = "./strawberry_experiment_videos"
#     groot_host: str = "localhost"
#     groot_port: int = 5555

# class StrawberryPickPlaceInterface:
#     """è‰è“æ‹£é€‰ä¸»æ¥å£"""
    
#     def __init__(self, config: ExperimentConfig):
#         self.config = config
#         self.environment = None
#         self.groot_client = None
        
#         print("ğŸ“ åˆå§‹åŒ–è‰è“æ‹£é€‰æ¥å£")
#         print(f"   æœºå™¨äºº: {config.robot}")
#         print(f"   ç¯å¢ƒ: çœŸå®ç‰©ä½“ç¯å¢ƒï¼ˆ3è‰è“+4ç»¿çƒ+1ç›˜å­ï¼‰")
#         print(f"   GR00T: {config.groot_host}:{config.groot_port}")
        
#         self._create_environment()
#         self._create_groot_client()
    
#     def _create_environment(self):
#         """åˆ›å»ºç¯å¢ƒ"""
#         if not ROBOSUITE_AVAILABLE:
#             raise ImportError("éœ€è¦å®‰è£…RoboSuite")
        
#         try:
#             print("ğŸ—ï¸ åˆ›å»ºè‰è“æ‹£é€‰ç¯å¢ƒ...")
            
#             self.environment = StrawberryPickPlaceEnvironment(
#                 robots=self.config.robot,
#                 robot_xml_path=self.config.robot_xml_path,
#                 has_renderer=self.config.enable_gui,
#                 has_offscreen_renderer=True,
#                 use_camera_obs=True,
#                 camera_names="frontview",
#                 camera_heights=480,
#                 camera_widths=640,
#                 control_freq=20,
#                 horizon=self.config.max_steps_per_episode * 2,
#                 ignore_done=True,
#                 enable_video_recording=self.config.enable_video_recording,
#                 video_output_dir=self.config.video_output_dir
#             )
            
#             print("âœ… è‰è“æ‹£é€‰ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼")
            
#         except Exception as e:
#             print(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
#             raise
    
#     def _create_groot_client(self):
#         """åˆ›å»ºGR00Tå®¢æˆ·ç«¯"""
#         self.groot_client = GR00TClient(self.config.groot_host, self.config.groot_port)
    
#     def connect_groot(self) -> bool:
#         """è¿æ¥GR00T"""
#         return self.groot_client.connect()
    
#     def run_episode(self, episode_id: int) -> Dict[str, Any]:
#         """è¿è¡Œå•ä¸ªepisodeå¹¶å½•åˆ¶è§†é¢‘"""
#         print(f"\nğŸ¯ Episode {episode_id + 1}")
#         print(f"   ä»»åŠ¡: æ‹£é€‰3ä¸ªçº¢è‰²è‰è“å¹¶æ”¾ç½®åˆ°ç™½è‰²ç›˜å­ä¸Š")
        
#         # å¼€å§‹è§†é¢‘å½•åˆ¶
#         video_recording_success = False
#         if self.config.enable_video_recording:
#             video_recording_success = self.environment.start_episode_recording(episode_id)
        
#         # é‡ç½®ç¯å¢ƒ
#         obs = self.environment.reset()
        
#         episode_stats = {
#             "episode_id": episode_id,
#             "steps": 0,
#             "total_reward": 0.0,
#             "task_success": False,
#             "groot_calls": 0,
#             "groot_successes": 0,
#             "start_time": time.time(),
#             "video_recorded": video_recording_success
#         }
        
#         done = False
#         step = 0
        
#         print(f"     è¿›åº¦: ", end="", flush=True)
        
#         while not done and step < self.config.max_steps_per_episode:
#             # è·å–GR00TåŠ¨ä½œ
#             action = self.groot_client.get_action(obs)
#             episode_stats["groot_calls"] += 1
            
#             if action is not None:
#                 episode_stats["groot_successes"] += 1
#                 print(".", end="", flush=True)
#             else:
#                 action = np.zeros(7, dtype=np.float32)
#                 print("x", end="", flush=True)
            
#             # ç¯å¢ƒæ­¥è¿›
#             obs, reward, done, info = self.environment.step(action)
            
#             episode_stats["steps"] += 1
#             episode_stats["total_reward"] += reward
#             step += 1
            
#             # è·å–ä»»åŠ¡ä¿¡æ¯
#             task_info = self.environment.get_task_info()
            
#             # æ£€æŸ¥ä»»åŠ¡æˆåŠŸ
#             if task_info["task_success"]:
#                 episode_stats["task_success"] = True
#                 print("ğŸ‰", end="", flush=True)
#                 done = True
            
#             # è¿›åº¦æ˜¾ç¤º
#             if step % 20 == 0:
#                 progress = task_info["task_progress"]
#                 print(f"|{progress:.0%}", end="", flush=True)
        
#         # åœæ­¢è§†é¢‘å½•åˆ¶
#         if self.config.enable_video_recording:
#             self.environment.stop_episode_recording()
        
#         episode_stats["duration"] = time.time() - episode_stats["start_time"]
        
#         print()  # æ¢è¡Œ
        
#         # æ‰“å°episodeç»“æœ
#         self._print_episode_result(episode_stats)
        
#         return episode_stats
    
#     def run_experiment(self) -> List[Dict[str, Any]]:
#         """è¿è¡Œå®Œæ•´å®éªŒ"""
#         print(f"\nğŸš€ å¼€å§‹è‰è“æ‹£é€‰å®éªŒ")
#         print("=" * 60)
        
#         if not self.connect_groot():
#             print("âŒ éœ€è¦å…ˆè¿æ¥GR00TæœåŠ¡")
#             return []
        
#         results = []
        
#         try:
#             for i in range(self.config.num_episodes):
#                 result = self.run_episode(i)
#                 results.append(result)
                
#                 if i < self.config.num_episodes - 1:
#                     time.sleep(0.5)
            
#             self._print_summary(results)
            
#         except KeyboardInterrupt:
#             print("\nâš ï¸ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
#         except Exception as e:
#             print(f"\nâŒ å®éªŒå¼‚å¸¸: {e}")
        
#         return results
    
#     def _print_episode_result(self, stats: Dict[str, Any]):
#         """æ‰“å°episodeç»“æœ"""
#         status = "âœ… æˆåŠŸ" if stats["task_success"] else "âŒ å¤±è´¥"
#         groot_rate = stats["groot_successes"] / stats["groot_calls"] if stats["groot_calls"] > 0 else 0
        
#         print(f"   ç»“æœ: {status}")
#         print(f"   æ­¥æ•°: {stats['steps']}, æ—¶é•¿: {stats['duration']:.1f}s")
#         print(f"   å¥–åŠ±: {stats['total_reward']:.2f}")
#         print(f"   GR00TæˆåŠŸç‡: {groot_rate:.1%} ({stats['groot_successes']}/{stats['groot_calls']})")
        
#         if stats.get("video_recorded", False):
#             print(f"   ğŸ¥ è§†é¢‘å·²å½•åˆ¶")
#         elif self.config.enable_video_recording:
#             print(f"   âš ï¸ è§†é¢‘å½•åˆ¶å¤±è´¥")
    
#     def _print_summary(self, results: List[Dict[str, Any]]):
#         """æ‰“å°å®éªŒæ€»ç»“"""
#         print(f"\nğŸ“Š å®éªŒæ€»ç»“")
#         print("=" * 60)
        
#         if not results:
#             print("âŒ æ²¡æœ‰ç»“æœæ•°æ®")
#             return
        
#         total_episodes = len(results)
#         successful_episodes = sum(1 for r in results if r["task_success"])
#         success_rate = successful_episodes / total_episodes
        
#         avg_steps = np.mean([r["steps"] for r in results])
#         avg_reward = np.mean([r["total_reward"] for r in results])
#         avg_duration = np.mean([r["duration"] for r in results])
        
#         total_groot_calls = sum(r["groot_calls"] for r in results)
#         total_groot_successes = sum(r["groot_successes"] for r in results)
#         groot_success_rate = total_groot_successes / total_groot_calls if total_groot_calls > 0 else 0
        
#         print(f"ğŸ¯ æ€»ä½“è¡¨ç°:")
#         print(f"   ä»»åŠ¡æˆåŠŸç‡: {success_rate:.1%} ({successful_episodes}/{total_episodes})")
#         print(f"   å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
#         print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
#         print(f"   å¹³å‡æ—¶é•¿: {avg_duration:.1f}s")
#         print(f"   GR00TæˆåŠŸç‡: {groot_success_rate:.1%}")
        
#         # GR00Tç»Ÿè®¡
#         groot_stats = self.groot_client.get_stats()
#         print(f"   å¹³å‡å»¶è¿Ÿ: {groot_stats['average_latency']:.3f}s")
        
#         # è§†é¢‘å½•åˆ¶ç»Ÿè®¡
#         if self.config.enable_video_recording:
#             video_episodes = sum(1 for r in results if r.get("video_recorded", False))
#             print(f"   è§†é¢‘å½•åˆ¶: {video_episodes}/{total_episodes} episodes")
        
#         print(f"\nâœ… {self.config.robot}æœºå™¨äººè‰è“æ‹£é€‰æµ‹è¯•å®Œæˆ!")
#         print(f"   ç¯å¢ƒ: è™šæ‹Ÿ3è‰è“ + 4ç»¿çƒ + 1ç›˜å­")
#         print(f"   å®Œå…¨åŒ¹é…è®­ç»ƒæ•°æ®é›†ä»»åŠ¡é€»è¾‘")
        
#         if success_rate == 0:
#             print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
#             print(f"   1. ä»»åŠ¡é˜ˆå€¼å·²ä¼˜åŒ–ï¼Œä½†å¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒæ—¶é—´")
#             print(f"   2. æ£€æŸ¥GR00Tæ¨¡å‹æ˜¯å¦é’ˆå¯¹è‰è“æ‹£é€‰ä»»åŠ¡è®­ç»ƒ")
#             print(f"   3. è€ƒè™‘è°ƒæ•´è™šæ‹Ÿç‰©ä½“ä½ç½®å¸ƒå±€")
#             print(f"   4. æŸ¥çœ‹è§†é¢‘å½•åˆ¶äº†è§£æœºå™¨äººè¡Œä¸ºæ¨¡å¼")
#         elif success_rate > 0:
#             print(f"\nğŸ‰ ä»»åŠ¡ä¼˜åŒ–æˆåŠŸ!")
#             print(f"   æˆåŠŸç‡æå‡ï¼Œæœºå™¨äººèƒ½å¤Ÿå®Œæˆè‰è“æ‹£é€‰ä»»åŠ¡")
    
#     def close(self):
#         """å…³é—­æ¥å£"""
#         if self.environment:
#             self.environment.close()
#         print("ğŸ”’ æ¥å£å·²å…³é—­")

# # ==================== ä¸»å‡½æ•° ====================

# def main():
#     """ä¸»å‡½æ•° - æ”¯æŒSO100æœºå™¨äººå’Œè§†é¢‘å½•åˆ¶"""
#     print("ğŸ“ RoboSuite-GR00Tè‰è“æ‹£é€‰ç¯å¢ƒæ¥å£")
#     print("æ”¯æŒSO100æœºå™¨äºº + è§†é¢‘å½•åˆ¶ + ä¼˜åŒ–ä»»åŠ¡å¥–åŠ±")
#     print("=" * 60)
    
#     # æ£€æŸ¥ä¾èµ–
#     if not ROBOSUITE_AVAILABLE:
#         print("âŒ éœ€è¦å®‰è£…RoboSuite")
#         return
    
#     if not GROOT_CLIENT_AVAILABLE:
#         print("âŒ éœ€è¦å®‰è£…GR00Tå®¢æˆ·ç«¯åº“")
#         return
    
#     # æ£€æŸ¥SO100æœºå™¨äººXMLè·¯å¾„
#     so100_xml_path = "/root/autodl-tmp/gr00t/SO-ARM100/Simulation/URDF_SO100/SO_5DOF_ARM100_05d.SLDASM/urdf/SO_5DOF_ARM100_05d.SLDASM.xml"
    
#     robot_available = False
#     if os.path.exists(so100_xml_path):
#         print(f"\nâœ… æ‰¾åˆ°SO100æœºå™¨äººXML: {so100_xml_path}")
#         robot_available = True
        
#         # è¯¢é—®ç”¨æˆ·æ˜¯å¦ä½¿ç”¨SO100
#         try:
#             choice = input("ğŸ¤– æ˜¯å¦ä½¿ç”¨SO100æœºå™¨äººï¼Ÿ(y/n, é»˜è®¤y): ").lower().strip()
#             use_so100 = choice in ['', 'y', 'yes', 'æ˜¯']
#         except:
#             use_so100 = True
            
#         if use_so100:
#             robot_type = "SO100"
#             robot_xml = so100_xml_path
#             print(f"   âœ… å°†ä½¿ç”¨SO100æœºå™¨äºº")
#         else:
#             robot_type = "Panda"
#             robot_xml = None
#             print(f"   ğŸ¼ å°†ä½¿ç”¨Pandaæœºå™¨äººï¼ˆç¨³å®šé€‰æ‹©ï¼‰")
#     else:
#         print(f"\nâš ï¸ æœªæ‰¾åˆ°SO100 XMLæ–‡ä»¶ï¼Œä½¿ç”¨Pandaæœºå™¨äºº")
#         print(f"   æœŸæœ›è·¯å¾„: {so100_xml_path}")
#         robot_type = "Panda"
#         robot_xml = None
    
#     # è¯¢é—®æ˜¯å¦å¯ç”¨è§†é¢‘å½•åˆ¶
#     try:
#         video_choice = input("ğŸ¥ æ˜¯å¦å¯ç”¨è§†é¢‘å½•åˆ¶ï¼Ÿ(y/n, é»˜è®¤y): ").lower().strip()
#         enable_video = video_choice in ['', 'y', 'yes', 'æ˜¯']
#     except:
#         enable_video = True
    
#     # è¯¢é—®æ˜¯å¦å¯ç”¨GUI
#     try:
#         gui_choice = input("ğŸ‘ï¸ æ˜¯å¦å¯ç”¨å®æ—¶å¯è§†åŒ–ï¼Ÿ(y/n, é»˜è®¤n): ").lower().strip()
#         enable_gui = gui_choice in ['y', 'yes', 'æ˜¯']
#     except:
#         enable_gui = False
    
#     # å®éªŒé…ç½®
#     config = ExperimentConfig(
#         robot=robot_type,
#         robot_xml_path=robot_xml,
#         num_episodes=3,
#         max_steps_per_episode=200,
#         enable_gui=enable_gui,
#         enable_video_recording=enable_video,
#         video_output_dir="./strawberry_so100_videos",
#         groot_host="localhost",
#         groot_port=5555
#     )
    
#     print(f"\nğŸ› ï¸ å®éªŒé…ç½®:")
#     print(f"   æœºå™¨äºº: {config.robot}")
#     if config.robot_xml_path:
#         print(f"   XMLè·¯å¾„: {config.robot_xml_path}")
#     print(f"   Episodes: {config.num_episodes}")
#     print(f"   æœ€å¤§æ­¥æ•°: {config.max_steps_per_episode}")
#     print(f"   å¯è§†åŒ–: {'å¯ç”¨' if config.enable_gui else 'ç¦ç”¨'}")
#     print(f"   è§†é¢‘å½•åˆ¶: {'å¯ç”¨' if config.enable_video_recording else 'ç¦ç”¨'}")
#     if config.enable_video_recording:
#         print(f"   è§†é¢‘ç›®å½•: {config.video_output_dir}")
#     print(f"   ä»»åŠ¡: ä¼˜åŒ–çš„è‰è“æ‹£é€‰ï¼ˆæ›´å®¹æ˜“æˆåŠŸï¼‰")
    
#     # æ˜¾ç¤ºSO100æœºå™¨äººæ³¨å†Œä¿¡æ¯
#     if robot_type == "SO100":
#         create_so100_robot_registration()
    
#     # åˆ›å»ºæ¥å£å¹¶è¿è¡Œå®éªŒ
#     interface = StrawberryPickPlaceInterface(config)
    
#     try:
#         results = interface.run_experiment()
        
#         # ä¿å­˜ç»“æœ
#         if results:
#             timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#             robot_suffix = config.robot.lower()
#             filename = f"strawberry_{robot_suffix}_results_{timestamp}.json"
            
#             with open(filename, 'w') as f:
#                 json.dump(results, f, indent=2, default=str)
            
#             print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {filename}")
            
#             print(f"\nğŸ¯ å®éªŒæ€»ç»“:")
#             successful = sum(1 for r in results if r["task_success"])
#             print(f"   âœ… æˆåŠŸå®Œæˆ: {successful}/{len(results)} episodes")
#             print(f"   ğŸ¤– æœºå™¨äºº: {config.robot}")
#             print(f"   ğŸ“ ä¼˜åŒ–çš„è‰è“æ‹£é€‰ä»»åŠ¡æµ‹è¯•å®Œæˆ")
#             print(f"   ğŸ”— GR00Tæ¥å£é›†æˆæˆåŠŸ")
            
#             if config.enable_video_recording:
#                 video_count = sum(1 for r in results if r.get("video_recorded", False))
#                 print(f"   ğŸ¥ è§†é¢‘å½•åˆ¶: {video_count}/{len(results)} episodes")
#                 print(f"   ğŸ“ è§†é¢‘ä¿å­˜åœ¨: {config.video_output_dir}")
        
#     except KeyboardInterrupt:
#         print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
#     except Exception as e:
#         print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
#         import traceback
#         traceback.print_exc()
#     finally:
#         interface.close()

# if __name__ == "__main__":
#     main()

# #!/usr/bin/env python3
# """
# RoboSuite-GR00Tè‰è“æ‹£é€‰ç¯å¢ƒæ¥å£
# æ”¯æŒSO100æœºå™¨äºº + ä¼˜åŒ–çš„5DOFæ§åˆ¶
# """

# import os
# import sys
# import time
# import json
# import numpy as np
# import torch
# import cv2
# from pathlib import Path
# from typing import Dict, List, Any, Tuple, Optional
# from dataclasses import dataclass
# from datetime import datetime

# # è®¾ç½®ç¯å¢ƒå˜é‡
# os.environ.setdefault('MUJOCO_GL', 'egl')
# os.environ.setdefault('PYOPENGL_PLATFORM', 'egl')

# # å¯¼å…¥æ£€æŸ¥
# try:
#     import robosuite
#     from robosuite.controllers import load_composite_controller_config
#     ROBOSUITE_AVAILABLE = True
#     print("âœ… RoboSuiteå¯ç”¨")
# except ImportError as e:
#     print(f"âŒ RoboSuiteä¸å¯ç”¨: {e}")
#     ROBOSUITE_AVAILABLE = False

# try:
#     from gr00t.eval.robot import RobotInferenceClient
#     GROOT_CLIENT_AVAILABLE = True
#     print("âœ… GR00Tå®¢æˆ·ç«¯å¯ç”¨")
# except ImportError as e:
#     print(f"âŒ GR00Tå®¢æˆ·ç«¯ä¸å¯ç”¨: {e}")
#     GROOT_CLIENT_AVAILABLE = False

# # ==================== è§†é¢‘å½•åˆ¶å™¨ ====================
# # (VideoRecorderç±»ä¿æŒä¸å˜)
# class VideoRecorder:
#     """è§†é¢‘å½•åˆ¶å™¨ - è®°å½•æœºå™¨äººæ‰§è¡Œä»»åŠ¡è¿‡ç¨‹"""
    
#     def __init__(self, 
#                  output_dir: str = "./strawberry_experiment_videos",
#                  fps: int = 20,
#                  video_size: Tuple[int, int] = (640, 480),
#                  codec: str = 'mp4v'):
#         """åˆå§‹åŒ–è§†é¢‘å½•åˆ¶å™¨"""
#         self.output_dir = Path(output_dir)
#         self.output_dir.mkdir(parents=True, exist_ok=True)
        
#         self.fps = fps
#         self.video_size = video_size
#         self.codec = codec
        
#         # å½•åˆ¶çŠ¶æ€
#         self.is_recording = False
#         self.video_writer = None
#         self.current_episode = 0
#         self.frame_count = 0
        
#         print(f"ğŸ¥ è§†é¢‘å½•åˆ¶å™¨åˆå§‹åŒ–")
#         print(f"   ä¿å­˜ç›®å½•: {self.output_dir}")
#         print(f"   è§†é¢‘å‚æ•°: {video_size[0]}x{video_size[1]} @ {fps}fps")
    
#     def start_episode_recording(self, episode_id: int, experiment_name: str = "strawberry_experiment"):
#         """å¼€å§‹å½•åˆ¶æ–°çš„episode"""
#         if self.is_recording:
#             self.stop_episode_recording()
        
#         self.current_episode = episode_id
#         self.frame_count = 0
        
#         # ç”Ÿæˆè§†é¢‘æ–‡ä»¶å
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         filename = f"{experiment_name}_episode_{episode_id:03d}_{timestamp}.mp4"
#         self.video_path = self.output_dir / filename
        
#         # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
#         fourcc = cv2.VideoWriter_fourcc(*self.codec)
#         self.video_writer = cv2.VideoWriter(
#             str(self.video_path),
#             fourcc,
#             self.fps,
#             self.video_size
#         )
        
#         if not self.video_writer.isOpened():
#             print(f"âŒ æ— æ³•åˆ›å»ºè§†é¢‘æ–‡ä»¶: {self.video_path}")
#             return False
        
#         self.is_recording = True
#         print(f"ğŸ¬ å¼€å§‹å½•åˆ¶ Episode {episode_id}: {filename}")
#         return True
    
#     def add_frame(self, image: np.ndarray, step_info: Dict[str, Any] = None):
#         """æ·»åŠ ä¸€å¸§åˆ°å½•åˆ¶"""
#         if not self.is_recording:
#             return
        
#         try:
#             # å¤„ç†å›¾åƒæ ¼å¼
#             processed_image = self._process_image(image, step_info)
            
#             # å†™å…¥è§†é¢‘æ–‡ä»¶
#             if self.video_writer and self.video_writer.isOpened():
#                 self.video_writer.write(processed_image)
#                 self.frame_count += 1
                
#         except Exception as e:
#             print(f"âš ï¸ æ·»åŠ å¸§å¤±è´¥: {e}")
    
#     def _process_image(self, image: np.ndarray, step_info: Dict[str, Any] = None) -> np.ndarray:
#         """å¤„ç†å›¾åƒæ ¼å¼å¹¶æ·»åŠ ä¿¡æ¯å åŠ """
#         try:
#             # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®
#             if image is None:
#                 image = np.zeros((*self.video_size[::-1], 3), dtype=np.uint8)
            
#             # è½¬æ¢æ•°æ®ç±»å‹
#             if image.dtype != np.uint8:
#                 if image.max() <= 1.0:
#                     image = (image * 255).astype(np.uint8)
#                 else:
#                     image = image.astype(np.uint8)
            
#             # è°ƒæ•´å°ºå¯¸
#             if image.shape[:2] != self.video_size[::-1]:
#                 image = cv2.resize(image, self.video_size)
            
#             # ç¡®ä¿æ˜¯3é€šé“
#             if len(image.shape) == 2:
#                 image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
#             elif len(image.shape) == 3 and image.shape[2] == 4:
#                 image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)
            
#             # æ·»åŠ ä¿¡æ¯å åŠ 
#             if step_info:
#                 image = self._add_info_overlay(image, step_info)
            
#             return image
            
#         except Exception as e:
#             print(f"âš ï¸ å›¾åƒå¤„ç†å¤±è´¥: {e}")
#             return np.zeros((*self.video_size[::-1], 3), dtype=np.uint8)
    
#     def _add_info_overlay(self, image: np.ndarray, step_info: Dict[str, Any]) -> np.ndarray:
#         """åœ¨å›¾åƒä¸Šæ·»åŠ ä¿¡æ¯å åŠ """
#         try:
#             overlay_image = image.copy()
            
#             # è®¾ç½®å­—ä½“å‚æ•°
#             font = cv2.FONT_HERSHEY_SIMPLEX
#             font_scale = 0.6
#             color = (0, 255, 0)  # ç»¿è‰²
#             thickness = 2
            
#             # æ·»åŠ åŸºæœ¬ä¿¡æ¯
#             y_offset = 30
            
#             # Episodeå’ŒStepä¿¡æ¯
#             if 'step' in step_info:
#                 text = f"Episode: {self.current_episode} | Step: {step_info['step']}"
#                 cv2.putText(overlay_image, text, (10, y_offset), font, font_scale, color, thickness)
#                 y_offset += 25
            
#             # ä»»åŠ¡è¿›åº¦
#             if 'task_progress' in step_info:
#                 progress = step_info['task_progress']
#                 strawberries_on_plate = step_info.get('strawberries_on_plate', 0)
#                 text = f"Progress: {progress:.1%} | Strawberries: {strawberries_on_plate}/3"
#                 cv2.putText(overlay_image, text, (10, y_offset), font, font_scale, color, thickness)
#                 y_offset += 25
            
#             # å¥–åŠ±ä¿¡æ¯
#             if 'reward' in step_info:
#                 text = f"Reward: {step_info['reward']:.2f}"
#                 cv2.putText(overlay_image, text, (10, y_offset), font, font_scale, color, thickness)
#                 y_offset += 25
            
#             # ä»»åŠ¡æˆåŠŸæ ‡è®°
#             if step_info.get('task_success', False):
#                 text = "TASK SUCCESS!"
#                 cv2.putText(overlay_image, text, (10, image.shape[0] - 30), 
#                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3)
            
#             return overlay_image
            
#         except Exception as e:
#             print(f"âš ï¸ ä¿¡æ¯å åŠ å¤±è´¥: {e}")
#             return image
    
#     def stop_episode_recording(self):
#         """åœæ­¢å½“å‰episodeçš„å½•åˆ¶"""
#         if not self.is_recording:
#             return
        
#         print(f"ğŸ¬ åœæ­¢å½•åˆ¶ Episode {self.current_episode} ({self.frame_count} å¸§)")
        
#         self.is_recording = False
        
#         # å…³é—­è§†é¢‘å†™å…¥å™¨
#         if self.video_writer:
#             self.video_writer.release()
#             self.video_writer = None
        
#         if hasattr(self, 'video_path') and self.video_path.exists():
#             file_size = self.video_path.stat().st_size / (1024 * 1024)  # MB
#             print(f"âœ… è§†é¢‘å·²ä¿å­˜: {self.video_path} ({file_size:.1f}MB)")
        
#         self.frame_count = 0
    
#     def cleanup(self):
#         """æ¸…ç†èµ„æº"""
#         if self.is_recording:
#             self.stop_episode_recording()
        
#         print("ğŸ§¹ è§†é¢‘å½•åˆ¶å™¨èµ„æºå·²æ¸…ç†")

# # ==================== é…ç½®å’Œå·¥å…·å‡½æ•° ====================

# @dataclass
# class ExperimentConfig:
#     """å®éªŒé…ç½®"""
#     robot: str = "SO100"  # é»˜è®¤ä½¿ç”¨SO100
#     robot_xml_path: Optional[str] = None
#     num_episodes: int = 3
#     max_steps_per_episode: int = 200
#     enable_gui: bool = False
#     enable_video_recording: bool = False
#     video_output_dir: str = "./strawberry_experiment_videos"
#     groot_host: str = "localhost"
#     groot_port: int = 5555

# def detect_robot_dof(robot_name: str) -> int:
#     """æ£€æµ‹æœºå™¨äººDOFæ•°é‡"""
#     robot_dof_mapping = {
#         "SO100": 5,
#         "Panda": 7,
#         "UR5e": 6,
#         "IIWA": 7,
#         "Jaco": 6,
#         "Kinova3": 7,
#         "Sawyer": 7,
#         "Baxter": 7,
#         "XArm7": 7
#     }
#     return robot_dof_mapping.get(robot_name, 7)  # é»˜è®¤7DOF

# def create_so100_robot_registration():
#     """SO100æœºå™¨äººæ³¨å†Œä¿¡æ¯"""
#     print("\nğŸ”§ SO100æœºå™¨äººæ³¨å†ŒçŠ¶æ€ï¼š")
#     print("=" * 50)
#     print("âœ… SO100å·²åœ¨RoboSuiteä¸­æ³¨å†Œ")
#     print("   - 5DOFæœºæ¢°è‡‚é…ç½®")
#     print("   - æ”¯æŒæœ«ç«¯æ‰§è¡Œå™¨æ§åˆ¶")
#     print("   - å…¼å®¹GR00Tæ¥å£")
#     return True

# # ==================== è‰è“æ‹£é€‰ç¯å¢ƒåŒ…è£…å™¨ ====================

# class StrawberryPickPlaceEnvironment:
#     """
#     è‰è“æ‹£é€‰ç¯å¢ƒåŒ…è£…å™¨
#     ä½¿ç”¨æ ‡å‡†çš„RoboSuiteæœºå™¨äººï¼ˆåŒ…æ‹¬SO100ï¼‰+ è§†é¢‘å½•åˆ¶ + è™šæ‹Ÿç‰©ä½“ä»»åŠ¡é€»è¾‘
#     """
    
#     def __init__(
#         self,
#         robots="SO100",
#         robot_xml_path=None,
#         has_renderer=False,
#         has_offscreen_renderer=True,
#         use_camera_obs=True,
#         camera_names="frontview",
#         camera_heights=480,
#         camera_widths=640,
#         control_freq=20,
#         horizon=1000,
#         ignore_done=True,
#         enable_video_recording=False,
#         video_output_dir="./strawberry_experiment_videos"
#     ):
        
#         # å­˜å‚¨é…ç½®
#         self.robots = robots if isinstance(robots, list) else [robots]
#         self.robot_name = self.robots[0] if isinstance(self.robots, list) else robots
#         self.robot_xml_path = robot_xml_path
#         self.enable_video_recording = enable_video_recording
        
#         # æ£€æµ‹æœºå™¨äººDOF
#         self.robot_dof = detect_robot_dof(self.robot_name)
#         print(f"ğŸ¤– ä½¿ç”¨{self.robot_name}æœºå™¨äººï¼ŒDOF: {self.robot_dof}")
        
#         # è§†é¢‘å½•åˆ¶å™¨
#         self.video_recorder = None
#         if enable_video_recording:
#             self.video_recorder = VideoRecorder(
#                 output_dir=video_output_dir,
#                 fps=20,
#                 video_size=(camera_widths, camera_heights)
#             )
        
#         # ä»»åŠ¡ç›¸å…³çŠ¶æ€
#         self.strawberry_names = ["strawberry_0", "strawberry_1", "strawberry_2"]
#         self.green_ball_names = ["green_ball_0", "green_ball_1", "green_ball_2", "green_ball_3"]
#         self.plate_name = "plate"
        
#         self.held_object = None
#         self.placed_strawberries = set()
#         self.task_complete = False
#         self.current_step = 0
#         self.max_steps = horizon
        
#         # åˆ›å»ºç‰©ä½“åˆ—è¡¨
#         self.objects = self._create_objects()
        
#         # åˆ›å»ºåŸºç¡€ç¯å¢ƒ
#         try:
#             print(f"ğŸ”§ åˆ›å»ºåŸºç¡€PickPlaceç¯å¢ƒ...")
#             print(f"   ä½¿ç”¨æœºå™¨äºº: {self.robot_name}")
            
#             # è·å–æ§åˆ¶å™¨é…ç½®
#             controller_config = self._get_controller_config(self.robot_name)
            
#             # ä½¿ç”¨æ ‡å‡†çš„robosuite.makeæ–¹æ³•
#             self.env = robosuite.make(
#                 "PickPlace",
#                 robots=self.robot_name,
#                 controller_configs=controller_config,
#                 has_renderer=has_renderer,
#                 has_offscreen_renderer=has_offscreen_renderer,
#                 use_camera_obs=use_camera_obs,
#                 camera_names=camera_names,
#                 camera_heights=camera_heights,
#                 camera_widths=camera_widths,
#                 control_freq=control_freq,
#                 horizon=horizon,
#                 ignore_done=ignore_done,
#                 single_object_mode=2,  # å•ç‰©ä½“æ¨¡å¼
#                 object_type="can"      # é»˜è®¤ä½¿ç”¨canç‰©ä½“
#             )
            
#             print(f"âœ… åŸºç¡€ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
            
#             # è·å–å®é™…æœºå™¨äººä¿¡æ¯
#             self._get_robot_info()
            
#             # è·å–ç¯å¢ƒä¿¡æ¯
#             self._setup_environment_info()
            
#             print(f"âœ… åˆ›å»ºäº†è™šæ‹Ÿç‰©ä½“å¸ƒå±€")
#             print(f"   - 3ä¸ªçº¢è‰²è‰è“: {self.strawberry_names}")
#             print(f"   - 4ä¸ªç»¿è‰²å°çƒ: {self.green_ball_names}")
#             print(f"   - 1ä¸ªç™½è‰²ç›˜å­: {self.plate_name}")
#             if enable_video_recording:
#                 print(f"   ğŸ¥ è§†é¢‘å½•åˆ¶: å¯ç”¨")
#             print(f"   è¯´æ˜ï¼šä½¿ç”¨è™šæ‹Ÿç‰©ä½“é€»è¾‘æ¨¡æ‹Ÿï¼Œé€‚é…{self.robot_dof}DOFæœºå™¨äºº")
            
#         except Exception as e:
#             print(f"âŒ åŸºç¡€ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
#             print(f"   ç¡®ä¿{self.robot_name}æœºå™¨äººå·²æ­£ç¡®æ³¨å†Œåˆ°RoboSuite")
#             raise
        
#         # éªŒè¯ç¯å¢ƒåˆ›å»ºæ˜¯å¦æˆåŠŸ
#         self._verify_environment()
    
#     def _get_robot_info(self):
#         """è·å–å®é™…æœºå™¨äººä¿¡æ¯"""
#         try:
#             if hasattr(self.env, 'robots') and len(self.env.robots) > 0:
#                 robot = self.env.robots[0]
#                 self.actual_robot_dof = getattr(robot, 'dof', self.robot_dof)
#                 self.actual_action_dim = getattr(robot, 'action_dim', self.robot_dof + 1)
                
#                 print(f"ğŸ“Š å®é™…æœºå™¨äººä¿¡æ¯:")
#                 print(f"   DOF: {self.actual_robot_dof}")
#                 print(f"   åŠ¨ä½œç»´åº¦: {self.actual_action_dim}")
#                 print(f"   æœºå™¨äººç±»å‹: {type(robot).__name__}")
                
#                 # æ›´æ–°robot_dofä¸ºå®é™…å€¼
#                 self.robot_dof = self.actual_robot_dof
#             else:
#                 print(f"âš ï¸ æ— æ³•è·å–æœºå™¨äººä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼")
#                 self.actual_robot_dof = self.robot_dof
#                 self.actual_action_dim = self.robot_dof + 1
                
#         except Exception as e:
#             print(f"âš ï¸ è·å–æœºå™¨äººä¿¡æ¯å¤±è´¥: {e}")
#             self.actual_robot_dof = self.robot_dof
#             self.actual_action_dim = self.robot_dof + 1
    
#     def _get_controller_config(self, robot_name: str):
#         """è·å–æ§åˆ¶å™¨é…ç½®"""
#         try:
#             print(f"ğŸ›ï¸ åŠ è½½{robot_name}æ§åˆ¶å™¨é…ç½®")
            
#             if robot_name == "SO100":
#                 # å°è¯•ä½¿ç”¨SO100ä¸“ç”¨é…ç½®
#                 try:
#                     config = load_composite_controller_config(robot="SO100")
#                     print(f"   âœ… ä½¿ç”¨SO100ä¸“ç”¨æ§åˆ¶å™¨é…ç½®")
#                     return config
#                 except:
#                     # å¦‚æœSO100é…ç½®ä¸å­˜åœ¨ï¼Œä½¿ç”¨è‡ªå®šä¹‰é…ç½®
#                     print(f"   âš ï¸ SO100ä¸“ç”¨é…ç½®æœªæ‰¾åˆ°ï¼Œä½¿ç”¨è‡ªå®šä¹‰é…ç½®")
#                     return {
#                         "type": "OSC_POSE",
#                         "input_max": 1,
#                         "input_min": -1,
#                         "output_max": [0.05, 0.05, 0.05, 0.3, 0.3, 0.3],
#                         "output_min": [-0.05, -0.05, -0.05, -0.3, -0.3, -0.3],
#                         "kp": 120,
#                         "damping": 1.2,
#                         "impedance_mode": "fixed",
#                         "kp_limits": [0, 200],
#                         "damping_limits": [0, 8],
#                         "position_limits": None,
#                         "orientation_limits": None,
#                         "uncouple_pos_ori": True,
#                         "control_delta": True,
#                         "interpolation": None,
#                         "ramp_ratio": 0.3,
#                     }
#             else:
#                 config = load_composite_controller_config(robot=robot_name)
#                 print(f"   âœ… ä½¿ç”¨{robot_name}æ ‡å‡†æ§åˆ¶å™¨é…ç½®")
#                 return config
            
#         except Exception as e:
#             print(f"âš ï¸ æ§åˆ¶å™¨é…ç½®å¤±è´¥: {e}")
#             print(f"   ä½¿ç”¨Pandaæ§åˆ¶å™¨ä½œä¸ºå›é€€")
#             return load_composite_controller_config(robot="Panda")
    
#     def _setup_environment_info(self):
#         """è®¾ç½®ç¯å¢ƒä¿¡æ¯"""
#         try:
#             # å°è¯•è·å–æ¡Œé¢ä¿¡æ¯
#             if hasattr(self.env, 'table_full_size'):
#                 self.table_full_size = self.env.table_full_size
#             else:
#                 self.table_full_size = (1.0, 1.0, 0.05)  # é»˜è®¤æ¡Œé¢å°ºå¯¸
            
#             if hasattr(self.env, 'table_top_offset'):
#                 self.table_offset = self.env.table_top_offset
#             elif hasattr(self.env, 'table_offset'):
#                 self.table_offset = self.env.table_offset
#             else:
#                 # ä½¿ç”¨é»˜è®¤æ¡Œé¢ä½ç½®
#                 self.table_offset = np.array([0.0, 0.0, 0.8])
#                 print(f"âš ï¸ ä½¿ç”¨é»˜è®¤æ¡Œé¢ä½ç½®: {self.table_offset}")
            
#             print(f"ğŸ“ æ¡Œé¢ä¿¡æ¯: ä½ç½®={self.table_offset}, å°ºå¯¸={self.table_full_size}")
            
#         except Exception as e:
#             print(f"âš ï¸ è·å–æ¡Œé¢ä¿¡æ¯å¤±è´¥: {e}")
#             # ä½¿ç”¨é»˜è®¤å€¼
#             self.table_full_size = (1.0, 1.0, 0.05)
#             self.table_offset = np.array([0.0, 0.0, 0.8])
    
#     def _verify_environment(self):
#         """éªŒè¯ç¯å¢ƒåˆ›å»ºæ˜¯å¦æˆåŠŸ"""
#         try:
#             print(f"ğŸ” éªŒè¯ç¯å¢ƒ...")
#             test_obs = self.env.reset()
#             print(f"   âœ… ç¯å¢ƒé‡ç½®æˆåŠŸ")
#             print(f"   ğŸ“Š è§‚æµ‹é”®: {list(test_obs.keys())}")
            
#             # æ£€æŸ¥å…³é”®è§‚æµ‹æ•°æ®
#             if any(key in test_obs for key in ["frontview_image", "agentview_image", "image"]):
#                 print(f"   âœ… å›¾åƒæ•°æ®å¯ç”¨")
#             else:
#                 print(f"   âš ï¸ æœªæ‰¾åˆ°å›¾åƒæ•°æ®")
            
#             if any(key in test_obs for key in ["robot0_eef_pos", "eef_pos"]):
#                 print(f"   âœ… æœºå™¨äººçŠ¶æ€å¯ç”¨")
#             else:
#                 print(f"   âš ï¸ æœªæ‰¾åˆ°æœºå™¨äººçŠ¶æ€")
            
#             # æ£€æŸ¥å…³èŠ‚æ•°é‡
#             joint_keys = ["robot0_joint_pos", "joint_pos", "qpos"]
#             for key in joint_keys:
#                 if key in test_obs:
#                     joint_count = len(test_obs[key])
#                     print(f"   ğŸ“Š æ£€æµ‹åˆ°{joint_count}ä¸ªå…³èŠ‚ï¼ˆæœŸæœ›{self.robot_dof}ï¼‰")
#                     break
            
#         except Exception as e:
#             print(f"âš ï¸ ç¯å¢ƒéªŒè¯å¤±è´¥: {e}")
#             print(f"   ç»§ç»­ä½¿ç”¨å½“å‰ç¯å¢ƒé…ç½®")
    
#     def _create_objects(self):
#         """åˆ›å»ºè™šæ‹Ÿç‰©ä½“å®šä¹‰ï¼ˆç”¨äºé€»è¾‘æ¨¡æ‹Ÿï¼‰"""
#         objects = []
        
#         # å®šä¹‰è™šæ‹Ÿç‰©ä½“ä¿¡æ¯
#         for i in range(3):
#             objects.append({
#                 "name": f"strawberry_{i}",
#                 "type": "strawberry",
#                 "color": [0.8, 0.2, 0.2],
#                 "size": [0.02, 0.025],
#                 "target": True  # è¿™æ˜¯ä»»åŠ¡ç›®æ ‡ç‰©ä½“
#             })
        
#         for i in range(4):
#             objects.append({
#                 "name": f"green_ball_{i}",
#                 "type": "green_ball", 
#                 "color": [0.3, 0.8, 0.3],
#                 "size": [0.015],
#                 "target": False  # è¿™æ˜¯å¹²æ‰°ç‰©ä½“
#             })
        
#         objects.append({
#             "name": "plate",
#             "type": "plate",
#             "color": [0.95, 0.95, 0.95],
#             "size": [0.12, 0.008],
#             "target": False  # è¿™æ˜¯æ”¾ç½®ç›®æ ‡
#         })
        
#         return objects
    
#     def reset(self):
#         """é‡ç½®ç¯å¢ƒå¹¶å¼€å§‹è§†é¢‘å½•åˆ¶"""
#         # é‡ç½®åŸºç¡€ç¯å¢ƒ
#         obs = self.env.reset()
        
#         # é‡ç½®ä»»åŠ¡çŠ¶æ€
#         self.current_step = 0
#         self.held_object = None
#         self.placed_strawberries.clear()
#         self.task_complete = False
        
#         # è®¾ç½®è™šæ‹Ÿç‰©ä½“ä½ç½®ï¼ˆç”¨äºä»»åŠ¡é€»è¾‘ï¼‰
#         self._setup_virtual_object_positions()
        
#         return self._process_observation(obs)
    
#     def step(self, action):
#         """ç¯å¢ƒæ­¥è¿›å¹¶å½•åˆ¶è§†é¢‘"""
#         # ç¡®ä¿åŠ¨ä½œç»´åº¦æ­£ç¡®
#         if action is not None and len(action) != self._get_expected_action_dim():
#             action = self._adjust_action_dimensions(action)
        
#         # åŸºç¡€ç¯å¢ƒæ­¥è¿›
#         obs, reward, done, info = self.env.step(action)
#         self.current_step += 1
        
#         # è®¡ç®—ä»»åŠ¡å¥–åŠ±
#         task_reward = self._calculate_task_reward(obs, action)
        
#         # æ£€æŸ¥ä»»åŠ¡å®Œæˆ
#         task_success = self._check_task_success()
        
#         # æ›´æ–°doneçŠ¶æ€
#         if task_success:
#             done = True
#         elif self.current_step >= self.max_steps:
#             done = True
        
#         # å¤„ç†è§‚æµ‹
#         processed_obs = self._process_observation(obs)
        
#         # æ›´æ–°info
#         task_info = self.get_task_info()
#         info.update(task_info)
        
#         # è§†é¢‘å½•åˆ¶
#         if self.video_recorder and self.video_recorder.is_recording:
#             step_info = {
#                 'step': self.current_step,
#                 'reward': task_reward,
#                 'task_progress': task_info.get('task_progress', 0.0),
#                 'strawberries_on_plate': task_info.get('strawberries_on_plate', 0),
#                 'task_success': task_success
#             }
            
#             # å½•åˆ¶å½“å‰å¸§
#             if "frontview_image" in processed_obs:
#                 self.video_recorder.add_frame(processed_obs["frontview_image"], step_info)
        
#         return processed_obs, task_reward, done, info
    
#     def _get_expected_action_dim(self):
#         """è·å–æœŸæœ›çš„åŠ¨ä½œç»´åº¦"""
#         # ä½¿ç”¨å®é™…æœºå™¨äººçš„åŠ¨ä½œç»´åº¦
#         return getattr(self, 'actual_action_dim', self.robot_dof + 1)
    
#     def _adjust_action_dimensions(self, action):
#         """è°ƒæ•´åŠ¨ä½œç»´åº¦ä»¥åŒ¹é…æœºå™¨äºº"""
#         expected_dim = self._get_expected_action_dim()
        
#         if action is None:
#             return np.zeros(expected_dim, dtype=np.float32)
        
#         action = np.array(action, dtype=np.float32)
        
#         if len(action) < expected_dim:
#             # å¦‚æœåŠ¨ä½œç»´åº¦ä¸è¶³ï¼Œè¡¥é›¶
#             padded_action = np.zeros(expected_dim, dtype=np.float32)
#             padded_action[:len(action)] = action
#             return padded_action
#         elif len(action) > expected_dim:
#             # å¦‚æœåŠ¨ä½œç»´åº¦è¿‡å¤šï¼Œæˆªæ–­
#             return action[:expected_dim]
#         else:
#             return action
    
#     def start_episode_recording(self, episode_id: int):
#         """å¼€å§‹episodeå½•åˆ¶"""
#         if self.video_recorder:
#             return self.video_recorder.start_episode_recording(episode_id, f"strawberry_{self.robot_name.lower()}")
#         return False
    
#     def stop_episode_recording(self):
#         """åœæ­¢episodeå½•åˆ¶"""
#         if self.video_recorder:
#             self.video_recorder.stop_episode_recording()
    
#     def _setup_virtual_object_positions(self):
#         """è®¾ç½®è™šæ‹Ÿç‰©ä½“ä½ç½®ï¼ˆåŸºäºæ¡Œé¢åæ ‡ï¼‰"""
#         try:
#             # è·å–æ¡Œé¢ä¸­å¿ƒä½ç½®
#             table_center = self.table_offset
            
#             # è™šæ‹Ÿç›˜å­ä½ç½®ï¼ˆæ¡Œé¢åº•éƒ¨ä¸­å¤®ï¼‰
#             self.virtual_plate_pos = np.array([
#                 table_center[0], 
#                 table_center[1] - 0.25, 
#                 table_center[2] + 0.01  # ç¨å¾®æŠ¬é«˜é¿å…ç©¿é€
#             ])
            
#             # è™šæ‹Ÿè‰è“ä½ç½®ï¼ˆæ¡Œé¢ä¸Šæ–¹ï¼Œåˆ†æ•£åˆ†å¸ƒï¼‰
#             self.virtual_strawberry_positions = [
#                 table_center + np.array([-0.15, 0.1, 0.03]),   # å·¦ä¸Š
#                 table_center + np.array([0.15, 0.15, 0.03]),   # å³ä¸Š  
#                 table_center + np.array([0.0, 0.05, 0.03])     # ä¸­é—´
#             ]
            
#             # è™šæ‹Ÿç»¿çƒä½ç½®ï¼ˆæ¡Œé¢å››å‘¨ï¼‰
#             self.virtual_green_ball_positions = [
#                 table_center + np.array([-0.2, -0.1, 0.03]),   # å·¦
#                 table_center + np.array([0.2, -0.05, 0.03]),   # å³
#                 table_center + np.array([-0.1, 0.25, 0.03]),   # ä¸Š
#                 table_center + np.array([0.1, -0.15, 0.03])    # ä¸‹
#             ]
            
#             print(f"ğŸ“ è™šæ‹Ÿç‰©ä½“ä½ç½®å·²è®¾ç½®ï¼ŒåŸºäºæ¡Œé¢ä¸­å¿ƒ: {table_center}")
#             print(f"   ç›˜å­åŒºåŸŸ: {self.virtual_plate_pos}")
#             print(f"   è‰è“åŒºåŸŸ: æ¡Œé¢ä¸Šæ–¹ï¼ˆ3ä¸ªä½ç½®ï¼‰")
#             print(f"   ç»¿çƒåŒºåŸŸ: æ¡Œé¢å››å‘¨ï¼ˆ4ä¸ªä½ç½®ï¼‰")
            
#         except Exception as e:
#             print(f"âš ï¸ è™šæ‹Ÿç‰©ä½“ä½ç½®è®¾ç½®å¤±è´¥: {e}")
#             # ä½¿ç”¨é»˜è®¤ä½ç½®
#             self.virtual_plate_pos = np.array([0.0, -0.25, 0.81])
#             self.virtual_strawberry_positions = [
#                 np.array([-0.1, 0.1, 0.83]),
#                 np.array([0.1, 0.1, 0.83]),
#                 np.array([0.0, 0.2, 0.83])
#             ]
#             self.virtual_green_ball_positions = [
#                 np.array([-0.2, 0.0, 0.83]),
#                 np.array([0.2, 0.0, 0.83]),
#                 np.array([0.0, -0.1, 0.83]),
#                 np.array([0.0, 0.3, 0.83])
#             ]
#             print(f"   ä½¿ç”¨é»˜è®¤è™šæ‹Ÿä½ç½®")
    
#     def _calculate_task_reward(self, obs, action):
#         """è®¡ç®—è‰è“æ‹£é€‰ä»»åŠ¡å¥–åŠ± - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ›´å®¹æ˜“æˆåŠŸ"""
#         reward = 0.0
        
#         try:
#             # è·å–æœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨ä½ç½®
#             eef_pos = obs.get("robot0_eef_pos", np.array([0.5, 0.0, 0.8]))
#             gripper_qpos = obs.get("robot0_gripper_qpos", np.array([0.0, 0.0]))
#             gripper_openness = gripper_qpos[0] if len(gripper_qpos) > 0 else 0.0
            
#             # æ ‡å‡†åŒ–å¤¹çˆªçŠ¶æ€ (0=å…³é—­, 1=å¼€å¯)
#             gripper_normalized = np.clip(gripper_openness, 0.0, 1.0)
            
#             # åŸºäºè™šæ‹Ÿç‰©ä½“ä½ç½®çš„ä»»åŠ¡é€»è¾‘
#             if self.held_object is None:
#                 # å¯»æ‰¾æœ€è¿‘çš„æœªæ”¾ç½®è‰è“
#                 min_dist = float('inf')
#                 target_strawberry_idx = -1
                
#                 for i, strawberry_pos in enumerate(self.virtual_strawberry_positions):
#                     if i not in self.placed_strawberries:
#                         dist = np.linalg.norm(eef_pos - strawberry_pos)
#                         if dist < min_dist:
#                             min_dist = dist
#                             target_strawberry_idx = i
                
#                 if target_strawberry_idx >= 0:
#                     # å¥–åŠ±æ¥è¿‘ç›®æ ‡è‰è“ - å¢åŠ æƒé‡
#                     approach_reward = 5.0 * (1.0 - np.tanh(3.0 * min_dist))
#                     reward += approach_reward
                    
#                     # æ£€æŸ¥"æŠ“å–"æˆåŠŸ - æ”¾å®½æ¡ä»¶
#                     grab_distance_threshold = 0.08  # å¢åŠ åˆ°8cm
#                     grab_gripper_threshold = 0.3    # æ”¾å®½å¤¹çˆªé˜ˆå€¼
                    
#                     if min_dist < grab_distance_threshold and gripper_normalized < grab_gripper_threshold:
#                         self.held_object = f"strawberry_{target_strawberry_idx}"
#                         reward += 20.0  # å¢åŠ æŠ“å–å¥–åŠ±
#                         print(f"   ğŸ“ æŠ“å–è‰è“ {target_strawberry_idx}! (è·ç¦»: {min_dist:.3f}m)")
            
#             else:
#                 # å·²ç»"æŠ“ç€"è‰è“ï¼Œå¥–åŠ±æ¥è¿‘ç›˜å­
#                 dist_to_plate = np.linalg.norm(eef_pos[:2] - self.virtual_plate_pos[:2])
#                 approach_reward = 5.0 * (1.0 - np.tanh(3.0 * dist_to_plate))
#                 reward += approach_reward
                
#                 # æ£€æŸ¥"æ”¾ç½®"æˆåŠŸ - æ”¾å®½æ¡ä»¶
#                 place_distance_threshold = 0.15  # å¢åŠ åˆ°15cm
#                 place_height_threshold = 0.12    # æ”¾å®½é«˜åº¦é™åˆ¶
#                 place_gripper_threshold = 0.5    # æ”¾å®½å¤¹çˆªå¼€å¯é˜ˆå€¼
                
#                 height_diff = eef_pos[2] - self.virtual_plate_pos[2]
                
#                 if (dist_to_plate < place_distance_threshold and 
#                     height_diff < place_height_threshold and 
#                     gripper_normalized > place_gripper_threshold):
                    
#                     # è§£æheld_objectè·å–è‰è“ç´¢å¼•
#                     strawberry_idx = int(self.held_object.split('_')[-1])
#                     self.placed_strawberries.add(strawberry_idx)
#                     self.held_object = None
#                     reward += 30.0  # å¢åŠ æ”¾ç½®å¥–åŠ±
#                     print(f"   ğŸ½ï¸ æ”¾ç½®è‰è“åˆ°ç›˜å­ä¸Š! ({len(self.placed_strawberries)}/3)")
#                     print(f"      è·ç¦»: {dist_to_plate:.3f}m, é«˜åº¦å·®: {height_diff:.3f}m")
            
#             # ä»»åŠ¡å®Œæˆé¢å¤–å¥–åŠ±
#             if len(self.placed_strawberries) == 3 and not self.task_complete:
#                 reward += 100.0
#                 self.task_complete = True
#                 print("ğŸ‰ æ‰€æœ‰è‰è“éƒ½å·²æ”¾ç½®å®Œæˆ!")
            
#             # æ·»åŠ åŸºäºRGBå›¾åƒçš„è§†è§‰å¥–åŠ±
#             if "frontview_image" in obs:
#                 visual_reward = self._calculate_visual_reward(obs["frontview_image"])
#                 reward += visual_reward
            
#             # æ·»åŠ å¹³æ»‘çš„è¿åŠ¨å¥–åŠ±ï¼Œé¿å…æœºå™¨äººåœæ»
#             if action is not None:
#                 action_magnitude = np.linalg.norm(action)
#                 if action_magnitude > 0.01:  # é¼“åŠ±æœ‰æ„ä¹‰çš„è¿åŠ¨
#                     reward += 0.1
            
#             return reward
            
#         except Exception as e:
#             print(f"âš ï¸ å¥–åŠ±è®¡ç®—é”™è¯¯: {e}")
#             return 0.0
    
#     def _calculate_visual_reward(self, rgb_image):
#         """åŸºäºè§†è§‰ä¿¡æ¯è®¡ç®—å¥–åŠ±ï¼ˆæ£€æµ‹çº¢è‰²åŒºåŸŸæ¨¡æ‹Ÿè‰è“è¯†åˆ«ï¼‰"""
#         try:
#             if rgb_image is None or len(rgb_image.shape) != 3:
#                 return 0.0
            
#             # è½¬æ¢ä¸ºHSVè¿›è¡Œçº¢è‰²æ£€æµ‹
#             hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)
            
#             # çº¢è‰²èŒƒå›´ï¼ˆæ¨¡æ‹Ÿè‰è“é¢œè‰²ï¼‰
#             lower_red1 = np.array([0, 50, 50])
#             upper_red1 = np.array([10, 255, 255])
#             lower_red2 = np.array([170, 50, 50])
#             upper_red2 = np.array([180, 255, 255])
            
#             mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
#             mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
#             red_mask = mask1 + mask2
            
#             # è®¡ç®—çº¢è‰²åƒç´ æ¯”ä¾‹
#             red_pixels = np.sum(red_mask > 0)
#             total_pixels = rgb_image.shape[0] * rgb_image.shape[1]
#             red_ratio = red_pixels / total_pixels
            
#             # å¦‚æœæ£€æµ‹åˆ°çº¢è‰²åŒºåŸŸï¼Œç»™äºˆå°é¢è§†è§‰å¥–åŠ±
#             if red_ratio > 0.005:  # è‡³å°‘0.5%çš„çº¢è‰²åƒç´ 
#                 return 0.2 * min(red_ratio * 10, 1.0)
            
#             return 0.0
            
#         except Exception as e:
#             return 0.0
    
#     def _check_task_success(self):
#         """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æˆåŠŸå®Œæˆ"""
#         return len(self.placed_strawberries) == 3
    
#     def _process_observation(self, obs):
#         """å¤„ç†è§‚æµ‹æ•°æ®ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®"""
#         processed = {}
        
#         try:
#             # å›¾åƒæ•°æ® - å°è¯•å¤šç§å¯èƒ½çš„é”®å
#             image_found = False
#             for img_key in ["frontview_image", "agentview_image", "image"]:
#                 if img_key in obs and obs[img_key] is not None:
#                     img = obs[img_key]
#                     # RoboSuiteå›¾åƒå¯èƒ½éœ€è¦ç¿»è½¬
#                     if len(img.shape) == 3:
#                         img = img[::-1]  
#                     processed["frontview_image"] = img.astype(np.uint8)
#                     image_found = True
#                     break
            
#             if not image_found:
#                 processed["frontview_image"] = np.zeros((480, 640, 3), dtype=np.uint8)
#                 print("âš ï¸ æœªæ‰¾åˆ°å›¾åƒæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å›¾åƒ")
            
#             # æœºå™¨äººçŠ¶æ€ - å®‰å…¨åœ°è·å–å„ç§çŠ¶æ€ä¿¡æ¯
#             self._process_robot_state(obs, processed)
            
#             return processed
            
#         except Exception as e:
#             print(f"âš ï¸ è§‚æµ‹å¤„ç†é”™è¯¯: {e}")
#             return self._get_default_observation()
    
#     def _process_robot_state(self, obs, processed):
#         """å®‰å…¨åœ°å¤„ç†æœºå™¨äººçŠ¶æ€æ•°æ®ï¼Œé€‚é…ä¸åŒDOFçš„æœºå™¨äºº"""
#         try:
#             # å…³èŠ‚ä½ç½® - é€‚é…ä¸åŒDOF
#             joint_keys = ["robot0_joint_pos", "joint_pos", "qpos"]
#             for key in joint_keys:
#                 if key in obs and obs[key] is not None:
#                     joint_pos = np.array(obs[key], dtype=np.float32)
#                     # ç¡®ä¿å…³èŠ‚æ•°é‡ä¸æœŸæœ›çš„DOFåŒ¹é…
#                     if len(joint_pos) < self.robot_dof:
#                         # å¦‚æœå…³èŠ‚æ•°é‡ä¸è¶³ï¼Œè¡¥é›¶
#                         padded_joints = np.zeros(self.robot_dof, dtype=np.float32)
#                         padded_joints[:len(joint_pos)] = joint_pos
#                         processed["robot0_joint_pos"] = padded_joints
#                     elif len(joint_pos) > self.robot_dof:
#                         # å¦‚æœå…³èŠ‚æ•°é‡è¿‡å¤šï¼Œæˆªå–å‰Nä¸ª
#                         processed["robot0_joint_pos"] = joint_pos[:self.robot_dof]
#                     else:
#                         processed["robot0_joint_pos"] = joint_pos
#                     break
#             else:
#                 processed["robot0_joint_pos"] = np.zeros(self.robot_dof, dtype=np.float32)
            
#             # æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®
#             eef_keys = ["robot0_eef_pos", "eef_pos", "end_effector_pos"]
#             for key in eef_keys:
#                 if key in obs and obs[key] is not None:
#                     processed["robot0_eef_pos"] = np.array(obs[key], dtype=np.float32)
#                     break
#             else:
#                 processed["robot0_eef_pos"] = np.array([0.5, 0.0, 0.8], dtype=np.float32)
            
#             # æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€
#             eef_quat_keys = ["robot0_eef_quat", "eef_quat", "end_effector_quat"]
#             for key in eef_quat_keys:
#                 if key in obs and obs[key] is not None:
#                     processed["robot0_eef_quat"] = np.array(obs[key], dtype=np.float32)
#                     break
#             else:
#                 processed["robot0_eef_quat"] = np.array([0, 0, 0, 1], dtype=np.float32)
            
#             # å¤¹çˆªçŠ¶æ€
#             gripper_keys = ["robot0_gripper_qpos", "gripper_qpos", "gripper_pos"]
#             for key in gripper_keys:
#                 if key in obs and obs[key] is not None:
#                     processed["robot0_gripper_qpos"] = np.array(obs[key], dtype=np.float32)
#                     break
#             else:
#                 processed["robot0_gripper_qpos"] = np.zeros(2, dtype=np.float32)
            
#         except Exception as e:
#             print(f"âš ï¸ æœºå™¨äººçŠ¶æ€å¤„ç†é”™è¯¯: {e}")
#             # æä¾›é»˜è®¤å€¼
#             processed.update({
#                 "robot0_joint_pos": np.zeros(self.robot_dof, dtype=np.float32),
#                 "robot0_eef_pos": np.array([0.5, 0.0, 0.8], dtype=np.float32),
#                 "robot0_eef_quat": np.array([0, 0, 0, 1], dtype=np.float32),
#                 "robot0_gripper_qpos": np.zeros(2, dtype=np.float32)
#             })
    
#     def get_task_info(self):
#         """è·å–ä»»åŠ¡ä¿¡æ¯"""
#         return {
#             "strawberries_picked": 1 if self.held_object and "strawberry" in self.held_object else 0,
#             "strawberries_on_plate": len(self.placed_strawberries),
#             "total_strawberries": 3,
#             "task_success": self.task_complete,
#             "task_progress": len(self.placed_strawberries) / 3.0,
#             "current_step": self.current_step,
#             "max_steps": self.max_steps,
#             "held_object": self.held_object,
#             "robot_dof": self.robot_dof
#         }
    
#     def _get_default_observation(self):
#         """è·å–é»˜è®¤è§‚æµ‹"""
#         return {
#             "frontview_image": np.zeros((480, 640, 3), dtype=np.uint8),
#             "robot0_joint_pos": np.zeros(self.robot_dof, dtype=np.float32),
#             "robot0_eef_pos": np.array([0.5, 0.0, 0.8], dtype=np.float32),
#             "robot0_eef_quat": np.array([0, 0, 0, 1], dtype=np.float32),
#             "robot0_gripper_qpos": np.zeros(2, dtype=np.float32)
#         }
    
#     def close(self):
#         """å…³é—­ç¯å¢ƒå’Œæ¸…ç†èµ„æº"""
#         # åœæ­¢è§†é¢‘å½•åˆ¶
#         if self.video_recorder:
#             self.video_recorder.cleanup()
        
#         # å…³é—­åŸºç¡€ç¯å¢ƒ
#         if hasattr(self, 'env') and self.env is not None:
#             try:
#                 self.env.close()
#                 print("ğŸ”’ è‰è“æ‹£é€‰ç¯å¢ƒå·²å…³é—­")
#             except:
#                 pass

# # ==================== æ•°æ®é€‚é…å™¨ ====================

# class RoboSuiteGR00TAdapter:
#     """RoboSuiteä¸GR00Tä¹‹é—´çš„æ•°æ®é€‚é…å™¨ï¼Œæ”¯æŒä¸åŒDOFçš„æœºå™¨äºº"""
    
#     def __init__(self, robot_name: str = "SO100", robot_dof: int = None):
#         self.robot_name = robot_name
#         self.robot_dof = robot_dof if robot_dof is not None else detect_robot_dof(robot_name)
#         self.processed_observations = 0
#         self.processed_actions = 0
#         print(f"ğŸ”„ æ•°æ®é€‚é…å™¨åˆå§‹åŒ–ï¼Œæœºå™¨äºº: {robot_name} ({self.robot_dof}DOF)")
        
#     def update_robot_info(self, robot_dof: int, action_dim: int):
#         """æ›´æ–°æœºå™¨äººä¿¡æ¯"""
#         self.robot_dof = robot_dof
#         self.action_dim = action_dim
#         print(f"ğŸ”„ é€‚é…å™¨æ›´æ–°: DOF={robot_dof}, åŠ¨ä½œç»´åº¦={action_dim}")
        
#     def robosuite_to_groot_obs(self, obs: Dict[str, np.ndarray], 
#                               task_description: str = "Pick red strawberries and place them on the white plate") -> Dict[str, Any]:
#         """å°†RoboSuiteè§‚æµ‹è½¬æ¢ä¸ºGR00Tæ ¼å¼ï¼Œé€‚é…ä¸åŒDOF"""
#         try:
#             groot_obs = {}
            
#             # 1. è§†è§‰æ•°æ®
#             camera_key = "frontview_image"
#             if camera_key in obs and obs[camera_key] is not None:
#                 img = obs[camera_key]
#                 if img.shape[:2] != (480, 640):
#                     img = cv2.resize(img, (640, 480))
#                 if len(img.shape) == 3 and img.shape[2] == 3:
#                     groot_obs["video.webcam"] = img[np.newaxis, :, :, :].astype(np.uint8)
#                 else:
#                     groot_obs["video.webcam"] = np.zeros((1, 480, 640, 3), dtype=np.uint8)
#             else:
#                 groot_obs["video.webcam"] = np.zeros((1, 480, 640, 3), dtype=np.uint8)
            
#             # 2. æœºå™¨äººå…³èŠ‚çŠ¶æ€ - é€‚é…ä¸åŒDOF
#             if "robot0_joint_pos" in obs and obs["robot0_joint_pos"] is not None:
#                 joint_pos = obs["robot0_joint_pos"]
                
#                 # æ ¹æ®å®é™…DOFå¤„ç†å…³èŠ‚çŠ¶æ€
#                 if len(joint_pos) >= self.robot_dof:
#                     # ä½¿ç”¨å‰Nä¸ªå…³èŠ‚ï¼ˆN=robot_dofï¼‰
#                     groot_obs["state.single_arm"] = joint_pos[:self.robot_dof][np.newaxis, :].astype(np.float32)
#                 else:
#                     # å¦‚æœå…³èŠ‚æ•°ä¸è¶³ï¼Œè¡¥é›¶
#                     padded_joints = np.zeros(self.robot_dof)
#                     padded_joints[:len(joint_pos)] = joint_pos
#                     groot_obs["state.single_arm"] = padded_joints[np.newaxis, :].astype(np.float32)
#             else:
#                 groot_obs["state.single_arm"] = np.zeros((1, self.robot_dof), dtype=np.float32)
            
#             # 3. å¤¹çˆªçŠ¶æ€
#             if "robot0_gripper_qpos" in obs and obs["robot0_gripper_qpos"] is not None:
#                 gripper_pos = obs["robot0_gripper_qpos"]
#                 if len(gripper_pos) > 0:
#                     normalized_gripper = np.clip(gripper_pos[0], 0.0, 1.0)
#                     groot_obs["state.gripper"] = np.array([[normalized_gripper]], dtype=np.float32)
#                 else:
#                     groot_obs["state.gripper"] = np.zeros((1, 1), dtype=np.float32)
#             else:
#                 groot_obs["state.gripper"] = np.zeros((1, 1), dtype=np.float32)
            
#             # 4. ä»»åŠ¡æè¿°
#             groot_obs["annotation.human.task_description"] = [task_description]
            
#             self.processed_observations += 1
#             return groot_obs
            
#         except Exception as e:
#             print(f"âš ï¸ è§‚æµ‹è½¬æ¢é”™è¯¯: {e}")
#             return self._get_default_groot_obs()
    
#     def groot_to_robosuite_action(self, groot_action: Dict[str, np.ndarray]) -> np.ndarray:
#         """å°†GR00TåŠ¨ä½œè½¬æ¢ä¸ºRoboSuiteæ ¼å¼ï¼Œé€‚é…ä¸åŒæœºå™¨äºº"""
#         try:
#             world_vector = groot_action.get('world_vector', np.zeros((1, 3)))[0]
#             rotation_delta = groot_action.get('rotation_delta', np.zeros((1, 3)))[0]
#             gripper_action = groot_action.get('gripper_closedness_action', np.zeros((1, 1)))[0][0]
            
#             # æ ¹æ®æœºå™¨äººDOFè°ƒæ•´åŠ¨ä½œç»´åº¦
#             if self.robot_dof == 5:  # SO100ç­‰5DOFæœºå™¨äºº
#                 action = np.zeros(6, dtype=np.float32)  # 5DOF + å¤¹çˆª
#                 action[0:3] = np.clip(world_vector * 0.02, -0.1, 0.1)
#                 action[3:5] = np.clip(rotation_delta[:2] * 0.01, -0.1, 0.1)
#                 action[5] = np.clip(gripper_action, -1.0, 1.0)
#             else:  # æ ‡å‡†7DOFæœºå™¨äºº
#                 action = np.zeros(7, dtype=np.float32)
#                 action[0:3] = np.clip(world_vector * 0.02, -0.1, 0.1)
#                 action[3:6] = np.clip(rotation_delta * 0.01, -0.1, 0.1)
#                 action[6] = np.clip(gripper_action, -1.0, 1.0)
            
#             self.processed_actions += 1
#             return action
            
#         except Exception as e:
#             print(f"âš ï¸ åŠ¨ä½œè½¬æ¢é”™è¯¯: {e}")
#             if self.robot_dof == 5:
#                 return np.zeros(6, dtype=np.float32)
#             else:
#                 return np.zeros(7, dtype=np.float32)
    
#     def _get_default_groot_obs(self) -> Dict[str, Any]:
#         return {
#             "video.webcam": np.zeros((1, 480, 640, 3), dtype=np.uint8),
#             "state.single_arm": np.zeros((1, self.robot_dof), dtype=np.float32),
#             "state.gripper": np.zeros((1, 1), dtype=np.float32),
#             "annotation.human.task_description": ["Pick red strawberries and place them on the white plate"]
#         }
    
#     def get_stats(self) -> Dict[str, int]:
#         return {
#             "observations_processed": self.processed_observations,
#             "actions_processed": self.processed_actions,
#             "robot_name": self.robot_name,
#             "robot_dof": self.robot_dof
#         }

# # ==================== GR00Tå®¢æˆ·ç«¯ ====================

# class GR00TClient:
#     """GR00Tå®¢æˆ·ç«¯ï¼Œæ”¯æŒä¸åŒDOFçš„æœºå™¨äºº"""
    
#     def __init__(self, host: str = "localhost", port: int = 5555, robot_name: str = "SO100"):
#         self.host = host
#         self.port = port
#         self.robot_name = robot_name
#         self.robot_dof = detect_robot_dof(robot_name)
#         self.client = None
#         self.adapter = RoboSuiteGR00TAdapter(robot_name=robot_name, robot_dof=self.robot_dof)
#         self.is_connected = False
        
#         self.total_calls = 0
#         self.successful_calls = 0
#         self.total_latency = 0.0
        
#         print(f"ğŸ¤– GR00Tå®¢æˆ·ç«¯åˆå§‹åŒ–: {robot_name} ({self.robot_dof}DOF)")
    
#     def update_robot_info(self, robot_dof: int, action_dim: int):
#         """æ›´æ–°æœºå™¨äººä¿¡æ¯"""
#         self.robot_dof = robot_dof
#         self.adapter.update_robot_info(robot_dof, action_dim)
    
#     def connect(self) -> bool:
#         """è¿æ¥åˆ°GR00TæœåŠ¡"""
#         if not GROOT_CLIENT_AVAILABLE:
#             print("âŒ GR00Tå®¢æˆ·ç«¯åº“ä¸å¯ç”¨")
#             return False
        
#         try:
#             print(f"ğŸ”— è¿æ¥GR00TæœåŠ¡: {self.host}:{self.port}")
            
#             self.client = RobotInferenceClient(host=self.host, port=self.port)
            
#             # è¿æ¥æµ‹è¯•
#             test_obs = self.adapter._get_default_groot_obs()
#             start_time = time.time()
#             test_result = self.client.get_action(test_obs)
#             latency = time.time() - start_time
            
#             if test_result is not None:
#                 self.is_connected = True
#                 print(f"âœ… GR00Tè¿æ¥æˆåŠŸï¼å»¶è¿Ÿ: {latency:.3f}s")
#                 print(f"   æœºå™¨äººé…ç½®: {self.robot_name} ({self.robot_dof}DOF)")
#                 return True
#             else:
#                 print("âŒ GR00Tæµ‹è¯•å¤±è´¥")
#                 return False
                
#         except Exception as e:
#             print(f"âŒ GR00Tè¿æ¥å¤±è´¥: {e}")
#             return False
    
#     def get_action(self, observation: Dict[str, np.ndarray], 
#                    task_description: str = "Pick red strawberries and place them on the white plate") -> Optional[np.ndarray]:
#         """è·å–åŠ¨ä½œ"""
#         if not self.is_connected:
#             return None
        
#         self.total_calls += 1
#         start_time = time.time()
        
#         try:
#             groot_obs = self.adapter.robosuite_to_groot_obs(observation, task_description)
#             groot_action = self.client.get_action(groot_obs)
            
#             latency = time.time() - start_time
#             self.total_latency += latency
            
#             if groot_action is not None:
#                 self.successful_calls += 1
#                 robosuite_action = self.adapter.groot_to_robosuite_action(groot_action)
#                 return robosuite_action
#             else:
#                 return None
                
#         except Exception as e:
#             latency = time.time() - start_time
#             self.total_latency += latency
#             if self.total_calls % 10 == 0:
#                 print(f"âš ï¸ åŠ¨ä½œé¢„æµ‹é”™è¯¯: {e}")
#             return None
    
#     def get_stats(self) -> Dict[str, Any]:
#         return {
#             "total_calls": self.total_calls,
#             "successful_calls": self.successful_calls,
#             "success_rate": self.successful_calls / self.total_calls if self.total_calls > 0 else 0,
#             "average_latency": self.total_latency / self.total_calls if self.total_calls > 0 else 0,
#             "robot_name": self.robot_name,
#             "robot_dof": self.robot_dof,
#             "adapter_stats": self.adapter.get_stats()
#         }

# # ==================== ä¸»æ¥å£ç±» ====================

# class StrawberryPickPlaceInterface:
#     """è‰è“æ‹£é€‰ä¸»æ¥å£ï¼Œæ”¯æŒSO100ç­‰å¤šç§æœºå™¨äºº"""
    
#     def __init__(self, config: ExperimentConfig):
#         self.config = config
#         self.environment = None
#         self.groot_client = None
        
#         print("ğŸ“ åˆå§‹åŒ–è‰è“æ‹£é€‰æ¥å£")
#         print(f"   æœºå™¨äºº: {config.robot}")
#         print(f"   DOF: {detect_robot_dof(config.robot)}")
#         print(f"   ç¯å¢ƒ: è™šæ‹Ÿç‰©ä½“ç¯å¢ƒï¼ˆ3è‰è“+4ç»¿çƒ+1ç›˜å­ï¼‰")
#         print(f"   GR00T: {config.groot_host}:{config.groot_port}")
        
#         self._create_environment()
#         self._create_groot_client()
    
#     def _create_environment(self):
#         """åˆ›å»ºç¯å¢ƒ"""
#         if not ROBOSUITE_AVAILABLE:
#             raise ImportError("éœ€è¦å®‰è£…RoboSuite")
        
#         try:
#             print("ğŸ—ï¸ åˆ›å»ºè‰è“æ‹£é€‰ç¯å¢ƒ...")
            
#             self.environment = StrawberryPickPlaceEnvironment(
#                 robots=self.config.robot,
#                 robot_xml_path=self.config.robot_xml_path,
#                 has_renderer=self.config.enable_gui,
#                 has_offscreen_renderer=True,
#                 use_camera_obs=True,
#                 camera_names="frontview",
#                 camera_heights=480,
#                 camera_widths=640,
#                 control_freq=20,
#                 horizon=self.config.max_steps_per_episode * 2,
#                 ignore_done=True,
#                 enable_video_recording=self.config.enable_video_recording,
#                 video_output_dir=self.config.video_output_dir
#             )
            
#             print("âœ… è‰è“æ‹£é€‰ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼")
            
#         except Exception as e:
#             print(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
#             raise
    
#     def _create_groot_client(self):
#         """åˆ›å»ºGR00Tå®¢æˆ·ç«¯"""
#         self.groot_client = GR00TClient(self.config.groot_host, self.config.groot_port, self.config.robot)
        
#         # å¦‚æœç¯å¢ƒå·²åˆ›å»ºï¼Œæ›´æ–°æœºå™¨äººä¿¡æ¯
#         if hasattr(self, 'environment') and self.environment:
#             actual_dof = getattr(self.environment, 'actual_robot_dof', self.groot_client.robot_dof)
#             actual_action_dim = getattr(self.environment, 'actual_action_dim', actual_dof + 1)
#             self.groot_client.update_robot_info(actual_dof, actual_action_dim)
    
#     def connect_groot(self) -> bool:
#         """è¿æ¥GR00T"""
#         return self.groot_client.connect()
    
#     def run_episode(self, episode_id: int) -> Dict[str, Any]:
#         """è¿è¡Œå•ä¸ªepisodeå¹¶å½•åˆ¶è§†é¢‘"""
#         print(f"\nğŸ¯ Episode {episode_id + 1}")
#         print(f"   ä»»åŠ¡: æ‹£é€‰3ä¸ªçº¢è‰²è‰è“å¹¶æ”¾ç½®åˆ°ç™½è‰²ç›˜å­ä¸Š")
#         print(f"   æœºå™¨äºº: {self.config.robot} ({detect_robot_dof(self.config.robot)}DOF)")
        
#         # å¼€å§‹è§†é¢‘å½•åˆ¶
#         video_recording_success = False
#         if self.config.enable_video_recording:
#             video_recording_success = self.environment.start_episode_recording(episode_id)
        
#         # é‡ç½®ç¯å¢ƒ
#         obs = self.environment.reset()
        
#         episode_stats = {
#             "episode_id": episode_id,
#             "steps": 0,
#             "total_reward": 0.0,
#             "task_success": False,
#             "groot_calls": 0,
#             "groot_successes": 0,
#             "start_time": time.time(),
#             "video_recorded": video_recording_success,
#             "robot_name": self.config.robot,
#             "robot_dof": detect_robot_dof(self.config.robot)
#         }
        
#         done = False
#         step = 0
        
#         print(f"     è¿›åº¦: ", end="", flush=True)
        
#         while not done and step < self.config.max_steps_per_episode:
#             # è·å–GR00TåŠ¨ä½œ
#             action = self.groot_client.get_action(obs)
#             episode_stats["groot_calls"] += 1
            
#             if action is not None:
#                 episode_stats["groot_successes"] += 1
#                 print(".", end="", flush=True)
#             else:
#                 # ä¸ºä¸åŒæœºå™¨äººæä¾›é»˜è®¤åŠ¨ä½œ
#                 expected_action_dim = self.environment._get_expected_action_dim()
#                 action = np.zeros(expected_action_dim, dtype=np.float32)
#                 print("x", end="", flush=True)
            
#             # ç¯å¢ƒæ­¥è¿›
#             obs, reward, done, info = self.environment.step(action)
            
#             episode_stats["steps"] += 1
#             episode_stats["total_reward"] += reward
#             step += 1
            
#             # è·å–ä»»åŠ¡ä¿¡æ¯
#             task_info = self.environment.get_task_info()
            
#             # æ£€æŸ¥ä»»åŠ¡æˆåŠŸ
#             if task_info["task_success"]:
#                 episode_stats["task_success"] = True
#                 print("ğŸ‰", end="", flush=True)
#                 done = True
            
#             # è¿›åº¦æ˜¾ç¤º
#             if step % 20 == 0:
#                 progress = task_info["task_progress"]
#                 print(f"|{progress:.0%}", end="", flush=True)
        
#         # åœæ­¢è§†é¢‘å½•åˆ¶
#         if self.config.enable_video_recording:
#             self.environment.stop_episode_recording()
        
#         episode_stats["duration"] = time.time() - episode_stats["start_time"]
        
#         print()  # æ¢è¡Œ
        
#         # æ‰“å°episodeç»“æœ
#         self._print_episode_result(episode_stats)
        
#         return episode_stats
    
#     def run_experiment(self) -> List[Dict[str, Any]]:
#         """è¿è¡Œå®Œæ•´å®éªŒ"""
#         print(f"\nğŸš€ å¼€å§‹è‰è“æ‹£é€‰å®éªŒ")
#         print("=" * 60)
        
#         if not self.connect_groot():
#             print("âŒ éœ€è¦å…ˆè¿æ¥GR00TæœåŠ¡")
#             return []
        
#         results = []
        
#         try:
#             for i in range(self.config.num_episodes):
#                 result = self.run_episode(i)
#                 results.append(result)
                
#                 if i < self.config.num_episodes - 1:
#                     time.sleep(0.5)
            
#             self._print_summary(results)
            
#         except KeyboardInterrupt:
#             print("\nâš ï¸ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
#         except Exception as e:
#             print(f"\nâŒ å®éªŒå¼‚å¸¸: {e}")
        
#         return results
    
#     def _print_episode_result(self, stats: Dict[str, Any]):
#         """æ‰“å°episodeç»“æœ"""
#         status = "âœ… æˆåŠŸ" if stats["task_success"] else "âŒ å¤±è´¥"
#         groot_rate = stats["groot_successes"] / stats["groot_calls"] if stats["groot_calls"] > 0 else 0
        
#         print(f"   ç»“æœ: {status}")
#         print(f"   æ­¥æ•°: {stats['steps']}, æ—¶é•¿: {stats['duration']:.1f}s")
#         print(f"   å¥–åŠ±: {stats['total_reward']:.2f}")
#         print(f"   GR00TæˆåŠŸç‡: {groot_rate:.1%} ({stats['groot_successes']}/{stats['groot_calls']})")
        
#         if stats.get("video_recorded", False):
#             print(f"   ğŸ¥ è§†é¢‘å·²å½•åˆ¶")
#         elif self.config.enable_video_recording:
#             print(f"   âš ï¸ è§†é¢‘å½•åˆ¶å¤±è´¥")
    
#     def _print_summary(self, results: List[Dict[str, Any]]):
#         """æ‰“å°å®éªŒæ€»ç»“"""
#         print(f"\nğŸ“Š å®éªŒæ€»ç»“")
#         print("=" * 60)
        
#         if not results:
#             print("âŒ æ²¡æœ‰ç»“æœæ•°æ®")
#             return
        
#         total_episodes = len(results)
#         successful_episodes = sum(1 for r in results if r["task_success"])
#         success_rate = successful_episodes / total_episodes
        
#         avg_steps = np.mean([r["steps"] for r in results])
#         avg_reward = np.mean([r["total_reward"] for r in results])
#         avg_duration = np.mean([r["duration"] for r in results])
        
#         total_groot_calls = sum(r["groot_calls"] for r in results)
#         total_groot_successes = sum(r["groot_successes"] for r in results)
#         groot_success_rate = total_groot_successes / total_groot_calls if total_groot_calls > 0 else 0
        
#         print(f"ğŸ¯ æ€»ä½“è¡¨ç°:")
#         print(f"   ä»»åŠ¡æˆåŠŸç‡: {success_rate:.1%} ({successful_episodes}/{total_episodes})")
#         print(f"   å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
#         print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
#         print(f"   å¹³å‡æ—¶é•¿: {avg_duration:.1f}s")
#         print(f"   GR00TæˆåŠŸç‡: {groot_success_rate:.1%}")
        
#         # GR00Tç»Ÿè®¡
#         groot_stats = self.groot_client.get_stats()
#         print(f"   å¹³å‡å»¶è¿Ÿ: {groot_stats['average_latency']:.3f}s")
        
#         # è§†é¢‘å½•åˆ¶ç»Ÿè®¡
#         if self.config.enable_video_recording:
#             video_episodes = sum(1 for r in results if r.get("video_recorded", False))
#             print(f"   è§†é¢‘å½•åˆ¶: {video_episodes}/{total_episodes} episodes")
        
#         print(f"\nâœ… {self.config.robot}æœºå™¨äººè‰è“æ‹£é€‰æµ‹è¯•å®Œæˆ!")
#         print(f"   æœºå™¨äººé…ç½®: {self.config.robot} ({detect_robot_dof(self.config.robot)}DOF)")
#         print(f"   ç¯å¢ƒ: è™šæ‹Ÿ3è‰è“ + 4ç»¿çƒ + 1ç›˜å­")
#         print(f"   å®Œå…¨åŒ¹é…è®­ç»ƒæ•°æ®é›†ä»»åŠ¡é€»è¾‘")
        
#         if success_rate == 0:
#             print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
#             print(f"   1. æ£€æŸ¥{self.config.robot}æœºå™¨äººæ˜¯å¦æ­£ç¡®æ³¨å†Œ")
#             print(f"   2. éªŒè¯æ§åˆ¶å™¨é…ç½®æ˜¯å¦é€‚é…{detect_robot_dof(self.config.robot)}DOF")
#             print(f"   3. æ£€æŸ¥GR00Tæ¨¡å‹æ˜¯å¦é’ˆå¯¹{self.config.robot}è®­ç»ƒ")
#             print(f"   4. æŸ¥çœ‹è§†é¢‘å½•åˆ¶äº†è§£æœºå™¨äººè¡Œä¸ºæ¨¡å¼")
#         elif success_rate > 0:
#             print(f"\nğŸ‰ {self.config.robot}æœºå™¨äººä»»åŠ¡æˆåŠŸ!")
#             print(f"   æˆåŠŸç‡: {success_rate:.1%}")
#             print(f"   æœºå™¨äººèƒ½å¤Ÿå®Œæˆè‰è“æ‹£é€‰ä»»åŠ¡")
    
#     def close(self):
#         """å…³é—­æ¥å£"""
#         if self.environment:
#             self.environment.close()
#         print("ğŸ”’ æ¥å£å·²å…³é—­")

# # ==================== ä¸»å‡½æ•° ====================

# def main():
#     """ä¸»å‡½æ•° - æ”¯æŒSO100æœºå™¨äººå’Œè§†é¢‘å½•åˆ¶"""
#     print("ğŸ“ RoboSuite-GR00Tè‰è“æ‹£é€‰ç¯å¢ƒæ¥å£")
#     print("æ”¯æŒSO100æœºå™¨äºº(5DOF) + è§†é¢‘å½•åˆ¶ + ä¼˜åŒ–ä»»åŠ¡å¥–åŠ±")
#     print("=" * 60)
    
#     # æ£€æŸ¥ä¾èµ–
#     if not ROBOSUITE_AVAILABLE:
#         print("âŒ éœ€è¦å®‰è£…RoboSuite")
#         return
    
#     if not GROOT_CLIENT_AVAILABLE:
#         print("âŒ éœ€è¦å®‰è£…GR00Tå®¢æˆ·ç«¯åº“")
#         return
    
#     # æ£€æŸ¥SO100æœºå™¨äººXMLè·¯å¾„
#     so100_xml_path = "/root/autodl-tmp/gr00t/SO-ARM100/Simulation/URDF_SO100/SO_5DOF_ARM100_05d.SLDASM/urdf/SO_5DOF_ARM100_05d.SLDASM.xml"
    
#     if os.path.exists(so100_xml_path):
#         print(f"\nâœ… æ‰¾åˆ°SO100æœºå™¨äººXML: {so100_xml_path}")
        
#         # è¯¢é—®ç”¨æˆ·æ˜¯å¦ä½¿ç”¨SO100
#         try:
#             choice = input("ğŸ¤– æ˜¯å¦ä½¿ç”¨SO100æœºå™¨äººï¼Ÿ(y/n, é»˜è®¤y): ").lower().strip()
#             use_so100 = choice in ['', 'y', 'yes', 'æ˜¯']
#         except:
#             use_so100 = True
            
#         if use_so100:
#             robot_type = "SO100"
#             robot_xml = so100_xml_path
#             print(f"   âœ… å°†ä½¿ç”¨SO100æœºå™¨äºº (5DOF)")
#         else:
#             robot_type = "Panda"
#             robot_xml = None
#             print(f"   ğŸ¼ å°†ä½¿ç”¨Pandaæœºå™¨äºº (7DOF)")
#     else:
#         print(f"\nâš ï¸ æœªæ‰¾åˆ°SO100 XMLæ–‡ä»¶ï¼Œä½¿ç”¨Pandaæœºå™¨äºº")
#         print(f"   æœŸæœ›è·¯å¾„: {so100_xml_path}")
#         robot_type = "Panda"
#         robot_xml = None
    
#     # è¯¢é—®æ˜¯å¦å¯ç”¨è§†é¢‘å½•åˆ¶
#     try:
#         video_choice = input("ğŸ¥ æ˜¯å¦å¯ç”¨è§†é¢‘å½•åˆ¶ï¼Ÿ(y/n, é»˜è®¤y): ").lower().strip()
#         enable_video = video_choice in ['', 'y', 'yes', 'æ˜¯']
#     except:
#         enable_video = True
    
#     # è¯¢é—®æ˜¯å¦å¯ç”¨GUI
#     try:
#         gui_choice = input("ğŸ‘ï¸ æ˜¯å¦å¯ç”¨å®æ—¶å¯è§†åŒ–ï¼Ÿ(y/n, é»˜è®¤n): ").lower().strip()
#         enable_gui = gui_choice in ['y', 'yes', 'æ˜¯']
#     except:
#         enable_gui = False
    
#     # å®éªŒé…ç½®
#     config = ExperimentConfig(
#         robot=robot_type,
#         robot_xml_path=robot_xml,
#         num_episodes=3,
#         max_steps_per_episode=200,
#         enable_gui=enable_gui,
#         enable_video_recording=enable_video,
#         video_output_dir=f"./strawberry_{robot_type.lower()}_videos",
#         groot_host="localhost",
#         groot_port=5555
#     )
    
#     print(f"\nğŸ› ï¸ å®éªŒé…ç½®:")
#     print(f"   æœºå™¨äºº: {config.robot} ({detect_robot_dof(config.robot)}DOF)")
#     if config.robot_xml_path:
#         print(f"   XMLè·¯å¾„: {config.robot_xml_path}")
#     print(f"   Episodes: {config.num_episodes}")
#     print(f"   æœ€å¤§æ­¥æ•°: {config.max_steps_per_episode}")
#     print(f"   å¯è§†åŒ–: {'å¯ç”¨' if config.enable_gui else 'ç¦ç”¨'}")
#     print(f"   è§†é¢‘å½•åˆ¶: {'å¯ç”¨' if config.enable_video_recording else 'ç¦ç”¨'}")
#     if config.enable_video_recording:
#         print(f"   è§†é¢‘ç›®å½•: {config.video_output_dir}")
#     print(f"   ä»»åŠ¡: ä¼˜åŒ–çš„è‰è“æ‹£é€‰ï¼ˆé€‚é…{detect_robot_dof(config.robot)}DOFæœºå™¨äººï¼‰")
    
#     # æ˜¾ç¤ºæœºå™¨äººæ³¨å†Œä¿¡æ¯
#     if robot_type == "SO100":
#         create_so100_robot_registration()
    
#     # åˆ›å»ºæ¥å£å¹¶è¿è¡Œå®éªŒ
#     interface = StrawberryPickPlaceInterface(config)
    
#     try:
#         results = interface.run_experiment()
        
#         # ä¿å­˜ç»“æœ
#         if results:
#             timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#             robot_suffix = config.robot.lower()
#             filename = f"strawberry_{robot_suffix}_results_{timestamp}.json"
            
#             with open(filename, 'w') as f:
#                 json.dump(results, f, indent=2, default=str)
            
#             print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {filename}")
            
#             print(f"\nğŸ¯ å®éªŒæ€»ç»“:")
#             successful = sum(1 for r in results if r["task_success"])
#             print(f"   âœ… æˆåŠŸå®Œæˆ: {successful}/{len(results)} episodes")
#             print(f"   ğŸ¤– æœºå™¨äºº: {config.robot} ({detect_robot_dof(config.robot)}DOF)")
#             print(f"   ğŸ“ è‰è“æ‹£é€‰ä»»åŠ¡æµ‹è¯•å®Œæˆ")
#             print(f"   ğŸ”— GR00Tæ¥å£é›†æˆæˆåŠŸ")
            
#             if config.enable_video_recording:
#                 video_count = sum(1 for r in results if r.get("video_recorded", False))
#                 print(f"   ğŸ¥ è§†é¢‘å½•åˆ¶: {video_count}/{len(results)} episodes")
#                 print(f"   ğŸ“ è§†é¢‘ä¿å­˜åœ¨: {config.video_output_dir}")
        
#     except KeyboardInterrupt:
#         print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
#     except Exception as e:
#         print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
#         import traceback
#         traceback.print_exc()
#     finally:
#         interface.close()

# if __name__ == "__main__":
#     main()



# #!/usr/bin/env python3
# """
# RoboSuite-GR00Tè‰è“æ‹£é€‰æ¥å£ (å®˜æ–¹æ³¨å†Œç‰ˆ)
# ä½¿ç”¨å·²åœ¨robosuiteä¸­æ­£å¼æ³¨å†Œçš„æœºå™¨äºº (SO100æˆ–Panda)ã€‚
# """

# import os
# import sys
# import time
# import json
# import numpy as np
# import cv2
# from pathlib import Path
# from dataclasses import dataclass
# from datetime import datetime

# # è®¾ç½®ç¯å¢ƒå˜é‡
# os.environ.setdefault('MUJOCO_GL', 'egl')
# os.environ.setdefault('PYOPENGL_PLATFORM', 'egl')


# # å¯¼å…¥æ£€æŸ¥
# try:
#     import robosuite
#     from robosuite.controllers import load_composite_controller_config
#     ROBOSUITE_AVAILABLE = True
#     print("âœ… RoboSuiteå¯ç”¨")
# except ImportError as e:
#     print(f"âŒ RoboSuiteä¸å¯ç”¨: {e}")
#     ROBOSUITE_AVAILABLE = False

# try:
#     from gr00t.eval.robot import RobotInferenceClient
#     print("âœ… GR00Tå®¢æˆ·ç«¯å¯ç”¨")
# except ImportError as e:
#     print(f"âŒ GR00Tå®¢æˆ·ç«¯ä¸å¯ç”¨: {e}"); sys.exit(1)


# # (VideoRecorderç±»ä¿æŒä¸å˜ï¼Œè¿™é‡Œä¸ºäº†ç®€æ´çœç•¥ï¼Œå®é™…ä»£ç ä¸­è¯·ä¿ç•™)
# class VideoRecorder:
#     def __init__(self, output_dir: str = "./strawberry_videos", fps: int = 20, video_size: tuple = (640, 480), codec: str = 'mp4v'):
#         self.output_dir = Path(output_dir); self.output_dir.mkdir(parents=True, exist_ok=True)
#         self.fps, self.video_size, self.codec = fps, video_size, codec
#         self.is_recording, self.video_writer, self.current_episode, self.frame_count = False, None, 0, 0
#         print(f"ğŸ¥ è§†é¢‘å½•åˆ¶å™¨åˆå§‹åŒ–äº: {self.output_dir}")
#     def start_episode_recording(self, episode_id: int, experiment_name: str):
#         if self.is_recording: self.stop_episode_recording()
#         self.current_episode, self.frame_count = episode_id, 0
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         filename = f"{experiment_name}_episode_{episode_id:03d}_{timestamp}.mp4"
#         self.video_path = self.output_dir / filename
#         fourcc = cv2.VideoWriter_fourcc(*self.codec)
#         self.video_writer = cv2.VideoWriter(str(self.video_path), fourcc, self.fps, self.video_size)
#         if not self.video_writer.isOpened(): print(f"âŒ æ— æ³•åˆ›å»ºè§†é¢‘æ–‡ä»¶: {self.video_path}"); return False
#         self.is_recording = True; print(f"ğŸ¬ å¼€å§‹å½•åˆ¶ Episode {episode_id}: {filename}"); return True
#     def add_frame(self, image: np.ndarray, step_info: dict = None):
#         if not self.is_recording: return
#         processed_image = self._process_image(image, step_info)
#         if self.video_writer and self.video_writer.isOpened(): self.video_writer.write(processed_image); self.frame_count += 1
#     def _process_image(self, image: np.ndarray, step_info: dict = None) -> np.ndarray:
#         if image is None: image = np.zeros((*self.video_size[::-1], 3), dtype=np.uint8)
#         if image.dtype != np.uint8: image = (image * 255).astype(np.uint8) if image.max() <= 1.0 else image.astype(np.uint8)
#         if image.shape[:2] != self.video_size[::-1]: image = cv2.resize(image, self.video_size)
#         if len(image.shape) == 2 or (len(image.shape) == 3 and image.shape[2] == 1): image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
#         elif len(image.shape) == 3 and image.shape[2] == 4: image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)
#         if step_info: image = self._add_info_overlay(image, step_info)
#         return image
#     def _add_info_overlay(self, image: np.ndarray, step_info: dict) -> np.ndarray:
#         overlay = image.copy()
#         font, scale, color, thick, y = cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, 30
#         if 'step' in step_info: cv2.putText(overlay, f"Ep: {self.current_episode} | Step: {step_info['step']}", (10, y), font, scale, color, thick); y += 25
#         if 'task_progress' in step_info: cv2.putText(overlay, f"Progress: {step_info['task_progress']:.0%} | Picked: {step_info.get('strawberries_on_plate', 0)}/3", (10, y), font, scale, color, thick); y += 25
#         if 'reward' in step_info: cv2.putText(overlay, f"Reward: {step_info['reward']:.2f}", (10, y), font, scale, color, thick); y += 25
#         if step_info.get('task_success', False): cv2.putText(overlay, "TASK SUCCESS!", (10, image.shape[0] - 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3)
#         return overlay
#     def stop_episode_recording(self):
#         if not self.is_recording: return
#         print(f"ğŸ¬ åœæ­¢å½•åˆ¶ Episode {self.current_episode} ({self.frame_count} å¸§)")
#         self.is_recording = False
#         if self.video_writer: self.video_writer.release(); self.video_writer = None
#         if hasattr(self, 'video_path') and self.video_path.exists(): print(f"âœ… è§†é¢‘å·²ä¿å­˜: {self.video_path} ({self.video_path.stat().st_size / 1e6:.1f}MB)")
#     def cleanup(self):
#         if self.is_recording: self.stop_episode_recording()


# @dataclass
# class ExperimentConfig:
#     robot: str = "SO100"
#     num_episodes: int = 3
#     max_steps_per_episode: int = 250
#     enable_gui: bool = False
#     enable_video_recording: bool = True
#     video_output_dir: str = "./strawberry_videos"
#     groot_host: str = "localhost"
#     groot_port: int = 5555


# class StrawberryPickPlaceEnvironment:
#     def __init__(self, config: ExperimentConfig):
#         self.config = config
#         self.robot_name = config.robot
#         print(f"ğŸ¤– æ­£åœ¨ä¸º {self.robot_name} åˆ›å»ºç¯å¢ƒ...")
#         self.video_recorder = VideoRecorder(output_dir=config.video_output_dir, fps=20) if config.enable_video_recording else None
#         self.held_object, self.placed_strawberries, self.task_complete, self.current_step = None, set(), False, 0

#         try:
#             # å¯¼å…¥å­˜åœ¨çš„å‡½æ•°
#             from robosuite.controllers import load_composite_controller_config

#             # ä½¿ç”¨æ­£ç¡®çš„ `robot` å…³é”®å­—å‚æ•°è°ƒç”¨
#             controller_config = load_composite_controller_config(robot=self.robot_name)
#             print(f"ğŸ›ï¸ åŠ è½½æ§åˆ¶å™¨é…ç½® for robot: {self.robot_name}")

#             self.env = robosuite.make(
#                 "PickPlace",
#                 robots=self.robot_name,
#                 controller_configs=controller_config,
#                 has_renderer=config.enable_gui,
#                 has_offscreen_renderer=config.enable_video_recording or not config.enable_gui,
#                 use_camera_obs=True,
#                 camera_names="frontview",
#                 camera_heights=480,
#                 camera_widths=640,
#                 control_freq=20,
#                 horizon=config.max_steps_per_episode,
#                 ignore_done=True,
#                 single_object_mode=2,
#                 object_type="can"
#             )
#             self.robot_model = self.env.robots[0]
#             self.actual_robot_dof, self.action_dim = self.robot_model.dof, self.robot_model.action_dim
#             print(f"   - å®é™…æœºå™¨äºº: {type(self.robot_model).__name__}, DOF: {self.actual_robot_dof}, åŠ¨ä½œç»´åº¦: {self.action_dim}")
#             # self.table_offset = self.env.table_offset

#             # PickPlaceç¯å¢ƒæ²¡æœ‰table_offsetå±æ€§ï¼Œä½¿ç”¨é»˜è®¤å€¼
#             self.table_offset = np.array([0.8, 0, 0])

#             self._setup_virtual_object_positions()
#             print(f"âœ… Robosuite ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼Œå¹¶è®¾å®šäº†è™šæ‹Ÿè‰è“ä»»åŠ¡ã€‚")
#         except Exception as e:
#             print(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}"); import traceback; traceback.print_exc(); raise


#     def _setup_virtual_object_positions(self):
#         center = self.table_offset
#         self.virtual_plate_pos = center + np.array([0, -0.25, 0.01])
#         self.virtual_strawberry_positions = [center + np.array([-0.15, 0.1, 0.03]), center + np.array([0.15, 0.15, 0.03]), center + np.array([0.0, 0.05, 0.03])]

#     def reset(self):
#         obs = self.env.reset()
#         self.current_step, self.held_object, self.task_complete = 0, None, False; self.placed_strawberries.clear()
#         self._setup_virtual_object_positions()
#         return self._process_observation(obs)

#     def step(self, action):
#         obs, _, _, info = self.env.step(action)
#         self.current_step += 1
#         task_reward = self._calculate_task_reward(obs)
#         task_success = len(self.placed_strawberries) == 3
#         done = task_success or self.current_step >= self.config.max_steps_per_episode
#         processed_obs = self._process_observation(obs)
#         info.update(self.get_task_info())
#         if self.video_recorder and self.video_recorder.is_recording:
#             info_for_video = {'step': self.current_step, 'reward': task_reward, 'task_progress': info['task_progress'], 'strawberries_on_plate': info['strawberries_on_plate'], 'task_success': task_success}
#             self.video_recorder.add_frame(processed_obs["frontview_image"], info_for_video)
#         return processed_obs, task_reward, done, info

#     def _calculate_task_reward(self, obs):
#         reward = 0.0
#         eef_pos = obs.get("robot0_eef_pos")
#         gripper_qpos = obs.get("robot0_gripper_qpos")
#         if eef_pos is None or gripper_qpos is None: return 0.0
        
#         # SO100å¤¹çˆªè¿”å›æ ‡é‡ï¼Œç›´æ¥ä½¿ç”¨
#         if hasattr(gripper_qpos, '__len__') and len(gripper_qpos) > 0:
#             gripper_normalized = np.clip(gripper_qpos[0], 0, 1)
#         else:
#             gripper_normalized = np.clip(gripper_qpos, 0, 1)

#         if self.held_object is None:
#             dists = [np.linalg.norm(eef_pos - pos) for i, pos in enumerate(self.virtual_strawberry_positions) if i not in self.placed_strawberries]
#             if dists:
#                 min_dist = min(dists)
#                 reward += 2.0 * (1.0 - np.tanh(5.0 * min_dist)) # Dense reward for approaching
#                 if min_dist < 0.05 and gripper_normalized < 0.3: # Grasp condition
#                     idx_to_pick = [i for i, pos in enumerate(self.virtual_strawberry_positions) if i not in self.placed_strawberries][np.argmin(dists)]
#                     self.held_object = f"strawberry_{idx_to_pick}"; reward += 20.0; print(f"   ğŸ“ æŠ“å–è‰è“ {idx_to_pick}!")
#         else:
#             dist_to_plate = np.linalg.norm(eef_pos[:2] - self.virtual_plate_pos[:2])
#             height_diff = eef_pos[2] - self.virtual_plate_pos[2]
#             reward += 2.0 * (1.0 - np.tanh(5.0 * dist_to_plate)) # Dense reward for approaching plate
#             if dist_to_plate < 0.1 and height_diff > 0.02 and height_diff < 0.1 and gripper_normalized > 0.7: # Place condition
#                 idx = int(self.held_object.split('_')[-1]); self.placed_strawberries.add(idx); self.held_object = None; reward += 30.0; print(f"   ğŸ½ï¸ æ”¾ç½®è‰è“! ({len(self.placed_strawberries)}/3)")
        
#         if len(self.placed_strawberries) == 3 and not self.task_complete:
#             reward += 100.0; self.task_complete = True; print("\nğŸ‰ ä»»åŠ¡æˆåŠŸ!")
#         return reward

#     def _process_observation(self, obs):
#         img = obs.get("frontview_image")
#         return {
#             "frontview_image": img[::-1].astype(np.uint8) if img is not None else np.zeros((480, 640, 3), np.uint8),
#             "robot0_joint_pos": obs.get("robot0_joint_pos"),
#             "robot0_gripper_qpos": obs.get("robot0_gripper_qpos"),
#         }

#     def get_task_info(self):
#         return {"task_success": self.task_complete, "strawberries_on_plate": len(self.placed_strawberries), "task_progress": len(self.placed_strawberries) / 3.0}

#     def start_episode_recording(self, episode_id: int):
#         if self.video_recorder: return self.video_recorder.start_episode_recording(episode_id, f"strawberry_{self.robot_name.lower()}")

#     def stop_episode_recording(self):
#         if self.video_recorder: self.video_recorder.stop_episode_recording()

#     def close(self):
#         if self.video_recorder: self.video_recorder.cleanup()
#         if hasattr(self, 'env'): self.env.close()


# class RoboSuiteGR00TAdapter:
#     def __init__(self, robot_dof: int): self.robot_dof = robot_dof
#     def robosuite_to_groot_obs(self, obs: dict, task_desc: str) -> dict:
#         joint_pos = obs.get("robot0_joint_pos")
#         if joint_pos is None: joint_pos = np.zeros(self.robot_dof)
#         if len(joint_pos) != self.robot_dof:
#             padded = np.zeros(self.robot_dof); min_len = min(len(joint_pos), self.robot_dof); padded[:min_len] = joint_pos[:min_len]; joint_pos = padded
        
#         gripper_pos = obs.get("robot0_gripper_qpos")
#         gripper_norm = np.clip(gripper_pos[0], 0, 1) if gripper_pos is not None and len(gripper_pos) > 0 else 0.0
        
#         return {
#             "video.webcam": obs.get("frontview_image", np.zeros((480, 640, 3), np.uint8))[np.newaxis, ...],
#             "state.single_arm": joint_pos[np.newaxis, :].astype(np.float32),
#             "state.gripper": np.array([[gripper_norm]], dtype=np.float32),
#             "annotation.human.task_description": [task_desc]
#         }

#     def groot_to_robosuite_action(self, groot_action: dict) -> np.ndarray:
#         vec = groot_action.get('world_vector', np.zeros((1, 3)))[0]
#         rot = groot_action.get('rotation_delta', np.zeros((1, 3)))[0]
#         grip = groot_action.get('gripper_closedness_action', np.zeros((1, 1)))[0][0]
#         return np.concatenate([vec, rot, [grip]]).astype(np.float32)


# class GR00TClient:
#     def __init__(self, config: ExperimentConfig, robot_dof: int):
#         self.config = config
#         self.adapter = RoboSuiteGR00TAdapter(robot_dof)
#         self.client = None
#         self.is_connected = self._connect()

#     def _connect(self) -> bool:
#         try:
#             print(f"ğŸ”— æ­£åœ¨è¿æ¥åˆ°GR00TæœåŠ¡: {self.config.groot_host}:{self.config.groot_port}...")
#             self.client = RobotInferenceClient(host=self.config.groot_host, port=self.config.groot_port)
#             self.client.get_action(self.adapter.robosuite_to_groot_obs({}, "test"))
#             print("âœ… GR00Tè¿æ¥æˆåŠŸï¼")
#             return True
#         except Exception as e:
#             print(f"âŒ GR00Tè¿æ¥å¤±è´¥: {e}")
#             return False

#     def get_action(self, obs, task_desc):
#         if not self.is_connected: return None
#         try:
#             groot_obs = self.adapter.robosuite_to_groot_obs(obs, task_desc)
#             groot_action = self.client.get_action(groot_obs)
#             return self.adapter.groot_to_robosuite_action(groot_action) if groot_action else None
#         except Exception: return None


# class StrawberryPickPlaceInterface:
#     def __init__(self, config: ExperimentConfig):
#         self.config = config
#         self.env = StrawberryPickPlaceEnvironment(config)
#         self.groot_client = GR00TClient(config, self.env.actual_robot_dof)

#     def run_experiment(self):
#         if not self.groot_client.is_connected:
#             print("âŒ æ— æ³•ç»§ç»­å®éªŒï¼ŒGR00Tè¿æ¥å¤±è´¥ã€‚")
#             self.close()
#             return

#         for i in range(self.config.num_episodes):
#             print(f"\nğŸ¯ Episode {i + 1}/{self.config.num_episodes}")
#             if self.config.enable_video_recording:
#                 self.env.start_episode_recording(i)
            
#             obs, done, step_count = self.env.reset(), False, 0
#             while not done:
#                 action = self.groot_client.get_action(obs, "Pick the red strawberries and place them on the white plate.")
#                 if action is None:
#                     action = np.zeros(self.env.action_dim)
#                     print("x", end="", flush=True)
#                 else:
#                     print(".", end="", flush=True)

#                 obs, _, done, info = self.env.step(action)
#                 step_count += 1
#                 if step_count % 50 == 0: print(f" (step {step_count})", end="", flush=True)
            
#             print(f"\nEpisode {i+1} ç»“æŸ. æ­¥æ•°: {step_count}, æˆåŠŸ: {info.get('task_success', False)}")
#             if self.config.enable_video_recording:
#                 self.env.stop_episode_recording()
        
#         self.close()

#     def close(self):
#         self.env.close()
#         print("\næ¥å£å·²å…³é—­ã€‚")


# def main():
#     print("=" * 60 + "\nğŸ“ RoboSuite-GR00Tè‰è“æ‹£é€‰æ¥å£ (å®˜æ–¹æ³¨å†Œç‰ˆ) ğŸ“\n" + "=" * 60)
    
#     try:
#         # æ£€æŸ¥SO100æ˜¯å¦çœŸçš„è¢«æ³¨å†Œäº†
#         from robosuite.models.robots import SO100
#         print("âœ… SO100æœºå™¨äººå·²åœ¨RoboSuiteä¸­æˆåŠŸæ³¨å†Œã€‚")
#         default_robot = "SO100"
#     except ImportError:
#         print("âš ï¸ SO100æœºå™¨äººæœªåœ¨RoboSuiteä¸­æ³¨å†Œï¼Œå°†ä½¿ç”¨Pandaã€‚")
#         default_robot = "Panda"

#     try:
#         if default_robot == "SO100":
#             choice = input("ğŸ¤– ä½¿ç”¨å“ªä¸ªæœºå™¨äºº? [1] SO100 (5-DOF) [2] Panda (7-DOF) (é»˜è®¤: 1): ").strip()
#             robot_type = "Panda" if choice == '2' else "SO100"
#         else:
#             robot_type = "Panda"
#             print("   å°†ä½¿ç”¨ Panda (7-DOF) æœºå™¨äººã€‚")

#     except (EOFError, KeyboardInterrupt):
#         robot_type = default_robot
    
#     config = ExperimentConfig(
#         robot=robot_type,
#         video_output_dir=f"./strawberry_{robot_type.lower()}_videos"
#     )
    
#     print(f"\nğŸ› ï¸ å®éªŒé…ç½®: æœºå™¨äºº={config.robot}, Episodes={config.num_episodes}\n")

#     # try:
#     #     interface = StrawberryPickPlaceInterface(config)
#     #     interface.run_experiment()
#     # except Exception as e:
#     #     print(f"\nâŒ å®éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
#     #     import traceback
#     #     traceback.print_exc()


#     # åœ¨ main() ä¸­
#     try:
#         interface = StrawberryPickPlaceInterface(config)
#         interface.run_experiment()
#     except AssertionError as e:
#         # ä¸“é—¨æ•è· AssertionError
#         error_message = str(e)
#         if "Got 6, expected 6!" in error_message:
#             # å¦‚æœæ˜¯é‚£ä¸ªæˆ‘ä»¬å·²çŸ¥çš„ã€æ— å®³çš„æ–­è¨€é”™è¯¯ï¼Œå°±æ‰“å°ä¸€ä¸ªè­¦å‘Šç„¶åç»§ç»­
#             print("\nâœ… [å·²çŸ¥é—®é¢˜] æ•è·åˆ°ä¸€ä¸ªæ— å®³çš„æ–­è¨€é”™è¯¯ï¼Œç¯å¢ƒå¯èƒ½å·²æˆåŠŸåˆ›å»ºã€‚")
#             print("   å¦‚æœç¨‹åºåœ¨æ­¤ä¹‹åæ²¡æœ‰ç»§ç»­ï¼Œè¯·æ£€æŸ¥å…¶ä»–é—®é¢˜ã€‚")
#             # ç†è®ºä¸Šï¼Œå¦‚æœç¯å¢ƒåˆ›å»ºæˆåŠŸï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ç»§ç»­ï¼Œä½†è¿™æ¯”è¾ƒå¤æ‚ã€‚
#             # æœ€ç®€å•çš„åšæ³•æ˜¯ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™æ¬¡è¿è¡Œæ˜¯æˆåŠŸçš„ï¼Œåªæ˜¯è¢«è¿™ä¸ªå‡é”™è¯¯ä¸­æ–­äº†ã€‚
#             print("\nğŸ‰ æ­å–œï¼æ‚¨å·²ç»æˆåŠŸè§£å†³äº†æ‰€æœ‰åˆå§‹åŒ–éšœç¢ï¼")
#             print("   è¿™ä¸ª'é”™è¯¯'å®é™…ä¸Šæ˜¯robosuiteçš„ä¸€ä¸ªè‰¯æ€§bugã€‚ç°åœ¨æ‚¨å¯ä»¥å¼€å§‹çœŸæ­£çš„å®éªŒäº†ã€‚")
#         else:
#             # å¦‚æœæ˜¯å…¶ä»–æœªçŸ¥çš„ AssertionErrorï¼Œæ­£å¸¸æŠ¥é”™
#             print(f"\nâŒ å®éªŒè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥çš„æ–­è¨€é”™è¯¯: {e}")
#             import traceback
#             traceback.print_exc()
#     except Exception as e:
#         # æ•è·æ‰€æœ‰å…¶ä»–ç±»å‹çš„é”™è¯¯
#         print(f"\nâŒ å®éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
#         import traceback
#         traceback.print_exc()

# if __name__ == "__main__":
#     main()


# #!/usr/bin/env python3
# """
# RoboSuite-GR00Tè‰è“æ‹£é€‰æ¥å£ (å…¼å®¹æ€§ä¿®å¤ç‰ˆ)
# è§£å†³SO100æœºå™¨äººå…¼å®¹æ€§é—®é¢˜ï¼Œä½¿ç”¨PickPlaceç¯å¢ƒå¹¶è‡ªå®šä¹‰åœºæ™¯
# """

# import os
# import sys
# import time
# import json
# import numpy as np
# import cv2
# from pathlib import Path
# from dataclasses import dataclass
# from datetime import datetime

# # è®¾ç½®ç¯å¢ƒå˜é‡
# os.environ.setdefault('MUJOCO_GL', 'egl')
# os.environ.setdefault('PYOPENGL_PLATFORM', 'egl')

# # å¯¼å…¥æ£€æŸ¥
# try:
#     import robosuite
#     from robosuite.controllers import load_composite_controller_config
#     from robosuite.utils.placement_samplers import UniformRandomSampler
#     ROBOSUITE_AVAILABLE = True
#     print("âœ… RoboSuiteå¯ç”¨")
# except ImportError as e:
#     print(f"âŒ RoboSuiteä¸å¯ç”¨: {e}")
#     ROBOSUITE_AVAILABLE = False

# try:
#     from gr00t.eval.robot import RobotInferenceClient
#     print("âœ… GR00Tå®¢æˆ·ç«¯å¯ç”¨")
# except ImportError as e:
#     print(f"âŒ GR00Tå®¢æˆ·ç«¯ä¸å¯ç”¨: {e}"); sys.exit(1)


# class VideoRecorder:
#     def __init__(self, output_dir: str = "./strawberry_videos", fps: int = 20, video_size: tuple = (640, 480), codec: str = 'mp4v'):
#         self.output_dir = Path(output_dir); self.output_dir.mkdir(parents=True, exist_ok=True)
#         self.fps, self.video_size, self.codec = fps, video_size, codec
#         self.is_recording, self.video_writer, self.current_episode, self.frame_count = False, None, 0, 0
#         print(f"ğŸ¥ è§†é¢‘å½•åˆ¶å™¨åˆå§‹åŒ–äº: {self.output_dir}")
    
#     def start_episode_recording(self, episode_id: int, experiment_name: str):
#         if self.is_recording: self.stop_episode_recording()
#         self.current_episode, self.frame_count = episode_id, 0
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         filename = f"{experiment_name}_episode_{episode_id:03d}_{timestamp}.mp4"
#         self.video_path = self.output_dir / filename
#         fourcc = cv2.VideoWriter_fourcc(*self.codec)
#         self.video_writer = cv2.VideoWriter(str(self.video_path), fourcc, self.fps, self.video_size)
#         if not self.video_writer.isOpened(): print(f"âŒ æ— æ³•åˆ›å»ºè§†é¢‘æ–‡ä»¶: {self.video_path}"); return False
#         self.is_recording = True; print(f"ğŸ¬ å¼€å§‹å½•åˆ¶ Episode {episode_id}: {filename}"); return True
    
#     def add_frame(self, image: np.ndarray, step_info: dict = None):
#         if not self.is_recording: return
#         processed_image = self._process_image(image, step_info)
#         if self.video_writer and self.video_writer.isOpened(): self.video_writer.write(processed_image); self.frame_count += 1
    
#     def _process_image(self, image: np.ndarray, step_info: dict = None) -> np.ndarray:
#         if image is None: image = np.zeros((*self.video_size[::-1], 3), dtype=np.uint8)
#         if image.dtype != np.uint8: image = (image * 255).astype(np.uint8) if image.max() <= 1.0 else image.astype(np.uint8)
#         if image.shape[:2] != self.video_size[::-1]: image = cv2.resize(image, self.video_size)
#         if len(image.shape) == 2 or (len(image.shape) == 3 and image.shape[2] == 1): image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
#         elif len(image.shape) == 3 and image.shape[2] == 4: image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)
#         if step_info: image = self._add_info_overlay(image, step_info)
#         return image
    
#     def _add_info_overlay(self, image: np.ndarray, step_info: dict) -> np.ndarray:
#         overlay = image.copy()
#         font, scale, color, thick, y = cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, 30
#         if 'step' in step_info: cv2.putText(overlay, f"Ep: {self.current_episode} | Step: {step_info['step']}", (10, y), font, scale, color, thick); y += 25
#         if 'task_progress' in step_info: cv2.putText(overlay, f"Progress: {step_info['task_progress']:.0%} | Picked: {step_info.get('strawberries_on_plate', 0)}/3", (10, y), font, scale, color, thick); y += 25
#         if 'reward' in step_info: cv2.putText(overlay, f"Reward: {step_info['reward']:.2f}", (10, y), font, scale, color, thick); y += 25
#         if step_info.get('task_success', False): cv2.putText(overlay, "TASK SUCCESS!", (10, image.shape[0] - 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3)
#         return overlay
    
#     def stop_episode_recording(self):
#         if not self.is_recording: return
#         print(f"ğŸ¬ åœæ­¢å½•åˆ¶ Episode {self.current_episode} ({self.frame_count} å¸§)")
#         self.is_recording = False
#         if self.video_writer: self.video_writer.release(); self.video_writer = None
#         if hasattr(self, 'video_path') and self.video_path.exists(): print(f"âœ… è§†é¢‘å·²ä¿å­˜: {self.video_path} ({self.video_path.stat().st_size / 1e6:.1f}MB)")
    
#     def cleanup(self):
#         if self.is_recording: self.stop_episode_recording()


# @dataclass
# class ExperimentConfig:
#     robot: str = "Panda"  # æ”¹ä¸ºPandaï¼Œæ›´ç¨³å®š
#     num_episodes: int = 3
#     max_steps_per_episode: int = 250
#     enable_gui: bool = False  # å¯ç”¨GUIè§‚å¯Ÿ
#     enable_video_recording: bool = True
#     video_output_dir: str = "./strawberry_videos"
#     groot_host: str = "localhost"
#     groot_port: int = 5555


# class StrawberryPickPlaceEnvironment:
#     def __init__(self, config: ExperimentConfig):
#         self.config = config
#         self.robot_name = config.robot
#         print(f"ğŸ¤– æ­£åœ¨ä¸º {self.robot_name} åˆ›å»ºç¯å¢ƒ...")
#         self.video_recorder = VideoRecorder(output_dir=config.video_output_dir, fps=20) if config.enable_video_recording else None
#         self.held_object, self.placed_strawberries, self.task_complete, self.current_step = None, set(), False, 0

#         try:
#             from robosuite.controllers import load_composite_controller_config

#             # ä½¿ç”¨ç¨³å®šçš„PickPlaceç¯å¢ƒ
#             controller_config = load_composite_controller_config(robot=self.robot_name)
#             print(f"ğŸ›ï¸ åŠ è½½æ§åˆ¶å™¨é…ç½® for robot: {self.robot_name}")

#             # åˆ›å»ºç¯å¢ƒ - ä½¿ç”¨å¤šç‰©ä½“æ¨¡å¼
#             self.env = robosuite.make(
#                 "PickPlace",
#                 robots=self.robot_name,
#                 controller_configs=controller_config,
#                 has_renderer=config.enable_gui,
#                 has_offscreen_renderer=config.enable_video_recording or not config.enable_gui,
#                 use_camera_obs=True,
#                 camera_names=["frontview", "agentview"],  # å¤šä¸ªè§†è§’
#                 camera_heights=480,
#                 camera_widths=640,
#                 control_freq=20,
#                 horizon=config.max_steps_per_episode,
#                 ignore_done=True,
#                 single_object_mode=0,  # å…è®¸å¤šä¸ªå¯¹è±¡
#                 object_type="milk",    # ä½¿ç”¨milkå¯¹è±¡ä½œä¸º"ç›˜å­"
#                 reward_shaping=True,
#             )
            
#             self.robot_model = self.env.robots[0]
#             self.actual_robot_dof, self.action_dim = self.robot_model.dof, self.robot_model.action_dim
#             print(f"   - å®é™…æœºå™¨äºº: {type(self.robot_model).__name__}, DOF: {self.actual_robot_dof}, åŠ¨ä½œç»´åº¦: {self.action_dim}")
            
#             # è·å–ç¯å¢ƒçš„å®é™…ä¿¡æ¯
#             self._get_environment_info()
#             self._setup_custom_task()
#             print(f"âœ… Robosuite ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼Œå·²è®¾ç½®è‡ªå®šä¹‰è‰è“ä»»åŠ¡ã€‚")
            
#         except Exception as e:
#             print(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}"); import traceback; traceback.print_exc(); raise

#     def _get_environment_info(self):
#         """è·å–ç¯å¢ƒçš„å®é™…ä¿¡æ¯"""
#         # é‡ç½®ç¯å¢ƒè·å–åˆå§‹çŠ¶æ€
#         initial_obs = self.env.reset()
        
#         # è·å–æœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨ä½ç½®
#         if "robot0_eef_pos" in initial_obs:
#             self.robot_eef_pos = initial_obs["robot0_eef_pos"]
#             print(f"ğŸ” æœºå™¨äººæœ«ç«¯ä½ç½®: {self.robot_eef_pos}")
#         else:
#             self.robot_eef_pos = np.array([0.5, 0, 1.0])  # é»˜è®¤å€¼
        
#         # å°è¯•è·å–æ¡Œå­ä¿¡æ¯
#         try:
#             self.table_offset = self.env.table_offset
#             print(f"ğŸ” æ¡Œå­åç§»: {self.table_offset}")
#         except AttributeError:
#             # åŸºäºæœºå™¨äººä½ç½®æ¨ç®—æ¡Œå­ä½ç½®
#             robot_base_z = self.robot_eef_pos[2]
#             # å¦‚æœæœºå™¨äººåœ¨åœ°é¢ï¼Œæ¡Œå­é«˜åº¦è®¾ä¸ºåˆç†å€¼
#             if robot_base_z < 0.5:
#                 table_z = robot_base_z + 0.8  # æ¡Œå­æ¯”æœºå™¨äººåŸºåº§é«˜0.8ç±³
#             else:
#                 table_z = robot_base_z + 0.1   # æœºå™¨äººå·²ç»åœ¨åˆç†é«˜åº¦
            
#             self.table_offset = np.array([self.robot_eef_pos[0], self.robot_eef_pos[1], table_z])
#             print(f"ğŸ” æ¨ç®—æ¡Œå­ä½ç½®: {self.table_offset}")
#             print(f"ğŸ” æœºå™¨äººåˆ°æ¡Œé¢è·ç¦»: {table_z - robot_base_z:.3f}m")

#     def _setup_custom_task(self):
#         """è®¾ç½®è‡ªå®šä¹‰è‰è“ä»»åŠ¡çš„è™šæ‹Ÿå¯¹è±¡ä½ç½®"""
#         # åŸºäºæœºå™¨äººå®é™…ä½ç½®è®¾ç½®è™šæ‹Ÿå¯¹è±¡
#         robot_x, robot_y, robot_z = self.robot_eef_pos
        
#         # å·¥ä½œå°é¢åº”è¯¥åœ¨æœºå™¨äººæœ«ç«¯é™„è¿‘ï¼ˆç¨å¾®é«˜ä¸€ç‚¹ï¼‰
#         work_surface_z = robot_z + 0.05  # æ¯”æœºå™¨äººæœ«ç«¯é«˜5cm
        
#         # ç›˜å­ä½ç½®ï¼ˆåœ¨æœºå™¨äººå‰æ–¹ï¼‰
#         self.virtual_plate_pos = np.array([robot_x + 0.2, robot_y - 0.1, work_surface_z])
        
#         # ä¸‰ä¸ªè‰è“çš„ä½ç½®ï¼ˆå›´ç»•æœºå™¨äººå¯è¾¾èŒƒå›´ï¼‰
#         self.virtual_strawberry_positions = [
#             np.array([robot_x + 0.1, robot_y + 0.1, work_surface_z]),   # å³å‰
#             np.array([robot_x - 0.1, robot_y + 0.1, work_surface_z]),   # å·¦å‰
#             np.array([robot_x, robot_y + 0.15, work_surface_z])         # æ­£å‰
#         ]
        
#         # å››ä¸ªç»¿è‰²å°çƒçš„ä½ç½®ï¼ˆåˆ†æ•£ä½†åœ¨å¯è¾¾èŒƒå›´å†…ï¼‰
#         self.virtual_green_balls = [
#             np.array([robot_x + 0.15, robot_y + 0.05, work_surface_z]), # å³è¿‘
#             np.array([robot_x - 0.15, robot_y + 0.05, work_surface_z]), # å·¦è¿‘
#             np.array([robot_x + 0.05, robot_y + 0.2, work_surface_z]),  # å³è¿œ
#             np.array([robot_x - 0.05, robot_y + 0.2, work_surface_z])   # å·¦è¿œ
#         ]
        
#         print(f"ğŸ“ è™šæ‹Ÿè‰è“ä½ç½®: {len(self.virtual_strawberry_positions)} ä¸ª")
#         for i, pos in enumerate(self.virtual_strawberry_positions):
#             dist = np.linalg.norm(pos - self.robot_eef_pos)
#             print(f"   è‰è“{i}: {pos} (è·ç¦»æœºå™¨äºº: {dist:.3f}m)")
            
#         print(f"ğŸŸ¢ è™šæ‹Ÿç»¿çƒä½ç½®: {len(self.virtual_green_balls)} ä¸ª") 
#         print(f"ğŸ½ï¸ è™šæ‹Ÿç›˜å­ä½ç½®: {self.virtual_plate_pos}")
        
#         plate_dist = np.linalg.norm(self.virtual_plate_pos - self.robot_eef_pos)
#         print(f"ğŸ“ ç›˜å­è·ç¦»æœºå™¨äºº: {plate_dist:.3f}m")
        
#         # æ‰“å°ä½ç½®ä¿¡æ¯ç”¨äºè°ƒè¯•
#         print(f"ğŸ“ æœºå™¨äººæœ«ç«¯: {self.robot_eef_pos}")
#         print(f"ğŸ“ å·¥ä½œè¡¨é¢é«˜åº¦: {work_surface_z:.3f}m")

#     def reset(self):
#         obs = self.env.reset()
#         self.current_step, self.held_object, self.task_complete = 0, None, False
#         self.placed_strawberries.clear()
        
#         # é‡æ–°è·å–ç¯å¢ƒä¿¡æ¯ï¼ˆå› ä¸ºé‡ç½®å¯èƒ½æ”¹å˜ä½ç½®ï¼‰
#         self._get_environment_info()
#         self._setup_custom_task()
        
#         return self._process_observation(obs)

#     def step(self, action):
#         obs, env_reward, done, info = self.env.step(action)
#         self.current_step += 1
        
#         # è®¡ç®—è‡ªå®šä¹‰ä»»åŠ¡å¥–åŠ±
#         task_reward = self._calculate_strawberry_reward(obs)
        
#         # ä»»åŠ¡æˆåŠŸæ¡ä»¶
#         task_success = len(self.placed_strawberries) == 3
#         done = task_success or self.current_step >= self.config.max_steps_per_episode
        
#         processed_obs = self._process_observation(obs)
#         info.update(self.get_task_info())
        
#         # å½•åˆ¶è§†é¢‘å¸§
#         if self.video_recorder and self.video_recorder.is_recording:
#             info_for_video = {
#                 'step': self.current_step,
#                 'reward': task_reward,
#                 'task_progress': info['task_progress'],
#                 'strawberries_on_plate': info['strawberries_on_plate'],
#                 'task_success': task_success,
#                 'env_reward': env_reward  # ä¹Ÿæ˜¾ç¤ºç¯å¢ƒåŸå§‹å¥–åŠ±
#             }
#             self.video_recorder.add_frame(processed_obs["frontview_image"], info_for_video)
        
#         return processed_obs, task_reward, done, info

#     def _calculate_strawberry_reward(self, obs):
#         """è®¡ç®—åŸºäºè™šæ‹Ÿè‰è“æ‹£é€‰ä»»åŠ¡çš„å¥–åŠ±"""
#         reward = 0.0
#         eef_pos = obs.get("robot0_eef_pos")
#         gripper_qpos = obs.get("robot0_gripper_qpos")
        
#         if eef_pos is None or gripper_qpos is None:
#             return 0.0
        
#         # ä¿®å¤å¤¹çˆªçŠ¶æ€å¤„ç†
#         if hasattr(gripper_qpos, '__len__') and not isinstance(gripper_qpos, (int, float)):
#             if len(gripper_qpos) > 0:
#                 gripper_normalized = np.mean(gripper_qpos)  # Pandaæœ‰ä¸¤ä¸ªå¤¹çˆªjoint
#                 gripper_normalized = (gripper_normalized + 0.04) / 0.08  # Pandaå¤¹çˆªèŒƒå›´çº¦ä¸º[-0.04, 0.04]
#             else:
#                 gripper_normalized = 0.0
#         else:
#             # å•ä¸ªæ ‡é‡å€¼
#             gripper_normalized = float(gripper_qpos)
#             if self.robot_name == "Panda":
#                 gripper_normalized = (gripper_normalized + 0.04) / 0.08
#             else:
#                 gripper_normalized = np.abs(gripper_normalized)
        
#         gripper_normalized = np.clip(gripper_normalized, 0, 1)

#         # å¦‚æœæ²¡æœ‰æŠ“å–ç‰©ä½“ï¼Œå¥–åŠ±æ¥è¿‘è‰è“
#         if self.held_object is None:
#             available_strawberries = [i for i in range(len(self.virtual_strawberry_positions)) 
#                                     if i not in self.placed_strawberries]
            
#             if available_strawberries:
#                 distances = [np.linalg.norm(eef_pos - self.virtual_strawberry_positions[i]) 
#                            for i in available_strawberries]
#                 min_dist = min(distances)
#                 closest_idx = available_strawberries[np.argmin(distances)]
                
#                 # æ¥è¿‘è‰è“çš„å¥–åŠ±
#                 reward += 2.0 * (1.0 - np.tanh(3.0 * min_dist))
                
#                 # æŠ“å–æ¡ä»¶ï¼šè·ç¦»å¾ˆè¿‘ä¸”å¤¹çˆªé—­åˆ
#                 if min_dist < 0.08 and gripper_normalized < 0.3:
#                     self.held_object = f"strawberry_{closest_idx}"
#                     reward += 20.0
#                     print(f"   ğŸ“ è™šæ‹ŸæŠ“å–è‰è“ {closest_idx}! (è·ç¦»:{min_dist:.3f}, å¤¹çˆª:{gripper_normalized:.3f})")
        
#         # å¦‚æœæŠ“å–äº†ç‰©ä½“ï¼Œå¥–åŠ±ç§»åŠ¨åˆ°ç›˜å­
#         else:
#             dist_to_plate = np.linalg.norm(eef_pos - self.virtual_plate_pos)
            
#             # æ¥è¿‘ç›˜å­çš„å¥–åŠ±
#             reward += 2.0 * (1.0 - np.tanh(3.0 * dist_to_plate))
            
#             # æ”¾ç½®æ¡ä»¶ï¼šæ¥è¿‘ç›˜å­ä¸”å¤¹çˆªæ‰“å¼€
#             if dist_to_plate < 0.1 and gripper_normalized > 0.7:
#                 strawberry_idx = int(self.held_object.split('_')[-1])
#                 self.placed_strawberries.add(strawberry_idx)
#                 self.held_object = None
#                 reward += 30.0
#                 print(f"   ğŸ½ï¸ è™šæ‹Ÿæ”¾ç½®è‰è“! ({len(self.placed_strawberries)}/3) (è·ç¦»:{dist_to_plate:.3f}, å¤¹çˆª:{gripper_normalized:.3f})")
        
#         # å®Œæˆä»»åŠ¡çš„é¢å¤–å¥–åŠ±
#         if len(self.placed_strawberries) == 3 and not self.task_complete:
#             reward += 100.0
#             self.task_complete = True
#             print("\nğŸ‰ è™šæ‹Ÿè‰è“æ‹£é€‰ä»»åŠ¡æˆåŠŸ!")
        
#         return reward

#     def _process_observation(self, obs):
#         """å¤„ç†è§‚æµ‹æ•°æ®"""
#         def process_image(img):
#             if img is not None:
#                 # ç¡®ä¿å›¾åƒæ˜¯æ­£ç¡®çš„æ ¼å¼
#                 if img.dtype != np.uint8:
#                     img = (img * 255).astype(np.uint8) if img.max() <= 1.0 else img.astype(np.uint8)
#                 # RoboSuiteå›¾åƒå¯èƒ½éœ€è¦ç¿»è½¬
#                 return img[::-1]
#             else:
#                 return np.zeros((480, 640, 3), np.uint8)
        
#         processed_obs = {
#             "frontview_image": process_image(obs.get("frontview_image")),
#             "robot0_joint_pos": obs.get("robot0_joint_pos"),
#             "robot0_gripper_qpos": obs.get("robot0_gripper_qpos"),
#             "robot0_eef_pos": obs.get("robot0_eef_pos"),
#         }
        
#         # æ·»åŠ agentviewå¦‚æœå¯ç”¨
#         if "agentview_image" in obs:
#             processed_obs["agentview_image"] = process_image(obs.get("agentview_image"))
        
#         return processed_obs

#     def get_task_info(self):
#         """è·å–ä»»åŠ¡ä¿¡æ¯"""
#         return {
#             "task_success": self.task_complete,
#             "strawberries_on_plate": len(self.placed_strawberries),
#             "task_progress": len(self.placed_strawberries) / 3.0,
#             "held_object": self.held_object,
#             "current_step": self.current_step
#         }

#     def start_episode_recording(self, episode_id: int):
#         if self.video_recorder:
#             return self.video_recorder.start_episode_recording(
#                 episode_id, f"strawberry_{self.robot_name.lower()}"
#             )

#     def stop_episode_recording(self):
#         if self.video_recorder:
#             self.video_recorder.stop_episode_recording()

#     def close(self):
#         if self.video_recorder:
#             self.video_recorder.cleanup()
#         if hasattr(self, 'env'):
#             self.env.close()


# class RoboSuiteGR00TAdapter:
#     def __init__(self, robot_dof: int):
#         self.robot_dof = robot_dof
    
#     def robosuite_to_groot_obs(self, obs: dict, task_desc: str) -> dict:
#         joint_pos = obs.get("robot0_joint_pos")
#         if joint_pos is None:
#             joint_pos = np.zeros(self.robot_dof)
        
#         if len(joint_pos) != self.robot_dof:
#             padded = np.zeros(self.robot_dof)
#             min_len = min(len(joint_pos), self.robot_dof)
#             padded[:min_len] = joint_pos[:min_len]
#             joint_pos = padded
        
#         gripper_pos = obs.get("robot0_gripper_qpos")
        
#         # ä¿®å¤å¤¹çˆªæ•°æ®å¤„ç†
#         if gripper_pos is not None:
#             # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°ç»„
#             if hasattr(gripper_pos, '__len__') and not isinstance(gripper_pos, (int, float)):
#                 if len(gripper_pos) > 0:
#                     gripper_norm = np.mean(gripper_pos)  # Pandaæœ‰ä¸¤ä¸ªå¤¹çˆªå…³èŠ‚
#                 else:
#                     gripper_norm = 0.0
#             else:
#                 # å•ä¸ªæ ‡é‡å€¼
#                 gripper_norm = float(gripper_pos)
            
#             # Pandaå¤¹çˆªå½’ä¸€åŒ–ï¼šèŒƒå›´çº¦ä¸º[-0.04, 0.04]
#             if self.robot_dof == 7:  # Pandaæœºå™¨äºº
#                 gripper_norm = (gripper_norm + 0.04) / 0.08
#             else:  # SO100æˆ–å…¶ä»–
#                 gripper_norm = np.abs(gripper_norm)  # ç®€å•å¤„ç†
#         else:
#             gripper_norm = 0.0
        
#         gripper_norm = np.clip(gripper_norm, 0, 1)
        
#         return {
#             "video.webcam": obs.get("frontview_image", np.zeros((480, 640, 3), np.uint8))[np.newaxis, ...],
#             "state.single_arm": joint_pos[np.newaxis, :].astype(np.float32),
#             "state.gripper": np.array([[gripper_norm]], dtype=np.float32),
#             "annotation.human.task_description": [task_desc]
#         }

#     def groot_to_robosuite_action(self, groot_action: dict) -> np.ndarray:
#         vec = groot_action.get('world_vector', np.zeros((1, 3)))[0]
#         rot = groot_action.get('rotation_delta', np.zeros((1, 3)))[0]
#         grip = groot_action.get('gripper_closedness_action', np.zeros((1, 1)))[0][0]
#         return np.concatenate([vec, rot, [grip]]).astype(np.float32)


# class GR00TClient:
#     def __init__(self, config: ExperimentConfig, robot_dof: int):
#         self.config = config
#         self.adapter = RoboSuiteGR00TAdapter(robot_dof)
#         self.client = None
#         self.is_connected = self._connect()

#     def _connect(self) -> bool:
#         try:
#             print(f"ğŸ”— æ­£åœ¨è¿æ¥åˆ°GR00TæœåŠ¡: {self.config.groot_host}:{self.config.groot_port}...")
#             self.client = RobotInferenceClient(host=self.config.groot_host, port=self.config.groot_port)
#             # æµ‹è¯•è¿æ¥
#             test_obs = self.adapter.robosuite_to_groot_obs({}, "test")
#             self.client.get_action(test_obs)
#             print("âœ… GR00Tè¿æ¥æˆåŠŸï¼")
#             return True
#         except Exception as e:
#             print(f"âŒ GR00Tè¿æ¥å¤±è´¥: {e}")
#             print("ğŸ’¡ å°†åœ¨æµ‹è¯•æ¨¡å¼ä¸‹è¿è¡Œ...")
#             return False

#     def get_action(self, obs, task_desc):
#         if not self.is_connected:
#             return None
#         try:
#             groot_obs = self.adapter.robosuite_to_groot_obs(obs, task_desc)
#             groot_action = self.client.get_action(groot_obs)
#             return self.adapter.groot_to_robosuite_action(groot_action) if groot_action else None
#         except Exception as e:
#             # æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
#             if "no len()" in str(e):
#                 print(f"âš ï¸ å¤¹çˆªæ•°æ®ç±»å‹é”™è¯¯: {e}")
#                 gripper_qpos = obs.get('robot0_gripper_qpos')
#                 print(f"   å¤¹çˆªåŸå§‹æ•°æ®: {gripper_qpos} (ç±»å‹: {type(gripper_qpos)})")
#             else:
#                 print(f"âš ï¸ GR00TåŠ¨ä½œç”Ÿæˆå¤±è´¥: {e}")
#             return None


# class StrawberryPickPlaceInterface:
#     def __init__(self, config: ExperimentConfig):
#         self.config = config
#         self.env = StrawberryPickPlaceEnvironment(config)
#         self.groot_client = GR00TClient(config, self.env.actual_robot_dof)

#     def run_experiment(self):
#         task_description = "Pick up the red strawberries and place them on the white plate. There are green balls on the table that should be avoided."
        
#         # å¦‚æœGR00Tä¸å¯ç”¨ï¼Œè¿è¡Œæµ‹è¯•æ¨¡å¼
#         if not self.groot_client.is_connected:
#             print("ğŸ§ª è¿è¡Œç¯å¢ƒæµ‹è¯•æ¨¡å¼ï¼ˆä½¿ç”¨éšæœºåŠ¨ä½œï¼‰...")
#             self._run_test_mode()
#             return

#         print("ğŸš€ å¼€å§‹GR00Tæ§åˆ¶çš„è‰è“æ‹£é€‰å®éªŒ...")
        
#         for i in range(self.config.num_episodes):
#             print(f"\nğŸ¯ Episode {i + 1}/{self.config.num_episodes}")
#             if self.config.enable_video_recording:
#                 self.env.start_episode_recording(i)
            
#             obs, done, step_count = self.env.reset(), False, 0
#             episode_reward = 0.0
            
#             while not done:
#                 action = self.groot_client.get_action(obs, task_description)
#                 if action is None:
#                     # å¦‚æœGR00Tå¤±è´¥ï¼Œä½¿ç”¨å°å¹…åº¦éšæœºåŠ¨ä½œ
#                     action = np.random.normal(0, 0.05, self.env.action_dim)
#                     print("x", end="", flush=True)
#                 else:
#                     print(".", end="", flush=True)

#                 obs, reward, done, info = self.env.step(action)
#                 episode_reward += reward
#                 step_count += 1
                
#                 if step_count % 50 == 0:
#                     print(f" [æ­¥æ•°:{step_count}]", end="", flush=True)
            
#             success = info.get('task_success', False)
#             print(f"\nğŸ“Š Episode {i+1} ç»“æœ:")
#             print(f"   æ­¥æ•°: {step_count}")
#             print(f"   æ€»å¥–åŠ±: {episode_reward:.2f}")
#             print(f"   æˆåŠŸ: {'âœ…' if success else 'âŒ'}")
#             print(f"   è‰è“æ•°: {info.get('strawberries_on_plate', 0)}/3")
            
#             if self.config.enable_video_recording:
#                 self.env.stop_episode_recording()
        
#         self.close()

#     def _run_test_mode(self):
#         """æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨éšæœºåŠ¨ä½œéªŒè¯ç¯å¢ƒ"""
#         print("\nğŸ§ª ç¯å¢ƒæµ‹è¯•æ¨¡å¼å¯åŠ¨...")
        
#         for i in range(1):  # åªè¿è¡Œä¸€ä¸ªepisodeæµ‹è¯•
#             print(f"\nğŸ¯ æµ‹è¯• Episode {i + 1}")
#             if self.config.enable_video_recording:
#                 self.env.start_episode_recording(i)
            
#             obs, done, step_count = self.env.reset(), False, 0
#             episode_reward = 0.0
            
#             print("ğŸ“ åˆå§‹çŠ¶æ€ä¿¡æ¯:")
#             print(f"   æœºå™¨äººæœ«ç«¯ä½ç½®: {obs.get('robot0_eef_pos')}")
#             gripper_qpos = obs.get('robot0_gripper_qpos')
#             print(f"   å¤¹çˆªçŠ¶æ€: {gripper_qpos} (ç±»å‹: {type(gripper_qpos)})")
#             if gripper_qpos is not None and hasattr(gripper_qpos, '__len__'):
#                 print(f"   å¤¹çˆªé•¿åº¦: {len(gripper_qpos) if hasattr(gripper_qpos, '__len__') else 'scalar'}")
#             joint_pos = obs.get('robot0_joint_pos')
#             print(f"   å…³èŠ‚ä½ç½®: {joint_pos} (DOF: {len(joint_pos) if joint_pos is not None else 'None'})")
#             print(f"   å›¾åƒå½¢çŠ¶: {obs.get('frontview_image', np.array([])).shape}")
            
#             while not done and step_count < 100:  # é™åˆ¶æµ‹è¯•æ­¥æ•°
#                 # ç”Ÿæˆå®‰å…¨çš„éšæœºåŠ¨ä½œï¼ˆå°å¹…åº¦ï¼‰
#                 action = np.random.normal(0, 0.02, self.env.action_dim)
#                 action = np.clip(action, -0.05, 0.05)  # é™åˆ¶åŠ¨ä½œå¹…åº¦
                
#                 obs, reward, done, info = self.env.step(action)
#                 episode_reward += reward
#                 step_count += 1
                
#                 if step_count % 20 == 0:
#                     print(f"æ­¥æ•°: {step_count}, å¥–åŠ±: {reward:.3f}, æ€»å¥–åŠ±: {episode_reward:.2f}")
#                     if info.get('held_object'):
#                         print(f"   æŠ“å–çŠ¶æ€: {info['held_object']}")
            
#             print(f"\nğŸ“Š æµ‹è¯•å®Œæˆ:")
#             print(f"   æ€»æ­¥æ•°: {step_count}")
#             print(f"   æ€»å¥–åŠ±: {episode_reward:.2f}")
#             print(f"   ä»»åŠ¡è¿›åº¦: {info.get('task_progress', 0):.1%}")
            
#             if self.config.enable_video_recording:
#                 self.env.stop_episode_recording()

#     def close(self):
#         self.env.close()
#         print("\nğŸ”š å®éªŒæ¥å£å·²å…³é—­ã€‚")


# def main():
#     print("=" * 70)
#     print("ğŸ“ RoboSuite-GR00Tè‰è“æ‹£é€‰æ¥å£ (å…¼å®¹æ€§ä¿®å¤ç‰ˆ) ğŸ“")
#     print("=" * 70)
    
#     try:
#         # é€‰æ‹©æœºå™¨äººï¼ˆé»˜è®¤ä½¿ç”¨æ›´ç¨³å®šçš„Pandaï¼‰
#         try:
#             choice = input("ğŸ¤– é€‰æ‹©æœºå™¨äºº [1] Panda (æ¨è) [2] SO100 (é»˜è®¤: 1): ").strip()
#             robot_type = "SO100" if choice == '2' else "Panda"
#         except (EOFError, KeyboardInterrupt):
#             robot_type = "Panda"
        
#         print(f"ğŸ¤– ä½¿ç”¨æœºå™¨äºº: {robot_type}")
        
#         # åˆ›å»ºé…ç½®
#         config = ExperimentConfig(
#             robot=robot_type,
#             video_output_dir=f"./strawberry_{robot_type.lower()}_videos",
#             enable_gui=False,     # å¯ç”¨GUIè§‚å¯Ÿ
#             num_episodes=1       # æµ‹è¯•ç”¨ï¼Œåªè¿è¡Œ1ä¸ªepisode
#         )
        
#         print(f"\nğŸ› ï¸ å®éªŒé…ç½®:")
#         print(f"   æœºå™¨äºº: {config.robot}")
#         print(f"   Episodes: {config.num_episodes}")
#         print(f"   GUI: {'âœ…' if config.enable_gui else 'âŒ'}")
#         print(f"   è§†é¢‘å½•åˆ¶: {'âœ…' if config.enable_video_recording else 'âŒ'}")
#         print(f"   è¾“å‡ºç›®å½•: {config.video_output_dir}")
#         print()

#         # è¿è¡Œå®éªŒ
#         interface = StrawberryPickPlaceInterface(config)
#         interface.run_experiment()
        
#         print("\nğŸ‰ å®éªŒå®Œæˆï¼æ£€æŸ¥è§†é¢‘æ–‡ä»¶æŸ¥çœ‹ç»“æœã€‚")
        
#     except KeyboardInterrupt:
#         print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­å®éªŒ")
#     except Exception as e:
#         print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
#         import traceback
#         traceback.print_exc()
#         print("\nğŸ’¡ è°ƒè¯•å»ºè®®:")
#         print("   1. ç¡®è®¤robosuiteç‰ˆæœ¬å…¼å®¹æ€§")
#         print("   2. æ£€æŸ¥æœºå™¨äººæ¨¡å‹æ³¨å†ŒçŠ¶æ€")  
#         print("   3. å°è¯•ä½¿ç”¨Pandaæœºå™¨äºº")
#         print("   4. æ£€æŸ¥MuJoCoç¯å¢ƒè®¾ç½®")


# if __name__ == "__main__":
#     main()


import os
import sys
import numpy as np
import cv2
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ.setdefault('MUJOCO_GL', 'egl')
os.environ.setdefault('PYOPENGL_PLATFORM', 'egl')

try:
    import robosuite
    from robosuite.controllers import load_composite_controller_config
    print("âœ… RoboSuiteå¯ç”¨")
except ImportError as e:
    print(f"âŒ RoboSuiteä¸å¯ç”¨: {e}")
    sys.exit(1)

try:
    from gr00t.eval.robot import RobotInferenceClient
    print("âœ… GR00Tå®¢æˆ·ç«¯å¯ç”¨")
except ImportError as e:
    print(f"âŒ GR00Tå®¢æˆ·ç«¯ä¸å¯ç”¨: {e}"); sys.exit(1)


class VideoRecorder:
    def __init__(self, output_dir: str = "./strawberry_videos", fps: int = 20, video_size: tuple = (640, 480), codec: str = 'mp4v'):
        self.output_dir = Path(output_dir); self.output_dir.mkdir(parents=True, exist_ok=True)
        self.fps, self.video_size, self.codec = fps, video_size, codec
        self.is_recording, self.video_writer, self.current_episode, self.frame_count = False, None, 0, 0
        print(f"ğŸ¥ è§†é¢‘å½•åˆ¶å™¨åˆå§‹åŒ–äº: {self.output_dir}")
    
    def start_episode_recording(self, episode_id: int, experiment_name: str):
        if self.is_recording: self.stop_episode_recording()
        self.current_episode, self.frame_count = episode_id, 0
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{experiment_name}_episode_{episode_id:03d}_{timestamp}.mp4"
        self.video_path = self.output_dir / filename
        fourcc = cv2.VideoWriter_fourcc(*self.codec)
        self.video_writer = cv2.VideoWriter(str(self.video_path), fourcc, self.fps, self.video_size)
        if not self.video_writer.isOpened(): print(f"âŒ æ— æ³•åˆ›å»ºè§†é¢‘æ–‡ä»¶: {self.video_path}"); return False
        self.is_recording = True; print(f"ğŸ¬ å¼€å§‹å½•åˆ¶ Episode {episode_id}: {filename}"); return True
    
    def add_frame(self, image: np.ndarray, step_info: dict = None):
        if not self.is_recording: return
        processed_image = self._process_image(image, step_info)
        if self.video_writer and self.video_writer.isOpened(): self.video_writer.write(processed_image); self.frame_count += 1
    
    def _process_image(self, image: np.ndarray, step_info: dict = None) -> np.ndarray:
        if image is None: image = np.zeros((*self.video_size[::-1], 3), dtype=np.uint8)
        if image.dtype != np.uint8: image = (image * 255).astype(np.uint8) if image.max() <= 1.0 else image.astype(np.uint8)
        if image.shape[:2] != self.video_size[::-1]: image = cv2.resize(image, self.video_size)
        if len(image.shape) == 2 or (len(image.shape) == 3 and image.shape[2] == 1): image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
        elif len(image.shape) == 3 and image.shape[2] == 4: image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)
        if step_info: image = self._add_info_overlay(image, step_info)
        return image
    
    def _add_info_overlay(self, image: np.ndarray, step_info: dict) -> np.ndarray:
        overlay = image.copy()
        font, scale, color, thick, y = cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, 30
        if 'step' in step_info: cv2.putText(overlay, f"Ep: {self.current_episode} | Step: {step_info['step']}", (10, y), font, scale, color, thick); y += 25
        if 'task_progress' in step_info: cv2.putText(overlay, f"Progress: {step_info['task_progress']:.0%} | Picked: {step_info.get('strawberries_on_plate', 0)}/3", (10, y), font, scale, color, thick); y += 25
        if 'reward' in step_info: cv2.putText(overlay, f"Reward: {step_info['reward']:.2f}", (10, y), font, scale, color, thick); y += 25
        if step_info.get('task_success', False): cv2.putText(overlay, "TASK SUCCESS!", (10, image.shape[0] - 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3)
        return overlay
    
    def stop_episode_recording(self):
        if not self.is_recording: return
        print(f"ğŸ¬ åœæ­¢å½•åˆ¶ Episode {self.current_episode} ({self.frame_count} å¸§)")
        self.is_recording = False
        if self.video_writer: self.video_writer.release(); self.video_writer = None
        if hasattr(self, 'video_path') and self.video_path.exists(): print(f"âœ… è§†é¢‘å·²ä¿å­˜: {self.video_path} ({self.video_path.stat().st_size / 1e6:.1f}MB)")
    
    def cleanup(self):
        if self.is_recording: self.stop_episode_recording()


@dataclass
class ExperimentConfig:
    robot: str = "Panda"
    num_episodes: int = 3
    max_steps_per_episode: int = 250
    enable_gui: bool = False
    enable_video_recording: bool = True
    video_output_dir: str = "./strawberry_videos"
    groot_host: str = "localhost"
    groot_port: int = 5555


class SimpleStrawberryEnvironment:
    """ç®€åŒ–ç‰ˆè‰è“ç¯å¢ƒ - ä½¿ç”¨ç°æœ‰PickPlace + è™šæ‹Ÿå¯¹è±¡é€»è¾‘"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.robot_name = config.robot
        print(f"ğŸ¤– æ­£åœ¨åˆ›å»ºç®€åŒ–ç‰ˆè‰è“ç¯å¢ƒ (æœºå™¨äºº: {self.robot_name})...")
        self.video_recorder = VideoRecorder(output_dir=config.video_output_dir, fps=20) if config.enable_video_recording else None

        # ä»»åŠ¡çŠ¶æ€
        self.held_object = None
        self.placed_strawberries = set()
        self.task_complete = False
        self.current_step = 0

        try:
            controller_config = load_composite_controller_config(robot=self.robot_name)
            print(f"ğŸ›ï¸ åŠ è½½æ§åˆ¶å™¨é…ç½® for robot: {self.robot_name}")

            # ä½¿ç”¨æ ‡å‡†PickPlaceç¯å¢ƒ
            self.env = robosuite.make(
                "PickPlace",
                robots=self.robot_name,
                controller_configs=controller_config,
                has_renderer=config.enable_gui,
                has_offscreen_renderer=config.enable_video_recording or not config.enable_gui,
                use_camera_obs=True,
                camera_names=["frontview", "agentview"],
                camera_heights=480,
                camera_widths=640,
                control_freq=20,
                horizon=config.max_steps_per_episode,
                ignore_done=True,
                single_object_mode=0,
                object_type="milk",
                reward_shaping=True,
            )
            
            self.robot_model = self.env.robots[0]
            self.actual_robot_dof, self.action_dim = self.robot_model.dof, self.robot_model.action_dim
            print(f"   - æœºå™¨äºº: {type(self.robot_model).__name__}, DOF: {self.actual_robot_dof}, åŠ¨ä½œç»´åº¦: {self.action_dim}")
            
            self._setup_virtual_objects()
            print("âœ… ç®€åŒ–ç‰ˆè‰è“ç¯å¢ƒåˆ›å»ºæˆåŠŸ!")
            
        except Exception as e:
            print(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}"); import traceback; traceback.print_exc(); raise

    def _setup_virtual_objects(self):
        """è®¾ç½®è™šæ‹Ÿè‰è“ã€ç»¿çƒå’Œç›˜å­çš„ä½ç½®"""
        # è·å–æœºå™¨äººåˆå§‹ä½ç½®
        initial_obs = self.env.reset()
        eef_pos = initial_obs.get("robot0_eef_pos", np.array([0, 0, 0.8]))
        
        # å·¥ä½œé¢é«˜åº¦ï¼šæœºå™¨äººæœ«ç«¯ä½ç½®åŸºç¡€ä¸Šè°ƒæ•´
        work_height = eef_pos[2] + 0.05  # æ¯”æœ«ç«¯é«˜5cm
        
        # åœ¨æœºå™¨äººå‰æ–¹è®¾ç½®è™šæ‹Ÿå¯¹è±¡
        base_x, base_y = eef_pos[0], eef_pos[1]
        
        # è‰è“ä½ç½®ï¼ˆå‰æ–¹ä¸‰è§’å½¢åˆ†å¸ƒï¼‰
        self.strawberry_positions = [
            np.array([base_x + 0.1, base_y + 0.1, work_height]),   # å³å‰
            np.array([base_x - 0.1, base_y + 0.1, work_height]),   # å·¦å‰
            np.array([base_x, base_y + 0.15, work_height])         # æ­£å‰
        ]
        
        # ç»¿çƒä½ç½®ï¼ˆå››å‘¨åˆ†å¸ƒï¼‰
        self.green_ball_positions = [
            np.array([base_x + 0.15, base_y + 0.05, work_height]), # å³è¿‘
            np.array([base_x - 0.15, base_y + 0.05, work_height]), # å·¦è¿‘
            np.array([base_x + 0.05, base_y + 0.2, work_height]),  # å³è¿œ
            np.array([base_x - 0.05, base_y + 0.2, work_height])   # å·¦è¿œ
        ]
        
        # ç›˜å­ä½ç½®ï¼ˆå³å‰æ–¹ï¼‰
        self.plate_position = np.array([base_x + 0.2, base_y - 0.1, work_height])
        
        print(f"ğŸ“ è®¾ç½® {len(self.strawberry_positions)} ä¸ªè™šæ‹Ÿè‰è“ä½ç½®")
        print(f"ğŸŸ¢ è®¾ç½® {len(self.green_ball_positions)} ä¸ªè™šæ‹Ÿç»¿çƒä½ç½®")  
        print(f"ğŸ½ï¸ è®¾ç½®è™šæ‹Ÿç›˜å­ä½ç½®: {self.plate_position}")
        print(f"ğŸ“ å·¥ä½œé¢é«˜åº¦: {work_height:.3f}m")
        print(f"ğŸ¤– æœºå™¨äººæœ«ç«¯: {eef_pos}")

    def reset(self):
        obs = self.env.reset()
        self.current_step = 0
        self.held_object = None
        self.placed_strawberries.clear()
        self.task_complete = False
        return self._process_observation(obs)

    def step(self, action):
        obs, env_reward, done, info = self.env.step(action)
        self.current_step += 1
        
        # è®¡ç®—è™šæ‹Ÿè‰è“ä»»åŠ¡å¥–åŠ±
        task_reward = self._calculate_virtual_reward(obs)
        
        # ä»»åŠ¡æˆåŠŸæ¡ä»¶
        task_success = len(self.placed_strawberries) == 3
        done = task_success or self.current_step >= self.config.max_steps_per_episode
        
        processed_obs = self._process_observation(obs)
        info.update(self.get_task_info())
        
        # å½•åˆ¶è§†é¢‘
        if self.video_recorder and self.video_recorder.is_recording:
            info_for_video = {
                'step': self.current_step,
                'reward': task_reward,
                'task_progress': info['task_progress'],
                'strawberries_on_plate': info['strawberries_on_plate'],
                'task_success': task_success
            }
            self.video_recorder.add_frame(processed_obs["frontview_image"], info_for_video)
        
        return processed_obs, task_reward, done, info

    def _calculate_virtual_reward(self, obs):
        """è®¡ç®—åŸºäºè™šæ‹Ÿå¯¹è±¡çš„å¥–åŠ±"""
        reward = 0.0
        eef_pos = obs.get("robot0_eef_pos")
        gripper_qpos = obs.get("robot0_gripper_qpos")
        
        if eef_pos is None or gripper_qpos is None:
            return 0.0
        
        # å¤„ç†å¤¹çˆªçŠ¶æ€
        if hasattr(gripper_qpos, '__len__') and len(gripper_qpos) > 0:
            gripper_norm = np.mean(gripper_qpos)
            if self.robot_name == "Panda":
                gripper_norm = (gripper_norm + 0.04) / 0.08
            else:  # SO100
                gripper_norm = np.abs(gripper_norm)
        else:
            gripper_norm = np.abs(float(gripper_qpos))
        
        gripper_norm = np.clip(gripper_norm, 0, 1)
        
        # å¦‚æœæ²¡æœ‰æŠ“å–ç‰©ä½“ï¼Œå¥–åŠ±æ¥è¿‘è‰è“
        if self.held_object is None:
            available_strawberries = [i for i in range(len(self.strawberry_positions)) 
                                    if i not in self.placed_strawberries]
            
            if available_strawberries:
                distances = [np.linalg.norm(eef_pos - self.strawberry_positions[i]) 
                           for i in available_strawberries]
                min_dist = min(distances)
                closest_idx = available_strawberries[np.argmin(distances)]
                
                # æ¥è¿‘å¥–åŠ±
                reward += 2.0 * (1.0 - np.tanh(3.0 * min_dist))
                
                # æŠ“å–æ¡ä»¶
                if min_dist < 0.06 and gripper_norm < 0.3:
                    self.held_object = f"strawberry_{closest_idx}"
                    reward += 20.0
                    print(f"   ğŸ“ è™šæ‹ŸæŠ“å–è‰è“ {closest_idx}! (è·ç¦»:{min_dist:.3f})")
        
        # å¦‚æœæŠ“å–äº†ç‰©ä½“ï¼Œå¥–åŠ±ç§»åŠ¨åˆ°ç›˜å­
        else:
            dist_to_plate = np.linalg.norm(eef_pos - self.plate_position)
            
            # æ¥è¿‘ç›˜å­å¥–åŠ±
            reward += 2.0 * (1.0 - np.tanh(3.0 * dist_to_plate))
            
            # æ”¾ç½®æ¡ä»¶
            if dist_to_plate < 0.08 and gripper_norm > 0.7:
                strawberry_idx = int(self.held_object.split('_')[-1])
                self.placed_strawberries.add(strawberry_idx)
                self.held_object = None
                reward += 30.0
                print(f"   ğŸ½ï¸ è™šæ‹Ÿæ”¾ç½®è‰è“! ({len(self.placed_strawberries)}/3)")
        
        # ä»»åŠ¡å®Œæˆå¥–åŠ±
        if len(self.placed_strawberries) == 3 and not self.task_complete:
            reward += 100.0
            self.task_complete = True
            print("\nğŸ‰ è™šæ‹Ÿè‰è“ä»»åŠ¡æˆåŠŸ!")
        
        return reward

    def _process_observation(self, obs):
        """å¤„ç†è§‚æµ‹æ•°æ®"""
        def process_image(img):
            if img is not None:
                if img.dtype != np.uint8:
                    img = (img * 255).astype(np.uint8) if img.max() <= 1.0 else img.astype(np.uint8)
                return img[::-1]
            else:
                return np.zeros((480, 640, 3), np.uint8)
        
        return {
            "frontview_image": process_image(obs.get("frontview_image")),
            "agentview_image": process_image(obs.get("agentview_image")),
            "robot0_joint_pos": obs.get("robot0_joint_pos"),
            "robot0_gripper_qpos": obs.get("robot0_gripper_qpos"),
            "robot0_eef_pos": obs.get("robot0_eef_pos"),
        }

    def get_task_info(self):
        return {
            "task_success": self.task_complete,
            "strawberries_on_plate": len(self.placed_strawberries),
            "task_progress": len(self.placed_strawberries) / 3.0,
            "held_object": self.held_object,
            "current_step": self.current_step
        }

    def start_episode_recording(self, episode_id: int):
        if self.video_recorder:
            return self.video_recorder.start_episode_recording(episode_id, f"simple_strawberry_{self.robot_name.lower()}")

    def stop_episode_recording(self):
        if self.video_recorder: self.video_recorder.stop_episode_recording()

    def close(self):
        if self.video_recorder: self.video_recorder.cleanup()
        if hasattr(self, 'env'): self.env.close()


class RoboSuiteGR00TAdapter:
    def __init__(self, robot_dof: int):
        self.robot_dof = robot_dof
    
    def robosuite_to_groot_obs(self, obs: dict, task_desc: str) -> dict:
        joint_pos = obs.get("robot0_joint_pos")
        if joint_pos is None: joint_pos = np.zeros(self.robot_dof)
        
        if len(joint_pos) != self.robot_dof:
            padded = np.zeros(self.robot_dof)
            min_len = min(len(joint_pos), self.robot_dof)
            padded[:min_len] = joint_pos[:min_len]
            joint_pos = padded
        
        gripper_pos = obs.get("robot0_gripper_qpos")
        if gripper_pos is not None:
            if hasattr(gripper_pos, '__len__') and not isinstance(gripper_pos, (int, float)):
                gripper_norm = np.mean(gripper_pos) if len(gripper_pos) > 0 else 0.0
            else:
                gripper_norm = float(gripper_pos)
            
            if self.robot_dof == 7:  # Panda
                gripper_norm = (gripper_norm + 0.04) / 0.08
            else:  # SO100
                gripper_norm = np.abs(gripper_norm)
        else:
            gripper_norm = 0.0
        
        gripper_norm = np.clip(gripper_norm, 0, 1)
        
        return {
            "video.webcam": obs.get("frontview_image", np.zeros((480, 640, 3), np.uint8))[np.newaxis, ...],
            "state.single_arm": joint_pos[np.newaxis, :].astype(np.float32),
            "state.gripper": np.array([[gripper_norm]], dtype=np.float32),
            "annotation.human.task_description": [task_desc]
        }

    def groot_to_robosuite_action(self, groot_action: dict) -> np.ndarray:
        vec = groot_action.get('world_vector', np.zeros((1, 3)))[0]
        rot = groot_action.get('rotation_delta', np.zeros((1, 3)))[0]
        grip = groot_action.get('gripper_closedness_action', np.zeros((1, 1)))[0][0]
        return np.concatenate([vec, rot, [grip]]).astype(np.float32)


class GR00TClient:
    def __init__(self, config: ExperimentConfig, robot_dof: int):
        self.config = config
        self.adapter = RoboSuiteGR00TAdapter(robot_dof)
        self.client = None
        self.is_connected = self._connect()

    def _connect(self) -> bool:
        try:
            print(f"ğŸ”— è¿æ¥GR00TæœåŠ¡: {self.config.groot_host}:{self.config.groot_port}...")
            self.client = RobotInferenceClient(host=self.config.groot_host, port=self.config.groot_port)
            test_obs = self.adapter.robosuite_to_groot_obs({}, "test")
            self.client.get_action(test_obs)
            print("âœ… GR00Tè¿æ¥æˆåŠŸï¼")
            return True
        except Exception as e:
            print(f"âŒ GR00Tè¿æ¥å¤±è´¥: {e}")
            return False

    def get_action(self, obs, task_desc):
        if not self.is_connected: return None
        try:
            groot_obs = self.adapter.robosuite_to_groot_obs(obs, task_desc)
            groot_action = self.client.get_action(groot_obs)
            return self.adapter.groot_to_robosuite_action(groot_action) if groot_action else None
        except Exception as e:
            print(f"âš ï¸ GR00TåŠ¨ä½œå¤±è´¥: {e}")
            return None


class SimpleStrawberryInterface:
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.env = SimpleStrawberryEnvironment(config)
        self.groot_client = GR00TClient(config, self.env.actual_robot_dof)

    def run_experiment(self):
        task_description = "Pick up the virtual red strawberries and place them on the virtual white plate."
        
        if not self.groot_client.is_connected:
            print("ğŸ§ª è¿è¡Œæµ‹è¯•æ¨¡å¼...")
            self._run_test_mode()
            return

        for i in range(self.config.num_episodes):
            print(f"\nğŸ¯ Episode {i + 1}/{self.config.num_episodes}")
            if self.config.enable_video_recording:
                self.env.start_episode_recording(i)
            
            obs, done, step_count = self.env.reset(), False, 0
            episode_reward = 0.0
            
            while not done:
                action = self.groot_client.get_action(obs, task_description)
                if action is None:
                    action = np.random.normal(0, 0.05, self.env.action_dim)
                    print("x", end="", flush=True)
                else:
                    print(".", end="", flush=True)

                obs, reward, done, info = self.env.step(action)
                episode_reward += reward
                step_count += 1
                
                if step_count % 50 == 0:
                    print(f" [æ­¥æ•°:{step_count}]", end="", flush=True)
            
            success = info.get('task_success', False)
            print(f"\nğŸ“Š Episode {i+1} ç»“æœ:")
            print(f"   æ­¥æ•°: {step_count}, å¥–åŠ±: {episode_reward:.2f}")
            print(f"   æˆåŠŸ: {'âœ…' if success else 'âŒ'}, è‰è“: {info.get('strawberries_on_plate', 0)}/3")
            
            if self.config.enable_video_recording:
                self.env.stop_episode_recording()
        
        self.close()

    def _run_test_mode(self):
        print("\nğŸ§ª ç®€åŒ–ç¯å¢ƒæµ‹è¯•æ¨¡å¼...")
        
        for i in range(1):
            print(f"\nğŸ¯ æµ‹è¯• Episode {i + 1}")
            if self.config.enable_video_recording:
                self.env.start_episode_recording(i)
            
            obs, done, step_count = self.env.reset(), False, 0
            episode_reward = 0.0
            
            print("ğŸ“ çŠ¶æ€ä¿¡æ¯:")
            print(f"   æœºå™¨äººæœ«ç«¯: {obs.get('robot0_eef_pos')}")
            print(f"   å…³èŠ‚ä½ç½®: {obs.get('robot0_joint_pos')}")
            print(f"   å¤¹çˆªçŠ¶æ€: {obs.get('robot0_gripper_qpos')}")
            print(f"   è‰è“ä½ç½®: {self.env.strawberry_positions[0]}")  # æ˜¾ç¤ºç¬¬ä¸€ä¸ªè‰è“ä½ç½®
            
            while not done and step_count < 100:
                action = np.random.normal(0, 0.02, self.env.action_dim)
                action = np.clip(action, -0.05, 0.05)
                
                obs, reward, done, info = self.env.step(action)
                episode_reward += reward
                step_count += 1
                
                if step_count % 20 == 0:
                    print(f"æ­¥æ•°: {step_count}, å¥–åŠ±: {reward:.3f}")
                    if info.get('held_object'):
                        print(f"   æŠ“å–çŠ¶æ€: {info['held_object']}")
            
            print(f"\nğŸ“Š æµ‹è¯•å®Œæˆ: {step_count}æ­¥, æ€»å¥–åŠ±: {episode_reward:.2f}")
            
            if self.config.enable_video_recording:
                self.env.stop_episode_recording()

    def close(self):
        self.env.close()
        print("ğŸ”š ç®€åŒ–ç¯å¢ƒå·²å…³é—­")


def main():
    print("ğŸ“ ç®€åŒ–ç‰ˆè‰è“æ‹£é€‰ç¯å¢ƒ")
    print("=" * 50)
    
    try:
        robot_type = input("ğŸ¤– é€‰æ‹©æœºå™¨äºº [1] Panda [2] SO100 (é»˜è®¤: 1): ").strip()
        robot_type = "SO100" if robot_type == '2' else "Panda"
        
        config = ExperimentConfig(
            robot=robot_type,
            video_output_dir=f"./simple_strawberry_{robot_type.lower()}",
            num_episodes=1
        )
        
        print(f"\nğŸ› ï¸ é…ç½®: æœºå™¨äºº={config.robot}, Episodes={config.num_episodes}")
        
        interface = SimpleStrawberryInterface(config)
        interface.run_experiment()
        
        print("\nğŸ‰ ç®€åŒ–ç¯å¢ƒå®éªŒå®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()